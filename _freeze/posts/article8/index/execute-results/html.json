{
  "hash": "6e3bc6ba79a353e13b1f0d540cf79c0e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Building a Book Recommender Engine from Scratch and Deploying it to a Web Application\"\nauthor: \"Tooba Rahimnia\"\ndate: \"2024-08-16\"\ncategories: [Web App, Recommendation System, Collaborative Filtering] #news\nimage: \"opening.jpg\"\n---\n\nOutlining the process of building a book recommendation system using distributed computing with Spark vs pandas, Databricks and Flask in a fully deployed web application. K-mean clustring for Collaborative filtering.\n\n![](wp4506810.jpg){fig-align=\"center\"}\n\nHave you ever found yourself so mesmerized by a book that you wished there were more similar content out there? For many of us book lovers, this isn't just a desire—it's a necessity. Fortunately, with the advancements in machine learning and data science, we now have the privilege of receiving recommendations tailored to our interests. In the world of AI, this is known as a \"Recommendation System.\" These systems are widely used by e-commerce and entertainment companies such as Amazon and Netflix to increase user engagement, loyalty, satisfaction, and ultimately, the company’s profit.\n\nOne popular approach to recommendation systems is collaborative filtering, a technique that predicts items a user might like based on the reactions of similar users. Collaborative filtering is basically a user-based filtering.\n\nIn the user-based approach, we leverage the opinions of people who share similar interests with you. For example, if you and a friend both enjoy the book Atomic Habits by James Clear, and your friend also likes The Compound Effect by Darren Hardy, chances are you’ll like that book too! Companies rely on user feedback and ratings to suggest contents to others with similar tastes. Simply put, if Person A and Person B both like content C, and Person B likes content D, the system will recommend content D to Person A. \n\nAnother approach to recommendation system is called content-based filtering which focuses solely on the contents themselves, identifying which contents are most similar to what the user already enjoys.\n\n![](Content-based-filtering-vs-Collaborative-filtering-Source.png){fig-align=\"center\"}\n\nIn this blog, we’ll implement content-based collaborative filtering. Given the name of a book chosen by our user, we’ll provide 10 suggested books that we think the user might also enjoy. How will we do this? By analyzing a pool of book titles, each ranked by multiple users, we'll identify which books are similar to the one the user likes, then suggest those titles, assuming the user will enjoy them as well.\n\n## Importing Libraries\n\n``` python\n# Handle warning messages\nimport warnings\nwarnings.filterwarnings('ignore')\n```\n\n``` python\n# Data manipulation\nimport numpy as np\nimport pandas as pd\n\n# File handling\nimport os\nimport pickle\n\n# Time management\nimport time\n\n# Data visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Machine learning\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.mixture import GaussianMixture\n```\n\n## Loading Data\n\nWe’re using the [Book Recommendation Dataset](https://www.kaggle.com/datasets/arashnic/book-recommendation-dataset/data) from Kaggle. Within this dataset, there are several files, but we only need `books.csv` and `ratings.csv`. Together, these files provide the ratings of over 1 million users on more than 270,000 books, with no duplicated rows in either dataset. Both files share a common column, `ISBN`, which we’ll use to merge them into one comprehensive dataset. This will make our analysis much smoother and more efficient.\n\nif you are on Kaggle, use the following commands to load your data:\n\n``` python\nbooks_df = pd.read_csv('/kaggle/input/book-recommendation-dataset/Books.csv')\nratings_df = pd.read_csv('/kaggle/input/book-recommendation-dataset/Ratings.csv')\n```\n\nAnd if on Google Colab:\n\n``` python\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\npath = '/path/to/your/dataset/folder'\n\n# List all CSV files in the directory\ncsv_files = [file for file in os.listdir(path) if file.endswith('.csv')]\n\n# Create a dictionary to store the DataFrames\ndataframes = {}\n\n# Read each CSV file into a DataFrame and store it in the dictionary\nfor file in csv_files:\n    # Extract the base name of the file (without extension) to use as the key\n    base_name = os.path.splitext(file)[0]\n\n    # Read the CSV file into a DataFrame\n    dataframes[base_name] = pd.read_csv(os.path.join(path, file), header=True, inferSchema=True)\n    # dataframes[base_name] = pd.read_csv(os.path.join(path, file))\n\n# Display the first few rows of each DataFrame\nfor name, df in dataframes.items():\n    print(f\"DataFrame: {name}\")\n```\n\n``` python\nbooks_df.head()\n```\n\n![](images/Screenshot%202024-08-14%20at%209.36.45%20PM.png){width=\"951\"}\n\n``` python\nratings_df.head()\n```\n\n![](images/Screenshot%202024-08-14%20at%209.37.28%20PM.png){width=\"261\"}\n\n## Data Preprocessing\n\nThe maximum rating a user can give to a book is 10, with 11 distinct rating options ranging from 0 to 10, where a higher rating indicates greater interest and zero means no rating. \n\nAs we merge the two datasets, `books_df` and `ratings_df`, we’ll notice that many items have no rating, resulting in `null` values. These `null` values indicate that the user did not rate that particular book. This situation makes our matrix sparse. Since we need to work exclusively with numerical data, we will replace these `null` values with zeros, signifying that the user did not rate the book.\n\n``` python\n# Calculate the maximum rating and find the unique ratings\nprint(ratings_df['Book-Rating'].max(), ratings_df['Book-Rating'].unique())\n\nratings_df.head(3)\n```\n\n![](images/Screenshot%202024-08-14%20at%209.40.01%20PM.png){width=\"320\"}\n\n``` python\nprint(f'Book dataframe\\'s shape is {books_df.shape}')\nprint('Ratings\\'s shape is: {}'.format(ratings_df.shape))\n```\n\n![](images/Screenshot%202024-08-14%20at%209.41.44%20PM.png){width=\"299\" height=\"49\"}\n\n``` python\nprint('Null values in books dataframe are as follows:')\nprint('**********************************************')\nprint(books_df.isnull().sum())\n```\n\n![](images/Screenshot%202024-08-14%20at%209.43.06%20PM.png){width=\"357\"}\n\n``` python\nprint('Null values in ratings dataframe are as follows:')\nprint('************************************************')\nprint(ratings_df.isnull().sum())\n```\n\n![](images/Screenshot%202024-08-14%20at%209.44.20%20PM.png){width=\"364\"}\n\n``` python\nprint('Number of duplicated values in books_df')\nprint('***************************************')\n\nprint(books_df.duplicated().sum())\n```\n\n![](images/Screenshot%202024-08-14%20at%209.45.13%20PM.png){width=\"314\" height=\"58\"}\n\n``` python\nprint('Number of duplicated values in ratings_df')\nprint('*****************************************')\n\nprint(ratings_df.duplicated().sum())\n```\n\n![](images/Screenshot%202024-08-14%20at%209.46.01%20PM.png){width=\"340\"}\n\nAs mentioned earlier, we merge the two datasets on the 'ISBN' column. To ensure more reliable results, we will filter our data to include only users who have rated more than 100 books and books that have received more than 50 votes. Entries that don't meet these criteria will be dropped from the dataset.\n\n``` python\ndf = books_df.merge(ratings_df, on='ISBN')\n\nuser_prune = df.groupby('User-ID')['Book-Rating'].count() > 100\nuser_and_rating = user_prune[user_prune].index # outputs the User-IDs for users that rate more than 100 books\n\nfiltered_rating = df[df['User-ID'].isin(user_and_rating)]\n\nrating_prune = df.groupby('Book-Title')['Book-Rating'].count() >= 50\nfamous_books = rating_prune[rating_prune].index\n\nfinal_rating = filtered_rating[filtered_rating['Book-Title'].isin(famous_books)]\n```\n\n## Pivot Table\n\nHere, we are creating an item-based filtering system, which means constructing a table where `Book-Title` serve as row and `User-ID` as column.\n\n``` python\nbook_pivot_table = final_rating.pivot_table(index='Book-Title', columns='User-ID', values='Book-Rating')\n\nbook_pivot_table.fillna(0, inplace=True)\n\nprint('datset dimension: ', (book_pivot_table.shape))\nbook_pivot_table.head(5)\n```\n\n![](images/Screenshot%202024-08-14%20at%209.48.01%20PM.png)\n\nNotice how the number of users has shrunk from over a million to just 1,642. This highlights that only 0.14% of users rated more than 100 books, and only 0.9% of books were rated more than 50 times by these avid readers.\n\n## Dimensionality Reduction\n\nEven with this significant reduction in dimensionality, our dataset still consumes a considerable amount of memory, making it computationally expensive to process. To address this, we can further reduce the matrix's dimensions (specifically, the number of users) to decrease memory usage and speed up the training process. For this, we'll use Truncated Singular Value Decomposition (SVD).\n\nTruncated SVD is a matrix factorization technique similar to Principal Component Analysis (PCA). The key difference is that Truncated SVD operates directly on the data matrix, while PCA is applied to the covariance matrix. Truncated SVD factorizes the data matrix, truncating it to a specified number of columns, which effectively reduces the complexity of the data.\n\nRead more at: [TruncatedSVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html)\n\n``` python\ntsvd = TruncatedSVD(n_components=200, random_state=42) # reduce the dimentionality - similar to PCA\nbook_pivot_table_tsvd = tsvd.fit_transform(book_pivot_table)\n```\n\n``` python\nprint('Original number of features before dimensionality reduction: ', (book_pivot_table.count(), len(book_pivot_table.columns)))\nprint('Number of features after dimensionality reduction: ', book_pivot_table_tsvd.shape)\nprint('Explained variance ratio: ', tsvd.explained_variance_ratio_[0:1500].sum())\n```\n\n![](images/Screenshot%202024-08-14%20at%209.49.59%20PM.png){width=\"532\"}\n\n``` python\nindices = book_pivot_table.index # all the user IDs\n\nbook_rating_clustering = pd.DataFrame(data=book_pivot_table_tsvd, index=indices)\nprint(book_rating_clustering.shape)\nbook_rating_clustering.head(10)\n```\n\n![](images/Screenshot%202024-08-14%20at%209.50.59%20PM.png)\n\n``` python\ntrain_rate, test_rate = train_test_split(book_rating_clustering, test_size=0.2, random_state=42)\nprint(f'Traing set shape: {train_rate.shape}')\nprint('Testing set shape: {}'.format(test_rate.shape))\n```\n\n![](images/Screenshot%202024-08-14%20at%209.51.38%20PM.png){width=\"257\" height=\"43\"}\n\n``` python\ntest_rate.head()\n```\n\n![](images/Screenshot%202024-08-14%20at%209.54.21%20PM.png)\n\n``` python\nindices = test_rate.index\ntest_set_rating = book_pivot_table.loc[indices] # .loc[] for label-based indexing and .iloc[] for position-based indexing.\ntest_set_rating.head()\n```\n\n![](images/Screenshot%202024-08-14%20at%209.54.55%20PM.png)\n\n## Exploratory Data Analysis (EDA)\n\nBefore diving into model training, let's explore our data further. We'll start by examining the distribution of the number of rating counts across the books. By plotting a histogram, we can visualize how frequently different books have been rated.\n\n``` python\n# Group the data by each book and counts how many ratings each book has recieved\nbook_rating_counts = final_rating.groupby('Book-Title')['Book-Rating'].count()\n\n# Calculate how many books fall into each rating count category and then sorts these counts\nrating_frequencies = book_rating_counts.value_counts().sort_index()\n\n# Plotting the histogram\nplt.figure(figsize=(8, 4))\nplt.bar(rating_frequencies[:500].index, rating_frequencies[:500].values, color='lightgreen', edgecolor='darkgreen')\n\nplt.xlim(0, 250)\n\n# Adding titles and labels\nplt.title('Books by number of ratings recieved')\nplt.xlabel('Number of Rating Counts')\nplt.ylabel('Frequency of Books')\n\n# Display the plot\nplt.show()\n```\n\nFor example, below we can observe that around 25 books have received approximately 35 ratings each. The majority of books fall within the range of 25 to 75 total ratings, indicating that most books have moderate engagement from users. Interestingly, there are very few books with rating counts exceeding 150, highlighting the rarity of highly rated books in our dataset. This insight will help us better understand the characteristics of our data as we move forward.\n\n![](images/Screenshot%202024-08-15%20at%207.55.13%20PM.png)\n\nNext, let's examine the distribution of the number of ratings provided by users. This will help us understand user engagement in our dataset.\n\nIn the plot below, we observe the frequency of users based on the number of ratings they have given. For instance, there are approximately 20 users who have voted around 37 times. Most users have given between 0 to 100 ratings, showing that casual engagement is common. Notably, the number of users who have rated more than 100 times drops sharply, with the count decreasing from about 10 to almost just 2 users. This indicates that only a small fraction of users are highly active in rating books.\n\n``` python\n# Group the data by each user and counts how many ratings each user has given\nuser_rating_counts = final_rating.groupby('User-ID')['Book-Rating'].count()\n\n# Calculate how many users fall into each rating count category and then sorts these counts\nrating_frequencies = user_rating_counts.value_counts().sort_index()\n\n\n# Plotting the histogram\nplt.figure(figsize=(8, 4))\nplt.bar(rating_frequencies.index, rating_frequencies.values, color='lightgreen', edgecolor='darkgreen')\n\nplt.xlim(0, 200)\n\n# Adding titles and labels\nplt.title('Raters by number of books rated')\nplt.xlabel('Number of Ratings')\nplt.ylabel('Frequency of Users')\n\n# Display the plot\nplt.show()\n```\n\n![](images/Screenshot%202024-08-15%20at%207.57.15%20PM.png)\n\nNow let's talk about another graph: a boxplot, which is a statistical tool used to visualize the distribution of a dataset. Here are its essential components:\n\n1.  **Box**: The central part of the plot represents the interquartile range (IQR), containing the middle 50% of the data.\n\n2.  **Median Line**: A line within the box indicates the median (50th percentile) of the data.\n\n3.  **Whiskers**: Lines extending from the box to the smallest and largest values within 1.5 times the IQR from the lower and upper quartiles.\n\n4.  **Outliers**: Points outside the whiskers are considered outliers, indicating unusually high or low values.\n\n5.  **Quartiles**: The edges of the box represent the first quartile (Q1, 25th percentile) and the third quartile (Q3, 75th percentile).\n\nA boxplot efficiently illustrates the data's spread, central tendency, and potential outliers.\n\n![](boxplot.png){fig-align=\"center\"}\n\n``` python\n# Count the number of ratings per book\nbook_rating_counts = final_rating.groupby('Book-Title')['Book-Rating'].count().reset_index()\n\n# Rename columns for clarity\nbook_rating_counts.columns = ['Book-Title', 'Number of Ratings']\n\n# Plotting the boxplot\nplt.figure(figsize=(8, 4))\nsns.boxplot(x='Number of Ratings', data=book_rating_counts, color='lightgreen')\nplt.xlabel('Number of Ratings per Book')\nplt.title('Distribution of Number of Ratings per Book')\nplt.grid(True)\nplt.show()\n```\n\n![](images/Screenshot%202024-08-15%20at%207.58.11%20PM.png)\n\nSo in the plot above, we can observe that the majority of books received a bit over 100 ratings. Half of the books have at most 50 votes. Additionally, 75% of the books have up to \\~75 votes. The maximum value within our fourth quartile (Q4) lies between 100 and 120, indicating that most books do not receive more than 120 votes. The minimum rating a user can give is 1, and there are outliers on the right-hand side, displayed as gray dots. These represent individual books, with one book on the far right edge having a rating as high as 556.\n\n``` python\n# Count the number of ratings per user\nuser_rating_counts = final_rating.groupby('User-ID')['Book-Rating'].count().reset_index()\n\n# Rename columns for clarity\nuser_rating_counts.columns = ['User-ID', 'Number of Ratings']\n\n# Plotting the boxplot\nplt.figure(figsize=(8, 4))\nsns.boxplot(x='Number of Ratings', data=user_rating_counts, color='LightCoral')\nplt.xlabel('Number of Ratings per User')\nplt.title('Distribution of Number of Ratings per User')\nplt.grid(True)\nplt.show()\n```\n\n![](images/Screenshot%202024-08-15%20at%207.59.33%20PM.png)\n\nThe plot for users is more condensed, with one outlier (an individual) having rated more than 2,000 books! However, the majority of voters fall within the range of 1 to just under 250 votes. Half of the voters have cast at most 60 votes, and 75% have rated at most close to 100 times. This suggests that most users don't engage in extensive voting.\n\n## Training Machine Learning Model\n\nContent filtering, especially when dealing with a large number of users and items, requires significant computational resources. As the number of users (U) and items (I) increases, the computational demands grow nonlinearly, primarily due to the time-intensive process of calculating similarities. The time complexity for such operations can be expressed as O(U × I), where the similarity calculations dominate the computational time.\n\nTo address these challenges, we propose using content clustering combined with neighbors' voting. Clustering is a technique used to group similar data points into clusters. In the context of recommendation systems, clustering can help group items (e.g., books) that have similar patterns of user ratings. This reduces the computational load by allowing us to focus on clusters of similar items rather than individual item comparisons across the entire dataset.\n\nClustering is an unsupervised learning task, meaning we don't have labeled data to guide the process. Instead, we aim to discover hidden patterns and relationships within the dataset. Our objective is to group books based on user rankings without any predefined categories or labels. By identifying clusters of books that have similar rating patterns, we can better understand how these books relate to each other in the eyes of users. This understanding is crucial for building an effective recommendation system that can suggest relevant items to users based on their preferences.![](1*Ht06cKFv9S9XWCsnR2kSpw.gif)\n\nFor this task, we experiment with two clustering algorithms: K-Means and Gaussian Mixture Modeling (GMM). Both are unsupervised learning techniques used for clustering, but they handle the assignment of data points to clusters differently.\n\n### K-mean Clustering\n\n-   Hard Assignment: K-Means clustering operates under the assumption that each data point belongs to one and only one cluster.\n\n-   Determinate Clustering: At any given point in the algorithm, we are certain about the cluster assignment of each data point. For example, if a point is assigned to the red cluster, the algorithm is fully confident in that assignment. In subsequent iterations, this assignment might change, but the certainty remains; the point will be entirely assigned to a different cluster (e.g., green). This approach is known as a \"hard assignment.\"\n\n``` python\n# Fit the KMeans model\nclusterer_KMeans = KMeans(n_clusters=6, random_state=42).fit(train_rate)\n\n# Transform the data to get predictions\npreds_KMeans = clusterer_KMeans.predict(train_rate)\n\nunique_labels = np.unique(preds_KMeans)\nprint(f\"Number of clusters: {len(unique_labels)}\")\n\nKMeans_score = silhouette_score(train_rate, preds_KMeans)\nprint('Silhouette score for k-mean approach: ', KMeans_score)\n```\n\n![](images/Screenshot%202024-08-14%20at%209.57.37%20PM.png){width=\"458\" height=\"40\"}\n\n### Gaussian Mixture Modeling\n\n-   Soft Assignment: GMM, on the other hand, allows for uncertainty in cluster assignments. Instead of assigning a data point to a single cluster with full certainty, GMM provides probabilities for a point’s membership in multiple clusters.\n\n-   Probabilistic Clustering: For instance, a data point might have a 70% probability of belonging to the red cluster, a 10% probability of being in the green cluster, and a 20% probability of being in the blue cluster. This approach is known as a \"soft assignment,\" where the model expresses the degree of uncertainty in cluster membership.\n\n-   Iterative Refinement: GMM starts with a prior belief about the cluster assignments, and as the algorithm iterates, it continuously revises these probabilities, taking into account the uncertainty in each assignment.\n\n``` python\n# clustering books\nclusterer_GM = GaussianMixture(n_components=6, random_state=42).fit(train_rate)\npreds_GM = clusterer_GM.predict(train_rate)\n\nGM_score = silhouette_score(train_rate, preds_GM)\nprint('Silhouette score for Gaussian Mixture approach: ', GM_score)\n```\n\n![](images/Screenshot%202024-08-14%20at%209.58.13%20PM.png){width=\"509\"}\n\nSilhouette scores measure how similar an object is to its own cluster compared to other clusters, providing a way to assess the quality of clustering.\n\nIn the above code snippets we used Silhouette scores to compare the performance of K-Means and Gaussian Mixture Models (GMM). The score must range from -1 to 1, with higher scores indicating better clustering—where points are more closely grouped within their own clusters and well-separated from others.\n\nAs observed, GMM (score = 0.124) outperforms K-Means (score = 0.014), as it creates more distinct and well-defined clusters.\n\n``` python\nindices = train_rate.index\npreds = pd.DataFrame(data=preds_GM, columns=['cluster'], index=indices)\nprint(preds.shape)\nfor i in range(7):\n  print('cluster ', i+1, ':', preds[preds['cluster'] == i+1].count())\npreds.head()\n```\n\n![](images/Screenshot%202024-08-14%20at%209.59.04%20PM.png){width=\"323\"}\n\n## Prediction\n\n``` python\ntest_preds = clusterer_GM.predict(test_rate)\ntest_indices = test_rate.index\ntest_cluster = pd.DataFrame(data=test_preds, columns=['cluster'], index=test_indices)\n\nTest_GM_score = silhouette_score(test_rate, test_preds)\n\nprint(f'Silhouette Score for the test set is: {Test_GM_score}')\ntest_cluster.head()\n```\n\n![](images/Screenshot%202024-08-14%20at%2010.02.28%20PM.png){width=\"476\"}\n\n## Downloading Necessary Files\n\n``` python\npickle.dump(book_pivot_table, open('book_pivot_table.pkl', 'wb'))\npickle.dump(books_df, open('books.pkl', 'wb'))\npickle.dump(preds, open('predicted_clusters.pkl', 'wb'))\n```\n\n## Creating the App\n\nWe are using Flask, Flask is .... wehave been using it in blog \\# as well.\n\nThe mapping should look like this.\n\n``` texinfo\n├── myproject\n|   └── dataset\n|       └── Books.csv\n|       └── Ratings.csv\n│   └── models\n|       └── book_pivot_table.pkl\n|       └── books.pkl\n|       └── predicted_clusters.pkl\n|   └── static/css\n|       └── styles.css\n|   └── templates\n|       └── index.html\n|       └── recommendation.html\n|   └── utils.py\n|   └── app.py\n```\n\n### utils.py\n\nWe begin by collecting the name of the book that the user enjoys and wants to find similar recommendations for. Using this input, we then need our book-user table, which contains the entire dataset, as well as the predicted clusters from our clustering algorithm, and the number of recommendations the user desires.\n\nThe first step is to check if the given book exists in our dataset. Once validated, we identify the cluster to which the book belongs. With this cluster identified, we retrieve a list of all the books within that specific cluster. To find books similar in style to the one the user enjoys, we calculate the cosine similarity between the rankings of the given book and all the other books in the cluster. This similarity score helps us determine which books are closest in style and content based on user rankings. Finally, we select the top 10 books with the highest similarity scores, ensuring that the recommended books align closely with the user's preferences. This method leverages both clustering and similarity metrics to provide personalized and relevant book recommendations.\n\n``` python\nimport numpy as np\n\ndef recommending_books(book_name, book_pivot_table, preds, n_recommendations=10):\n    # Check if the book is in the pivot table\n    if book_name not in book_pivot_table.index:\n        return np.array([f\"The book '{book_name}' is not in the dataset :/\"])\n\n    # Find the cluster of the given book\n    book_cluster = preds.loc[book_name, 'cluster']\n\n    # Get all the books in the same cluster\n    cluster_books = preds[preds['cluster'] == book_cluster].index\n\n    # Calculate similarity scores within the cluster\n    book_vector = book_pivot_table.loc[book_name].values.reshape(1, -1)\n\n    cluster_vectors = book_pivot_table.loc[cluster_books].values\n\n    similarity_scores = np.dot(cluster_vectors, book_vector.T).flatten()\n\n    # Sort the books by similarity scores\n    similar_books_indices = np.argsort(-similarity_scores)[1:n_recommendations+1]  # Skip the first one as it's the book itself\n\n    similar_books = cluster_books[similar_books_indices]\n\n    return list(similar_books)\n```\n\n\n---\ntitle: foo\n---\n\n::: {#786d4166 .cell execution_count=1}\n``` {.python .cell-code}\nprint(\"Hello, world.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nHello, world.\n```\n:::\n:::\n\n\n``` {.python filename=\"Python\"}\nprint(\"Hello, world.\")\n```\n\n### app.py\n\n``` python\n# env: book_rec\n'''\nEnsuring that mamba is using the correct Python interpreter within your virtual environment (which python, which python3):\nexport PATH=\"/Users/toobarahimnia/miniforge3/envs/book_rec/bin:$PATH\"\n'''\nfrom flask import Flask,render_template,request\nimport pickle\nimport numpy as np\nfrom utils import recommending_books\n\n# rec_books = pickle.load(open('models/model 2/recommended_books.pkl','rb'))\npt = pickle.load(open('models/model 2/book_pivot_table.pkl','rb'))\nbook = pickle.load(open('models/model 2/books.pkl','rb'))\npreds = pickle.load(open('models/model 2/predicted_clusters.pkl','rb'))\n\napp = Flask(__name__)\n\n@app.route('/')\ndef index():\n    return render_template('index.html',\n                           book_name = list(book['Book-Title'][109:121].values), #101, 113\n                           image = list(book['Image-URL-M'][109:121].values),\n                           )\n\n@app.route('/recommendation')\ndef recommendation_ui():\n    return render_template('recommendation.html')\n\n@app.route('/recommend_books', methods=['post'])\ndef recommendation():\n    user_input = request.form.get('user_input')\n\n    recommendations = recommending_books(user_input, pt, preds, n_recommendations=10)\n    \n    # Check if the book is not found\n    if isinstance(recommendations, np.ndarray) and recommendations[0].startswith(\"The book\"):\n        return render_template('recommendation.html', error=recommendations[0])\n    \n    # Gather the data for rendering\n    data = []\n    for book_name in recommendations:\n        item = []\n        temp_df = book[book['Book-Title'] == book_name]\n        item.extend(list(temp_df.drop_duplicates('Book-Title')['Book-Title'].values))\n        item.extend(list(temp_df.drop_duplicates('Book-Title')['Image-URL-M'].values))\n        data.append(item)\n        print(item)\n    \n    return render_template('recommendation.html', data=data)\n    \n\nif __name__ == '__main__':\n    app.run(debug=True)\n```\n\n### Templates/Static\n\n::: panel-tabset\n## index.html\n\n``` html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <title>Book Recommendation Website</title>\n    <link rel=\"stylesheet\" href=\"{{ url_for('static', filename='css/styles.css') }}\">\n    <link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\">\n    <script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js\"></script>\n    <script src=\"https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.16.0/umd/popper.min.js\"></script>\n    <script src=\"https://maxcdn.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js\"></script>\n    <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css\">\n</head>\n<body style=\"background-color:#ffe4e1\">\n    <nav class=\"navbar\" style=\"background-color:currentColor\">\n        <!-- <a class=\"navbar-brand\" style=\"color: white; font-size: 30px\">Book Recommendation System</a> -->\n        <ul class=\"nav navbar-nav ml-auto\">\n            <li><a href=\"/\" style=\"color: #d8bfd8\" ><i class=\"fas fa-home\" ></i></a></li>\n        </ul>\n    </nav>\n\n    <div class=\"container\">\n        <div class=\"row\">\n            <div class=\"col-md-12\">\n                <h1 class=\"text-blue\" style=\"font-size: 45px; color:#4a0f6f; margin-top: 10px;margin-left:300px\"><a href=\"/recommendation\"> 💻 Type in a Book's Name... </a></h1>\n            </div>\n\n            <div class=\"container\" style=\"padding-bottom: 50px;\"> <!-- Add padding at the bottom -->\n                <div class=\"row\">\n                    {% for i in range(book_name|length) %}\n                        <div class=\"col-md-3\" style=\"margin-top:50px\">\n                        <div class=\"card\" style=\"width: 100%; height: 100%\">\n                            <div class=\"card-body\" style=\"font-family: emoji;padding: 1.02rem\">\n                                <img class=\"card-img-top\" src=\"{{image[i]}}\" alt=\"Book Image\"\n                                style=\"width: 100%; height: 70%; object-fit: cover;\">\n                                <h4 class=\"text-blue\" style=\"font-size: 20px; font-weight: bold; margin-top: 10px;\">{{book_name[i]}}</h4>\n                            </div>\n                        </div>\n                        </div>\n                    {% endfor %}\n                </div>\n            </div>    \n\n        </div>\n    </div>\n</body>\n</html>\n```\n\n## recommendation.html\n\n``` html\n<!DOCTYPE html>\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <title>Recommendation Site</title>\n    <link rel=\"stylesheet\" href=\"{{ url_for('static', filename='css/styles.css') }}\">\n    <link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\">\n    <script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js\"></script>\n    <script src=\"https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.16.0/umd/popper.min.js\"></script>\n    <script src=\"https://maxcdn.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js\"></script>\n    <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css\">\n</head>\n<body style=\"background-color:#ffe4e1\" >\n    <nav class=\"navbar\" style=\"background-color:currentColor\">\n        <!-- <a class=\"navbar-brand\" style=\"color: white; font-size: 30px\">Book Recommendation System</a> -->\n        <ul class=\"nav navbar-nav ml-auto\">\n            <li><a href=\"/\" style=\"color: #d8bfd8\"><i class=\"fas fa-home\"></i></a></li>\n        </ul>\n    </nav>\n\n    <div class=\"container\">\n        <div class=\"row\">\n            <div class=\"col-md-12\">\n                <h4 class=\"text-blue\" style=\"font-size: 35px; color:#4a0f6f; margin-top: 15px;\">Recommended Books</h4>\n                <form action=\"/recommend_books\" method=\"post\" style=\"margin-top: 20px;\">\n                    <input name=\"user_input\" type=\"text\" class=\"form-control\" placeholder=\"Enter a book title\">\n                    <br>\n                    <input type=\"submit\" class=\"btn btn-lg btn-warning\" style=\"background-color: #b0e0e6; color: #4a0f6f\" value=\"Get Recommendations\">\n                </form>\n            </div>\n        </div>\n        \n        <div class=\"container\" style=\"padding-bottom: 50px;\"> <!-- Add padding at the bottom -->\n            <div class=\"row\">\n                {% if data %}\n                <p class=\"text-blue\" style=\"font-size: 25px; margin-top: 15px;\">Based on your preference, we think you might like the following books! :)</p>\n                    <div class=\"row\">\n                        {% for i in data %}\n                            <div class=\"col-md-3\" style=\"margin-top:50px\">\n                                <div class=\"card\" style=\"width: 100%; height: 100%\">\n                                    <img class=\"card-img-top\" src={{i[1]}} alt=\"Book Image\">\n                                    <div class=\"card-body\">\n                                        <h4 class=\"text-blue\" style=\"font-size: 20px; font-weight: bold; margin-top: 10px;\">{{i[0]}}</h4>\n                                    </div>\n                                </div>\n                            </div>\n                        {% endfor %}\n                    </div>\n                {% endif %}\n            </div> \n        </div>       \n    </div>\n    \n    </div>\n</body>\n</html>\n```\n\n## styles.css\n\n``` css\n.card {\n    transition: transform 0.3s ease, box-shadow 0.3s ease;\n}\n.card:hover {\n    transform: scale(1.05); /* Scales up the card */\n    box-shadow: 0px 4px 20px rgba(0, 0, 0, 0.2); /* Adds a shadow effect */\n}\n.card-body {\n    padding: 1.02rem;\n    font-family: emoji;\n}\n.card-img-top {\n    width: 100%;\n    height: 70%;\n    object-fit: cover;\n}\n.text-blue {\n    color: #4a0f6f; /* Example color */\n}\n```\n:::\n\n### Virtual Environment Setup and Run\n\n## Results and Limitations\n\nDemo\n\n\n{{< video MyMovie.mp4 >}}\n\n\n\nLimitation on only predicting books inside the dataset.\n\n## Conclusion\n\n## Sources\n\nlink to Kaggle\n\nGitHub with all the codes\n\nUpdate Drive and local folder on mac and notion\n\nprune codes\n\nchecklist: read all codes, check all write ups again, check links, title, bio...\n\nhelpful links: source: <https://github.com/trupti0101/Book-recommender-system/blob/master/ML-Models/Collaborative_filtering.ipynb>\n\nmy kaggle: <https://www.kaggle.com/code/toobarahimnia/content-based-book-recommendation-system>\n\n**Building Flask Application**\n\nFlask is a popular web framework for Python that allows us to build web applications quickly and easily. We will use Flask to build a web application that takes an input image and predicts the class of the image using the trained model.\n\nOur Flask application will have two routes: a home route and a prediction route. The home route will display a simple HTML page with a form for uploading an image. The prediction route will take the uploaded image, preprocess the image, and make a prediction using the trained model. The prediction result will be displayed on a new page.\n\n**Conclusion**\n\nIn this blog post, we have gone through the process of building an end-to-end machine learning project. We started by acquiring a dataset from Kaggle, then we preprocessed the data and trained a machine learning model. Finally, we deployed the model as a web application using Flask.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}