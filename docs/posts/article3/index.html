<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.553">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Tina Rahimnia">
<meta name="dcterms.date" content="2024-06-05">

<title>CognitiveDiscoveries - Navigating the Kaggle U.S. Patent Phrase to Phrase Matching Competition: A Step-by-Step Guide</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">CognitiveDiscoveries</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/toobarahimnia"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://linkedin.com/in/tooba-rahimnia"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:trahimnia@gmail.com"> <i class="bi bi-envelope" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.youtube.com/@toobarahimnia9171"> <i class="bi bi-youtube" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Navigating the Kaggle U.S. Patent Phrase to Phrase Matching Competition: A Step-by-Step Guide</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">NLP</div>
                <div class="quarto-category">Tabular Data</div>
                <div class="quarto-category">Kaggle</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Tina Rahimnia </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">June 5, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#on-kaggle" id="toc-on-kaggle" class="nav-link active" data-scroll-target="#on-kaggle">On Kaggle</a></li>
  <li><a href="#importing-libraries-and-set-required-seed" id="toc-importing-libraries-and-set-required-seed" class="nav-link" data-scroll-target="#importing-libraries-and-set-required-seed">Importing libraries and Set Required Seed</a></li>
  <li><a href="#reading-datasets" id="toc-reading-datasets" class="nav-link" data-scroll-target="#reading-datasets">Reading Datasets</a></li>
  <li><a href="#eda-warm-up" id="toc-eda-warm-up" class="nav-link" data-scroll-target="#eda-warm-up">EDA Warm Up</a></li>
  <li><a href="#tokenization" id="toc-tokenization" class="nav-link" data-scroll-target="#tokenization">Tokenization</a></li>
  <li><a href="#loading-data" id="toc-loading-data" class="nav-link" data-scroll-target="#loading-data">Loading Data</a></li>
  <li><a href="#training-the-model" id="toc-training-the-model" class="nav-link" data-scroll-target="#training-the-model">Training the Model</a></li>
  <li><a href="#clear-the-memory" id="toc-clear-the-memory" class="nav-link" data-scroll-target="#clear-the-memory">Clear the Memory</a></li>
  <li><a href="#testing-the-model" id="toc-testing-the-model" class="nav-link" data-scroll-target="#testing-the-model">Testing the Model</a></li>
  <li><a href="#submission" id="toc-submission" class="nav-link" data-scroll-target="#submission">Submission</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>Working on the U.S. Patent Phrase to Phrase Matching Problem using Natural Language Processing (NLP) techniques to learn how to develop models that accurately match phrases based on semantic similarity.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="patent1.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<p>In recent years, natural Language Processing (NLP) has been quit a transformitive force in the field of artificial intelligence, driving advancements in deep learning models that are bale to understand, interpret, and generate human language with high accuracy and sofistication.</p>
<p>Undoubtedly, <em>classification</em> stands out as one of the most practical and useful applications of NLP. Countless use cases exist for classification tasks, including:</p>
<ul>
<li><p>Spam Email Detection: Classifying emails as either spam or non-spam (ham).</p></li>
<li><p>Sentiment Analysis: Classifying text documents (e.g., reviews, social media posts) as positive, negative, or neutral.</p></li>
<li><p>Topic Classification: Classifying news articles, research papers, or forum posts into predefined topics or categories (e.g., politics, sports, technology).</p></li>
<li><p>Legal Document Classification: Classifying legal documents based on their type (e.g., contracts, briefs, complaints).</p></li>
<li><p>Medical Document Classification: Classifying medical records or research papers into categories such as disease diagnosis, treatment plans, or medical history.</p></li>
</ul>
<p>Another less obvious use case is the Kaggle <a href="https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching/">U.S. Patent Phrase to Phrase Matching</a> competition. Here, the task involves comparing two words or phrases and assigning a score based on their relevance to each other. A score of <code>1</code> indicates identical meaning, while <code>0</code> indicates completely different meanings. For example, <em>abatement</em> and <em>eliminating process</em> might have a score of 0.5, indicating they are somewhat similar but not identical.</p>
<p>Note that this scenario can be likened to a classification problem, where we question:</p>
<blockquote class="blockquote">
<p>For the following text…: “TEXT1: abatement; TEXT2: eliminating process” …chose a category of meaning similarity: “Different; Similar; Identical”.</p>
</blockquote>
<p>In this post, we will provide a step-by-step guide on solving the Patent Phrase Matching problem. We will approach this task as a classification problem, representing it in a manner similar to the example described above.</p>
<section id="on-kaggle" class="level2">
<h2 class="anchored" data-anchor-id="on-kaggle">On Kaggle</h2>
<p>Before we dive into the code, there are a few steps to follow. On the competition page, first <em>accept the rules</em>, then click on <code>Code</code> from the top menu, then select <code>+ New Notebook</code>. You will see a cell with prewritten code, which you can delete as we will be writing all our code from scratch.</p>
<p>Next, on the right-hand sidebar, follow these steps:</p>
<ul>
<li><p><strong>Download the Models</strong>: We will be using two different models for this task (<code>debertav3base</code> and <code>debertav3small</code>). Click on <code>+ Add Input</code>, then under the Datasets category, type in the names of these two models one by one and download them.</p>
<p><img src="Screenshot 2024-06-05 at 2.44.07 PM.png" class="img-fluid" width="359"></p>
<p>(Side note: You can also try <code>deberta-v3-large</code> model later on, however, it requires more computing power and I personally do not recommend using it for the start)</p></li>
<li><p><strong>Download CPC Dataset</strong>: There is another dataset that we need to download. Follow the same steps as before and type <code>cpc-codes</code> to obtain the new CSV file. We will explore the information contained in this dataset in later sections.</p>
<p><img src="Screenshot 2024-06-05 at 2.47.46 PM.png" class="img-fluid" width="365"></p></li>
<li><p><strong>Turn on GPU</strong>: You will need GPUs for this task… Click on the three dots at the top right corner, then choose Accelerator, and select a GPU option.</p>
<p><img src="Screenshot 2024-06-05 at 2.53.11 PM.png" class="img-fluid" width="433"></p></li>
<li><p><strong>Turn off the Internet</strong>: Finally, you need to work in offline mode as required by the competition organizer. Once all your files are downloaded, open the left-side window, select Session options, and turn off the internet connection. (screenshot)</p>
<p><img src="Screenshot 2024-06-05 at 2.57.20 PM.png" class="img-fluid" width="332"></p></li>
</ul>
</section>
<section id="importing-libraries-and-set-required-seed" class="level2">
<h2 class="anchored" data-anchor-id="importing-libraries-and-set-required-seed">Importing libraries and Set Required Seed</h2>
<p>Now we are ready to start the exciting part. We’ll be using PyTorch and Transformers, along with Matplotlib for some exploratory data analysis (EDA).</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">import</span> transformers</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForSequenceClassification, AutoTokenizer</span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="im">import</span> torch</span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="im">from</span> torch <span class="im">import</span> optim</span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, Dataset, random_split</span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="im">from</span> torchmetrics.regression <span class="im">import</span> PearsonCorrCoef</span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-9"><a href="#cb1-9"></a><span class="im">import</span> random</span>
<span id="cb1-10"><a href="#cb1-10"></a><span class="im">import</span> timeit</span>
<span id="cb1-11"><a href="#cb1-11"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb1-12"><a href="#cb1-12"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb1-13"><a href="#cb1-13"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-14"><a href="#cb1-14"></a><span class="im">from</span> operator <span class="im">import</span> itemgetter</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Additionally, we’ll use a seed to ensure that the random numbers generated are reproducible. Reproducibility is essential for controlling randomness in both CPU and GPU operations, ensuring deterministic behavior. This is crucial for debugging and for ensuring that experiments can be replicated exactly.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a>RANDOM_SEED <span class="op">=</span> <span class="dv">42</span></span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="co">#MODEL_PATH = '/kaggle/input/debertav3base'</span></span>
<span id="cb2-3"><a href="#cb2-3"></a>MODEL_PATH <span class="op">=</span> <span class="st">'/kaggle/input/debertav3small'</span></span>
<span id="cb2-4"><a href="#cb2-4"></a>MAX_LENGTH <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb2-5"><a href="#cb2-5"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb2-6"><a href="#cb2-6"></a>LEARNING_RATE <span class="op">=</span> <span class="fl">2e-5</span></span>
<span id="cb2-7"><a href="#cb2-7"></a>EPOCHS <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb2-8"><a href="#cb2-8"></a></span>
<span id="cb2-9"><a href="#cb2-9"></a><span class="kw">def</span> set_random_seed(seed):</span>
<span id="cb2-10"><a href="#cb2-10"></a>    random.seed(seed)</span>
<span id="cb2-11"><a href="#cb2-11"></a>    np.random.seed(seed)</span>
<span id="cb2-12"><a href="#cb2-12"></a>    torch.manual_seed(seed)</span>
<span id="cb2-13"><a href="#cb2-13"></a>    torch.cuda.manual_seed_all(seed)</span>
<span id="cb2-14"><a href="#cb2-14"></a>    torch.backends.cudnn.deterministic <span class="op">=</span> <span class="va">True</span></span>
<span id="cb2-15"><a href="#cb2-15"></a>    torch.backends.cudnn.benchmark <span class="op">=</span> <span class="va">False</span></span>
<span id="cb2-16"><a href="#cb2-16"></a></span>
<span id="cb2-17"><a href="#cb2-17"></a>set_random_seed(RANDOM_SEED)</span>
<span id="cb2-18"><a href="#cb2-18"></a>device <span class="op">=</span> <span class="st">'cuda'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">'cpu'</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The code below helps control the amount of logging output by focusing only on error messages, which can be particularly useful for cleaner logs and debugging.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a>transformers.utils.logging.set_verbosity_error()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="reading-datasets" class="level2">
<h2 class="anchored" data-anchor-id="reading-datasets">Reading Datasets</h2>
<p>Now, let’s read our files:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a>train_df <span class="op">=</span> pd.read_csv(<span class="st">'/kaggle/input/us-patent-phrase-to-phrase-matching/train.csv'</span>)</span>
<span id="cb4-2"><a href="#cb4-2"></a></span>
<span id="cb4-3"><a href="#cb4-3"></a>test_df <span class="op">=</span> pd.read_csv(<span class="st">'/kaggle/input/us-patent-phrase-to-phrase-matching/test.csv'</span>)</span>
<span id="cb4-4"><a href="#cb4-4"></a></span>
<span id="cb4-5"><a href="#cb4-5"></a>submission_df <span class="op">=</span> pd.read_csv(<span class="st">'/kaggle/input/us-patent-phrase-to-phrase-matching/sample_submission.csv'</span>)</span>
<span id="cb4-6"><a href="#cb4-6"></a></span>
<span id="cb4-7"><a href="#cb4-7"></a>patents_df <span class="op">=</span> pd.read_csv(<span class="st">'/kaggle/input/cpc-codes/titles.csv'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="eda-warm-up" class="level2">
<h2 class="anchored" data-anchor-id="eda-warm-up">EDA Warm Up</h2>
<p>Let’s explore and extract some useful information from our datasets, first.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a>train_df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="train.png" class="img-fluid" width="483"></p>
<p>The above table tells us that the training set has five columns:</p>
<p><code>id</code> - a unique identifier for a pair of phrases</p>
<p><code>anchor</code> - the first phrase</p>
<p><code>target</code> - the second phrase</p>
<p><code>context</code> - the CPC classification, which indicates the subject within which the similarity is to be scored</p>
<p><code>score</code> - the similarity. This is sourced from a combination of one or more manual expert ratings.</p>
<p>What else can we get?</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a>train_df.score.hist()</span>
<span id="cb6-2"><a href="#cb6-2"></a>plt.title(<span class="st">'Histogram of Scores'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="Screenshot 2024-06-05 at 4.45.09 PM.png" class="img-fluid" width="492"></p>
<p>Note that there are only 5 potential scores:</p>
<p><strong>1.0</strong> - Represents a very close match, typically indicating an exact match except for possible differences in conjugation, quantity (e.g., singular vs.&nbsp;plural), and the addition or removal of stopwords (e.g., “the,” “and,” “or”).</p>
<p><strong>0.75</strong> - Indicates a close synonym, such as “mobile phone” vs.&nbsp;“cellphone.” This also includes abbreviations, such as “TCP” -&gt; “transmission control protocol.”</p>
<p><strong>0.5</strong> - Denotes synonyms that do not have the same meaning (same function, same properties). This category includes broad-narrow (hyponym) and narrow-broad (hypernym) matches.</p>
<p><strong>0.25</strong> - Indicates that the two phrases are somewhat related, such as being in the same high-level domain but not being synonyms. This category also includes antonyms.</p>
<p><strong>0.0</strong> - Represents phrases that are unrelated.</p>
<p>Now let’s have a closer look at the <code>anchor</code> column:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a>ax <span class="op">=</span> train_df.groupby(<span class="st">'anchor'</span>)[<span class="st">'id'</span>].count().sort_values(ascending<span class="op">=</span><span class="va">False</span>).head(<span class="dv">10</span>).plot(kind<span class="op">=</span><span class="st">'bar'</span>, color<span class="op">=</span><span class="bu">list</span>(<span class="st">'gbymcr'</span>))</span>
<span id="cb7-2"><a href="#cb7-2"></a><span class="cf">for</span> container <span class="kw">in</span> ax.containers:</span>
<span id="cb7-3"><a href="#cb7-3"></a>    ax.bar_label(container)</span>
<span id="cb7-4"><a href="#cb7-4"></a>plt.title(<span class="st">'Top 10 Anchors'</span>)</span>
<span id="cb7-5"><a href="#cb7-5"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="Screenshot 2024-06-05 at 4.51.21 PM.png" class="img-fluid" width="560"></p>
<p>Here, we can see the top 10 mostly used <code>anchors</code>, respectively. The total number of entities containing each anchor is written on top of its bar.</p>
<p>The test dataset is quite similar to the train dataset, except that it does not have the score column.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a>test_df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="test.png" class="img-fluid" width="484"></p>
<p>And our submission file at the end should look like this:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a>submission_df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="submit.png" class="img-fluid" width="219"></p>
<p>The only dataset left is <code>patents_df</code>. But what information does it have inside? Let’s have a look.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a>patents_df.head() </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="patents_df.png" class="img-fluid" width="683"></p>
<p>Remember that earlier we suggested that we could represent the input to the model as something like “<em>TEXT1: abatement; TEXT2: eliminating process</em>”. We’ll need to apply that context here.</p>
<p>To begin, merge <code>train_df</code> and <code>patent_df</code> on the shared column: notice that <code>train_df['context']</code> and <code>patent_df['code']</code> contain similar information on both tables, so they’ll be the point of juncture.</p>
<p>next step, we will create a new column (<code>input)</code> on the new training dataset (<code>updated_train_df)</code>. It combines <code>updated_train_df['title']</code> and <code>updated_train_df['anchor']</code>.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a>updated_train_df <span class="op">=</span> train_df.merge(patents_df, left_on<span class="op">=</span><span class="st">'context'</span>, right_on<span class="op">=</span><span class="st">'code'</span>) </span>
<span id="cb11-2"><a href="#cb11-2"></a>updated_train_df[<span class="st">'input'</span>] <span class="op">=</span> updated_train_df[<span class="st">'title'</span>] <span class="op">+</span> <span class="st">"; "</span> <span class="op">+</span> updated_train_df[<span class="st">'anchor'</span>]</span>
<span id="cb11-3"><a href="#cb11-3"></a>updated_train_df.tail()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="Screenshot 2024-06-05 at 5.47.05 PM.png" class="img-fluid"></p>
<p>Do the same steps for <code>test_df</code>:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a>updated_test_df <span class="op">=</span> test_df.merge(patents_df, left_on<span class="op">=</span><span class="st">'context'</span>, right_on<span class="op">=</span><span class="st">'code'</span>)</span>
<span id="cb12-2"><a href="#cb12-2"></a>updated_test_df[<span class="st">'input'</span>] <span class="op">=</span> updated_test_df[<span class="st">'title'</span>] <span class="op">+</span> <span class="st">" "</span> <span class="op">+</span> updated_test_df[<span class="st">'anchor'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="tokenization" class="level2">
<h2 class="anchored" data-anchor-id="tokenization">Tokenization</h2>
<p>Now it’s time to tokenize our datasets… But what is it exactly?</p>
<blockquote class="blockquote">
<p>Tokenization, in the realm of Natural Language Processing (NLP) and machine learning, refers to the process of converting a sequence of text into smaller parts, known as tokens.</p>
</blockquote>
<p>In other words, we can’t pass the texts directly into a model. A deep learning model expects numbers as inputs, not English sentences! So we need to do two things:</p>
<ul>
<li><p><em>Tokenization</em>: Split each text up into <em>tokens</em></p></li>
<li><p><em>Numericalization</em>: Convert each token into a number.</p></li>
</ul>
<p>The details about how this is done depend on the particular model we choose. There are numerous models available, but for the starting point we will be using the two models that we have downloaded earlier.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(MODEL_PATH)</span>
<span id="cb13-2"><a href="#cb13-2"></a>model <span class="op">=</span> AutoModelForSequenceClassification.from_pretrained(MODEL_PATH, num_labels<span class="op">=</span><span class="dv">1</span>).to(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The <code>num_labels=1</code> basically turns it into a regression problem, as to output only one score.</p>
<p>You can safely ignore the warnings here.</p>
</section>
<section id="loading-data" class="level2">
<h2 class="anchored" data-anchor-id="loading-data">Loading Data</h2>
<p>In the following cells we will build our custom dataset classes for training and testing that inherit from <code>torch.utils.data.Dataset</code>.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a><span class="kw">class</span> TrainDataset(Dataset):</span>
<span id="cb14-2"><a href="#cb14-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, inputs, targets, scores, tokenizer):</span>
<span id="cb14-3"><a href="#cb14-3"></a>        <span class="va">self</span>.scores <span class="op">=</span> scores</span>
<span id="cb14-4"><a href="#cb14-4"></a>        <span class="va">self</span>.encodings <span class="op">=</span> tokenizer(inputs, </span>
<span id="cb14-5"><a href="#cb14-5"></a>                                   targets, </span>
<span id="cb14-6"><a href="#cb14-6"></a>                                   padding<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb14-7"><a href="#cb14-7"></a>                                   truncation<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb14-8"><a href="#cb14-8"></a>                                   max_length<span class="op">=</span>MAX_LENGTH)</span>
<span id="cb14-9"><a href="#cb14-9"></a>    </span>
<span id="cb14-10"><a href="#cb14-10"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb14-11"><a href="#cb14-11"></a>        out_dic <span class="op">=</span> {key: torch.tensor(val[idx]) <span class="cf">for</span> key, val <span class="kw">in</span> <span class="va">self</span>.encodings.items()}</span>
<span id="cb14-12"><a href="#cb14-12"></a>        out_dic[<span class="st">'scores'</span>] <span class="op">=</span> <span class="va">self</span>.scores[idx]</span>
<span id="cb14-13"><a href="#cb14-13"></a>        <span class="cf">return</span> out_dic</span>
<span id="cb14-14"><a href="#cb14-14"></a>        </span>
<span id="cb14-15"><a href="#cb14-15"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb14-16"><a href="#cb14-16"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.scores)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a><span class="kw">class</span> SubmitDataset(Dataset):</span>
<span id="cb15-2"><a href="#cb15-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, inputs, targets, ids, tokenizer):</span>
<span id="cb15-3"><a href="#cb15-3"></a>        <span class="va">self</span>.ids <span class="op">=</span> ids</span>
<span id="cb15-4"><a href="#cb15-4"></a>        <span class="va">self</span>.encodings <span class="op">=</span> tokenizer(inputs, </span>
<span id="cb15-5"><a href="#cb15-5"></a>                                   targets, </span>
<span id="cb15-6"><a href="#cb15-6"></a>                                   padding<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb15-7"><a href="#cb15-7"></a>                                   truncation<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb15-8"><a href="#cb15-8"></a>                                   max_length<span class="op">=</span>MAX_LENGTH)</span>
<span id="cb15-9"><a href="#cb15-9"></a>    </span>
<span id="cb15-10"><a href="#cb15-10"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb15-11"><a href="#cb15-11"></a>        out_dic <span class="op">=</span> {key: torch.tensor(val[idx]) <span class="cf">for</span> key, val <span class="kw">in</span> <span class="va">self</span>.encodings.items()}</span>
<span id="cb15-12"><a href="#cb15-12"></a>        out_dic[<span class="st">'ids'</span>] <span class="op">=</span> <span class="va">self</span>.ids[idx]</span>
<span id="cb15-13"><a href="#cb15-13"></a>        <span class="cf">return</span> out_dic</span>
<span id="cb15-14"><a href="#cb15-14"></a>        </span>
<span id="cb15-15"><a href="#cb15-15"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb15-16"><a href="#cb15-16"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.ids)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Taking <code>TrainDataset</code> for instance, let’s break it down into sections and see what it entails:</p>
<ol type="1">
<li><p>Initialization (<code>__init__</code> method):</p>
<ul>
<li><p>Parameters:</p>
<ul>
<li><p><code>inputs</code>: The input texts to be tokenized.</p></li>
<li><p><code>targets</code>: The target texts or labels to be tokenized.</p></li>
<li><p><code>scores</code>: A list or array of scores corresponding to the inputs and targets.</p></li>
<li><p><code>tokenizer</code>: A tokenizer object (such as one from the Hugging Face Transformers library).</p></li>
</ul></li>
<li><p>Attributes:</p>
<ul>
<li><p><code>self.scores</code>: Stores the scores parameter for later use in <code>__getitem__</code>.</p></li>
<li><p><code>self.encodings</code>: Stores the tokenized inputs and targets. The tokenizer processes the inputs and targets, applying padding, truncation, and limiting the length to <code>MAX_LENGTH</code>.</p></li>
</ul></li>
</ul></li>
<li><p>Getting an Item (<code>__getitem__</code> method):</p>
<ul>
<li><p>Parameters:&nbsp;</p>
<ul>
<li><code>idx</code>: The index of the item to retrieve.</li>
</ul></li>
<li><p>Functionality:</p>
<ul>
<li><p>Creates a dictionary <code>out_dic</code> where each key-value pair from <code>self.encodings</code> is converted into a tensor. The value at index idx for each key is taken.&nbsp;</p></li>
<li><p>Adds the score at index idx from <code>self.scores</code> to <code>out_dic</code>.&nbsp;</p></li>
<li><p>Returns <code>out_dic</code>, which now contains the tokenized input and target data for the given index, along with the corresponding score.</p></li>
</ul></li>
</ul></li>
<li><p>Length of Dataset (<code>__len__</code> method):</p>
<ul>
<li>Returns the length of the dataset, which is determined by the number of scores. This assumes that the length of <code>scores</code> corresponds to the number of data points in the dataset.</li>
</ul></li>
</ol>
<p>After tokenizing, let’s check our datasets:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a>dataset <span class="op">=</span> TrainDataset(updated_train_df[<span class="st">'input'</span>].to_list(), </span>
<span id="cb16-2"><a href="#cb16-2"></a>                       updated_train_df[<span class="st">'target'</span>].to_list(), </span>
<span id="cb16-3"><a href="#cb16-3"></a>                       updated_train_df[<span class="st">'score'</span>].to_list(), </span>
<span id="cb16-4"><a href="#cb16-4"></a>                       tokenizer)</span>
<span id="cb16-5"><a href="#cb16-5"></a></span>
<span id="cb16-6"><a href="#cb16-6"></a>test_dataset <span class="op">=</span> SubmitDataset(updated_test_df[<span class="st">'input'</span>].to_list(), </span>
<span id="cb16-7"><a href="#cb16-7"></a>                             updated_test_df[<span class="st">'target'</span>].to_list(), </span>
<span id="cb16-8"><a href="#cb16-8"></a>                             updated_test_df[<span class="st">'id'</span>].to_list(), </span>
<span id="cb16-9"><a href="#cb16-9"></a>                             tokenizer)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><img src="Screenshot 2024-06-05 at 7.32.19 PM.png" class="img-fluid"></p>
<p>There it is! Every input/output entity turned into a token and then into a number.</p>
<p>Now we split our data into train and validation sets.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a>generator <span class="op">=</span> torch.Generator().manual_seed(RANDOM_SEED)</span>
<span id="cb17-2"><a href="#cb17-2"></a>train_dataset, val_dataset <span class="op">=</span> random_split(dataset, [<span class="fl">0.8</span>, <span class="fl">0.2</span>], generator<span class="op">=</span>generator)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We use DataLoader after having our datasets to efficiently load and process the data in batches during training, validation, and testing of machine learning models. Therefore, it is easier to handle large datasets that don’t fit into memory all at once. It also provides additional functionalities like shuffling and batching, which are essential for training machine learning models effectively.</p>
<p>(Tip: Always shuffle=True on the training set and shuffle=False on the validation set and test set. [<a href="https://stackoverflow.com/questions/61702753/why-is-making-shuffle-false-on-validation-set-giving-better-results-in-confusion">source</a>])</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a>train_dataloader <span class="op">=</span> DataLoader(dataset<span class="op">=</span>train_dataset, </span>
<span id="cb18-2"><a href="#cb18-2"></a>                             batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb18-3"><a href="#cb18-3"></a>                             shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb18-4"><a href="#cb18-4"></a></span>
<span id="cb18-5"><a href="#cb18-5"></a>val_dataloader <span class="op">=</span> DataLoader(dataset<span class="op">=</span>val_dataset, </span>
<span id="cb18-6"><a href="#cb18-6"></a>                             batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb18-7"><a href="#cb18-7"></a>                             shuffle<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb18-8"><a href="#cb18-8"></a></span>
<span id="cb18-9"><a href="#cb18-9"></a>test_dataloader <span class="op">=</span> DataLoader(dataset<span class="op">=</span>test_dataset, </span>
<span id="cb18-10"><a href="#cb18-10"></a>                             batch_size<span class="op">=</span>BATCH_SIZE,</span>
<span id="cb18-11"><a href="#cb18-11"></a>                             shuffle<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="training-the-model" class="level2">
<h2 class="anchored" data-anchor-id="training-the-model">Training the Model</h2>
<p>Now it’s time to train our model. Notice that I am using <code>/kaggle/input/debertav3small</code> (let’s call it Model 1) as the model path. We will go ahead with training this model first, then we can change the path to <code>/kaggle/input/debertav3base</code> (we refer to this as Model 2) and compare the results.</p>
<p>Below we train our model using the AdamW optimizer and evaluate it on the validation dataset. We then compute the Pearson correlation coefficient between the predicted scores and the ground truth scores.</p>
<ol type="1">
<li><p>Optimization Initialization:</p>
<ul>
<li>Initialize the AdamW optimizer, which is a variant of the Adam optimizer with weight decay (W stands for weight decay).</li>
</ul></li>
<li><p>PearsonCorrCoef Initialization:</p>
<ul>
<li>Initialize an object of the <code>PearsonCorrCoef</code> class.</li>
</ul>
<blockquote class="blockquote">
<p>The Pearson correlation coefficient (r) is a number between –1 and 1 that measures the strength and direction of the relationship between two variables. When one variable changes, the other variable changes in the same direction.</p>
</blockquote></li>
<li><p>Training Loop:</p>
<ul>
<li>Iterate over the specified number of epochs (EPOCHS).</li>
<li>For each epoch:
<ul>
<li>Set the model to training mode (model.train()).</li>
<li>Iterate over batches of data in the training dataset (train_dataloader).
<ul>
<li>Move input data and targets to the device (e.g., GPU).</li>
<li>Compute the model outputs and loss.</li>
<li>Compute gradients and performs a parameter update (optimizer step).</li>
<li>Accumulate the training loss.</li>
</ul></li>
<li>Compute the average training loss for the epoch.</li>
<li>Set the model to evaluation mode (model.eval()).</li>
<li>Iterate over batches of data in the validation dataset (val_dataloader).
<ul>
<li>Compute the model outputs without gradient computation (since no training is performed).</li>
<li>Accumulate the validation loss.</li>
<li>Extend preds and golds lists with predicted scores and ground truth scores, respectively.</li>
</ul></li>
<li>Compute the average validation loss for the epoch.</li>
<li>Compute and print the Pearson correlation coefficient between predicted scores (preds) and ground truth scores (golds).</li>
</ul></li>
</ul>
<p>(Side note: outputs[‘logits’].squeeze() would return the logits with any singleton dimensions removed, converting them into a 1D array if they were originally a 2D array with a singleton dimension.)</p></li>
<li><p>Timing and Printing:</p>
<ul>
<li>Compute and prints the training time.</li>
</ul></li>
</ol>
<div class="sourceCode" id="cb19"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a>optimizer <span class="op">=</span> optim.AdamW(model.parameters(), lr<span class="op">=</span>LEARNING_RATE)</span>
<span id="cb19-2"><a href="#cb19-2"></a>pearson <span class="op">=</span> PearsonCorrCoef()</span>
<span id="cb19-3"><a href="#cb19-3"></a>start <span class="op">=</span> timeit.default_timer()</span>
<span id="cb19-4"><a href="#cb19-4"></a></span>
<span id="cb19-5"><a href="#cb19-5"></a><span class="cf">for</span> epoch <span class="kw">in</span> tqdm(<span class="bu">range</span>(EPOCHS), position<span class="op">=</span><span class="dv">0</span>, leave<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb19-6"><a href="#cb19-6"></a>    model.train()</span>
<span id="cb19-7"><a href="#cb19-7"></a>    train_running_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb19-8"><a href="#cb19-8"></a>    </span>
<span id="cb19-9"><a href="#cb19-9"></a>    <span class="cf">for</span> idx, sample <span class="kw">in</span> <span class="bu">enumerate</span>(tqdm(train_dataloader, position<span class="op">=</span><span class="dv">0</span>, leave<span class="op">=</span><span class="va">True</span>)):</span>
<span id="cb19-10"><a href="#cb19-10"></a>        input_ids <span class="op">=</span> sample[<span class="st">'input_ids'</span>].to(device) <span class="co"># text1</span></span>
<span id="cb19-11"><a href="#cb19-11"></a>        attention_mask <span class="op">=</span> sample[<span class="st">'attention_mask'</span>].to(device) <span class="co"># text2</span></span>
<span id="cb19-12"><a href="#cb19-12"></a>        targets <span class="op">=</span> sample[<span class="st">'scores'</span>].to(device) <span class="co"># relativeness</span></span>
<span id="cb19-13"><a href="#cb19-13"></a>        </span>
<span id="cb19-14"><a href="#cb19-14"></a>        outputs <span class="op">=</span> model(input_ids<span class="op">=</span>input_ids, </span>
<span id="cb19-15"><a href="#cb19-15"></a>                        attention_mask<span class="op">=</span>attention_mask, </span>
<span id="cb19-16"><a href="#cb19-16"></a>                        labels<span class="op">=</span>targets)</span>
<span id="cb19-17"><a href="#cb19-17"></a>        </span>
<span id="cb19-18"><a href="#cb19-18"></a>        loss <span class="op">=</span> outputs.loss</span>
<span id="cb19-19"><a href="#cb19-19"></a>        optimizer.zero_grad()</span>
<span id="cb19-20"><a href="#cb19-20"></a>        loss.backward()</span>
<span id="cb19-21"><a href="#cb19-21"></a>        optimizer.step()</span>
<span id="cb19-22"><a href="#cb19-22"></a>        train_running_loss <span class="op">+=</span> loss.item()</span>
<span id="cb19-23"><a href="#cb19-23"></a>        </span>
<span id="cb19-24"><a href="#cb19-24"></a>    train_loss <span class="op">=</span> train_running_loss <span class="op">/</span> (idx <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb19-25"><a href="#cb19-25"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb19-26"><a href="#cb19-26"></a>    val_running_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb19-27"><a href="#cb19-27"></a>    preds <span class="op">=</span> []</span>
<span id="cb19-28"><a href="#cb19-28"></a>    golds <span class="op">=</span> []</span>
<span id="cb19-29"><a href="#cb19-29"></a>    </span>
<span id="cb19-30"><a href="#cb19-30"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb19-31"><a href="#cb19-31"></a>        <span class="cf">for</span> idx, sample <span class="kw">in</span> <span class="bu">enumerate</span>(tqdm(val_dataloader, position<span class="op">=</span><span class="dv">0</span>, leave<span class="op">=</span><span class="va">True</span>)):</span>
<span id="cb19-32"><a href="#cb19-32"></a>            input_ids <span class="op">=</span> sample[<span class="st">'input_ids'</span>].to(device)</span>
<span id="cb19-33"><a href="#cb19-33"></a>            attention_mask <span class="op">=</span> sample[<span class="st">'attention_mask'</span>].to(device)</span>
<span id="cb19-34"><a href="#cb19-34"></a>            targets <span class="op">=</span> sample[<span class="st">'scores'</span>].to(device)</span>
<span id="cb19-35"><a href="#cb19-35"></a>            </span>
<span id="cb19-36"><a href="#cb19-36"></a>            outputs <span class="op">=</span> model(input_ids<span class="op">=</span>input_ids, </span>
<span id="cb19-37"><a href="#cb19-37"></a>                            attention_mask<span class="op">=</span>attention_mask, </span>
<span id="cb19-38"><a href="#cb19-38"></a>                            labels<span class="op">=</span>targets)</span>
<span id="cb19-39"><a href="#cb19-39"></a>            </span>
<span id="cb19-40"><a href="#cb19-40"></a>            preds.extend([<span class="bu">float</span>(i) <span class="cf">for</span> i <span class="kw">in</span> outputs[<span class="st">'logits'</span>].squeeze()])</span>
<span id="cb19-41"><a href="#cb19-41"></a>            golds.extend([<span class="bu">float</span>(i) <span class="cf">for</span> i <span class="kw">in</span> targets])</span>
<span id="cb19-42"><a href="#cb19-42"></a>            </span>
<span id="cb19-43"><a href="#cb19-43"></a>            val_running_loss <span class="op">+=</span> outputs.loss.item()</span>
<span id="cb19-44"><a href="#cb19-44"></a>        val_loss <span class="op">=</span> val_running_loss <span class="op">/</span> (idx <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb19-45"><a href="#cb19-45"></a>    </span>
<span id="cb19-46"><a href="#cb19-46"></a>    <span class="co"># printing out results </span></span>
<span id="cb19-47"><a href="#cb19-47"></a>    <span class="bu">print</span>(<span class="st">'-'</span> <span class="op">*</span> <span class="dv">30</span>)</span>
<span id="cb19-48"><a href="#cb19-48"></a>    <span class="bu">print</span>(<span class="ss">f'Pearson Score: </span><span class="sc">{</span><span class="bu">float</span>(pearson(torch.tensor(preds), torch.tensor(golds)))<span class="sc">:.3f}</span><span class="ss">'</span>)</span>
<span id="cb19-49"><a href="#cb19-49"></a>    <span class="bu">print</span>(<span class="ss">f'Train Loss EPOCH </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>train_loss<span class="sc">:.3f}</span><span class="ss">'</span>)</span>
<span id="cb19-50"><a href="#cb19-50"></a>    <span class="bu">print</span>(<span class="ss">f'Valid Loss EPOCH </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>val_loss<span class="sc">:.3f}</span><span class="ss">'</span>)</span>
<span id="cb19-51"><a href="#cb19-51"></a>    <span class="bu">print</span>(<span class="st">'-'</span> <span class="op">*</span> <span class="dv">30</span>)</span>
<span id="cb19-52"><a href="#cb19-52"></a>    </span>
<span id="cb19-53"><a href="#cb19-53"></a>stop <span class="op">=</span> timeit.default_timer()</span>
<span id="cb19-54"><a href="#cb19-54"></a><span class="bu">print</span>(<span class="ss">f'Training Time: </span><span class="sc">{</span>stop<span class="op">-</span>start<span class="sc">: .2f}</span><span class="ss">s'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Let’s compare the performance of Model 1 (left side) and Model 2 (right side).</p>
<p><img src="Screenshot 2024-06-05 at 8.44.01 PM.png" class="img-fluid"></p>
<p>As you can see, Model 2 is more accurate than Model 1. It performs 4.6% better on the first epoch and 2.8% better on the second epoch. The training and validation loss of Model 2 is almost half of Model 1.</p>
<p>However, the accuracy comes with the price of consumed time. Model 2 takes a bit over double the time of Model 1 to train. So, to conclude:</p>
<ul>
<li><p>Model 1 is faster and less accurate</p></li>
<li><p>Model 2 is slower and more accurate</p></li>
</ul>
<p>What other models can we use to improve the performance up to 90% or even more? Perhaps one option would be to use Microsoft’s DEBERa-v3-large. What other suggestions you might have?</p>
</section>
<section id="clear-the-memory" class="level2">
<h2 class="anchored" data-anchor-id="clear-the-memory">Clear the Memory</h2>
<p>We are close to the end. Make sure to free up your memory at this point.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1"></a>torch.cuda.empty_cache()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="testing-the-model" class="level2">
<h2 class="anchored" data-anchor-id="testing-the-model">Testing the Model</h2>
<div class="sourceCode" id="cb21"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a>preds <span class="op">=</span> []</span>
<span id="cb21-2"><a href="#cb21-2"></a>ids <span class="op">=</span> []</span>
<span id="cb21-3"><a href="#cb21-3"></a>model.<span class="bu">eval</span>()</span>
<span id="cb21-4"><a href="#cb21-4"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb21-5"><a href="#cb21-5"></a>    <span class="cf">for</span> idx, sample <span class="kw">in</span> <span class="bu">enumerate</span>(tqdm(test_dataloader)):</span>
<span id="cb21-6"><a href="#cb21-6"></a>        input_ids <span class="op">=</span> sample[<span class="st">'input_ids'</span>].to(device)</span>
<span id="cb21-7"><a href="#cb21-7"></a>        attention_mask <span class="op">=</span> sample[<span class="st">'attention_mask'</span>].to(device)</span>
<span id="cb21-8"><a href="#cb21-8"></a>        ids.extend(sample[<span class="st">"ids"</span>])</span>
<span id="cb21-9"><a href="#cb21-9"></a>        </span>
<span id="cb21-10"><a href="#cb21-10"></a>        outputs <span class="op">=</span> model(input_ids<span class="op">=</span>input_ids, attention_mask<span class="op">=</span>attention_mask)</span>
<span id="cb21-11"><a href="#cb21-11"></a>        preds.extend([<span class="bu">float</span>(i) <span class="cf">for</span> i <span class="kw">in</span> outputs[<span class="st">"logits"</span>].squeeze()])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="submission" class="level2">
<h2 class="anchored" data-anchor-id="submission">Submission</h2>
<p>Congratulations on making it this far! Your code is now ready to be submitted. Save your predicted scores in a <em>.</em>csv file and submit the results. Kaggle will use a hidden testing dataset to evaluate your model. You will then be able to see how well your model performs on these unseen datasets.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1"></a>submission_df <span class="op">=</span> pd.DataFrame(<span class="bu">list</span>(<span class="bu">zip</span>(ids, preds)),</span>
<span id="cb22-2"><a href="#cb22-2"></a>               columns <span class="op">=</span>[<span class="st">"id"</span>, <span class="st">"score"</span>])</span>
<span id="cb22-3"><a href="#cb22-3"></a>submission_df.to_csv(<span class="st">"submission.csv"</span>, index<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb22-4"><a href="#cb22-4"></a>submission_df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Thank you for sticking with me up to this point. This has been my first time working on an NLP case competition, and it’s also my first time writing about it. I hope it provided some value to you. Let’s continue practicing and improving together!</p>
<p>To access my complete code for this Kaggle competition, click <a href="https://www.kaggle.com/code/toobarahimnia/patent-phrase-to-phrase-matching">here</a>.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>