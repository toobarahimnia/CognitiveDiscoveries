<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.553">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Tooba Rahimnia">
<meta name="dcterms.date" content="2024-06-20">

<title>CognitiveDiscoveries - How to Build a Music Genre Classifier Using Hugging Face Transformers and Gradio</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">CognitiveDiscoveries</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/toobarahimnia"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">How to Build a Music Genre Classifier Using Hugging Face Transformers and Gradio</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Code</div>
                <div class="quarto-category">Analysis</div>
                <div class="quarto-category">Audio Signal Processing</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Tooba Rahimnia </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">June 20, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#load-and-prepare-the-dataset" id="toc-load-and-prepare-the-dataset" class="nav-link active" data-scroll-target="#load-and-prepare-the-dataset">Load and Prepare the Dataset</a></li>
  <li><a href="#generate-audio-samples-with-gradio" id="toc-generate-audio-samples-with-gradio" class="nav-link" data-scroll-target="#generate-audio-samples-with-gradio">Generate Audio Samples with Gradio</a>
  <ul class="collapse">
  <li><a href="#sampling-and-sampling-rate" id="toc-sampling-and-sampling-rate" class="nav-link" data-scroll-target="#sampling-and-sampling-rate">Sampling and Sampling Rate</a></li>
  <li><a href="#amplitude" id="toc-amplitude" class="nav-link" data-scroll-target="#amplitude">Amplitude</a></li>
  <li><a href="#gradio" id="toc-gradio" class="nav-link" data-scroll-target="#gradio">Gradio</a></li>
  </ul></li>
  <li><a href="#feature-extraction" id="toc-feature-extraction" class="nav-link" data-scroll-target="#feature-extraction">Feature Extraction</a></li>
  <li><a href="#preprocess-the-dataset" id="toc-preprocess-the-dataset" class="nav-link" data-scroll-target="#preprocess-the-dataset">Preprocess the Dataset</a></li>
  <li><a href="#prepare-labels" id="toc-prepare-labels" class="nav-link" data-scroll-target="#prepare-labels">Prepare Labels</a></li>
  <li><a href="#load-and-fine-tune-the-model" id="toc-load-and-fine-tune-the-model" class="nav-link" data-scroll-target="#load-and-fine-tune-the-model">Load and Fine-tune the Model</a></li>
  <li><a href="#training-and-evaluation" id="toc-training-and-evaluation" class="nav-link" data-scroll-target="#training-and-evaluation">Training and Evaluation</a></li>
  <li><a href="#inference" id="toc-inference" class="nav-link" data-scroll-target="#inference">Inference</a></li>
  <li><a href="#gradio-demo" id="toc-gradio-demo" class="nav-link" data-scroll-target="#gradio-demo">Gradio Demo</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>In this post, we will build a music genre classification system using the GTZAN dataset to identify the genre of a given audio track.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="pexels-pixabay-164697.jpg" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<p>Have you ever been curious about how machine learning models classify music genres? What features in the dataset are useful for the modelâ€™s understanding? And how can you deploy your trained model for users? If these questions have crossed your mind, then keep reading as I guide you through everything you need to quickly deploy a music classification app. By the end of this post, you will have a fully functional music genre classification model capable of predicting the genre of any audio track. You will also have a Gradio-based interactive interface to test and visualize the modelâ€™s predictions, and the model will be ready for deployment using the Hugging Face Hub.</p>
<section id="load-and-prepare-the-dataset" class="level2">
<h2 class="anchored" data-anchor-id="load-and-prepare-the-dataset">Load and Prepare the Dataset</h2>
<p>We start by loading the GTZAN dataset using the <code>datasets</code> library and split it into training and test sets. The reason for using GTZAN is that itâ€™s a popular dataset containing 1,000 songs for music genre classification. Each song is a 30-second clip from one of 10 genres of music, spanning blues to rock.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="cb1-2"><a href="#cb1-2"></a></span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="co"># Load the GTZAN dataset</span></span>
<span id="cb1-4"><a href="#cb1-4"></a>gtzan <span class="op">=</span> load_dataset(<span class="st">'marsyas/gtzan'</span>, <span class="st">'all'</span>)</span>
<span id="cb1-5"><a href="#cb1-5"></a></span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="co"># Split the dataset into training and test sets</span></span>
<span id="cb1-7"><a href="#cb1-7"></a>gtzan <span class="op">=</span> gtzan[<span class="st">'train'</span>].train_test_split(seed<span class="op">=</span><span class="dv">42</span>, shuffle<span class="op">=</span><span class="va">True</span>, test_size<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb1-8"><a href="#cb1-8"></a></span>
<span id="cb1-9"><a href="#cb1-9"></a><span class="co"># How does the dataset look like</span></span>
<span id="cb1-10"><a href="#cb1-10"></a><span class="bu">print</span>(gtzan)</span>
<span id="cb1-11"><a href="#cb1-11"></a></span>
<span id="cb1-12"><a href="#cb1-12"></a><span class="co"># Create a function to convert genre ID to genre name</span></span>
<span id="cb1-13"><a href="#cb1-13"></a>id2label_fn <span class="op">=</span> gtzan[<span class="st">'train'</span>].features[<span class="st">'genre'</span>].int2str</span>
<span id="cb1-14"><a href="#cb1-14"></a></span>
<span id="cb1-15"><a href="#cb1-15"></a><span class="co"># Example of converting a genre ID to genre name</span></span>
<span id="cb1-16"><a href="#cb1-16"></a><span class="bu">print</span>(id2label_fn(gtzan[<span class="st">'train'</span>][<span class="st">'genre'</span>][<span class="dv">1</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="output" class="level4">
<h4 class="anchored" data-anchor-id="output"><strong>Output</strong></h4>
<div class="sourceCode" id="cb2"><pre class="sourceCode numberSource bash number-lines code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1"></a><span class="ex">DatasetDict</span><span class="er">(</span><span class="kw">{</span></span>
<span id="cb2-2"><a href="#cb2-2"></a>    <span class="ex">train:</span> Dataset<span class="er">(</span><span class="kw">{</span></span>
<span id="cb2-3"><a href="#cb2-3"></a>        <span class="ex">features:</span> [<span class="st">'file'</span>, <span class="st">'audio'</span>, <span class="st">'genre'</span>],</span>
<span id="cb2-4"><a href="#cb2-4"></a>        <span class="ex">num_rows:</span> 899</span>
<span id="cb2-5"><a href="#cb2-5"></a>    <span class="kw">})</span></span>
<span id="cb2-6"><a href="#cb2-6"></a>    <span class="ex">test:</span> Dataset<span class="er">(</span><span class="kw">{</span></span>
<span id="cb2-7"><a href="#cb2-7"></a>        <span class="ex">features:</span> [<span class="st">'file'</span>, <span class="st">'audio'</span>, <span class="st">'genre'</span>],</span>
<span id="cb2-8"><a href="#cb2-8"></a>        <span class="ex">num_rows:</span> 100</span>
<span id="cb2-9"><a href="#cb2-9"></a>    <span class="kw">})</span></span>
<span id="cb2-10"><a href="#cb2-10"></a><span class="kw">})</span></span>
<span id="cb2-11"><a href="#cb2-11"></a><span class="ex">classical</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="explanation" class="level4">
<h4 class="anchored" data-anchor-id="explanation"><strong>Explanation</strong></h4>
<ul>
<li><strong>Loading Dataset:</strong> <code>load_dataset('marsyas/gtzan', 'all')</code> loads the GTZAN dataset.</li>
<li><strong>Splitting Dataset:</strong> <code>train_test_split</code> splits the dataset into training and validation sets with 90% training and 10% validation.</li>
<li><strong>Label Conversion Function:</strong> <code>int2str()</code> maps numeric genre IDs to their corresponding genre names (human-readable names).</li>
</ul>
</section>
</section>
<section id="generate-audio-samples-with-gradio" class="level2">
<h2 class="anchored" data-anchor-id="generate-audio-samples-with-gradio">Generate Audio Samples with Gradio</h2>
<p>As you have seen in the previous section, our dataset contains three types of features: <code>file</code>, <code>audio</code>, and <code>genre</code>. We learned about <code>genre</code> and now letâ€™s have a closer look at <code>audio</code> and figure out whatâ€™s inside of it.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a>gtzan[<span class="st">"train"</span>][<span class="dv">0</span>][<span class="st">"audio"</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="output-1" class="level4">
<h4 class="anchored" data-anchor-id="output-1"><strong>Output</strong></h4>
<div class="sourceCode" id="cb4"><pre class="sourceCode numberSource bash number-lines code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1"></a><span class="ex">{</span><span class="st">'path'</span><span class="ex">:</span> <span class="st">'/root/.cache/huggingface/datasets/downloads/extracted/5022b0984afa7334ff9a3c60566280b08b5179d4ac96a628052bada7d8940244/genres/pop/pop.00098.wav'</span>,</span>
<span id="cb4-2"><a href="#cb4-2"></a> <span class="st">'array'</span><span class="ex">:</span> array<span class="er">(</span><span class="bu">[</span> 0.10720825,  </span>
<span id="cb4-3"><a href="#cb4-3"></a>                  0.16122437,  </span>
<span id="cb4-4"><a href="#cb4-4"></a>                  <span class="er">0.28585815,</span> </span>
<span id="cb4-5"><a href="#cb4-5"></a>                  <span class="ex">...,</span> </span>
<span id="cb4-6"><a href="#cb4-6"></a>                  <span class="ex">-0.22924805,</span></span>
<span id="cb4-7"><a href="#cb4-7"></a>                  <span class="ex">-0.20629883,</span> </span>
<span id="cb4-8"><a href="#cb4-8"></a>                  <span class="ex">-0.11334229]</span></span>
<span id="cb4-9"><a href="#cb4-9"></a>                <span class="kw">)</span><span class="ex">,</span></span>
<span id="cb4-10"><a href="#cb4-10"></a> <span class="st">'sampling_rate'</span><span class="ex">:</span> 22050}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>As you can see, the audio file is represented as 1-dimensional NumPy array. But what does the value of <code>array</code> represent? And what is <code>sampling_rate</code>?</p>
</section>
<section id="sampling-and-sampling-rate" class="level3">
<h3 class="anchored" data-anchor-id="sampling-and-sampling-rate">Sampling and Sampling Rate</h3>
<p>In signal processing, sampling refers to the process of converting a continuous signal (such as sound) into a discrete signal by taking periodic samples.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="https://en.wikipedia.org/wiki/Sampling_(signal_processing)"><img src="Screenshot 2024-06-19 at 4.36.31 PM.png" class="img-fluid quarto-figure quarto-figure-center figure-img" alt="Wikipedia article: Sampling (signal_processing)"></a></p>
</figure>
</div>
<figcaption>Wikipedia article: Sampling (signal_processing)</figcaption>
</figure>
</div>
<p>In our example of audio sampling, <strong>sampling rate</strong> (or <strong>sampling frequency</strong>) refers to the number of samples of audio carried per second. It is usually measured in Hertz (Hz). To put it in perspective, standard media consumption has a sampling rate of 44,100 Hz, meaning it takes 44,100 samples per second. In comparison, high-resolution audio has a sampling rate of 192,000 Hz (192 kHz). For training speech models, a commonly used sampling rate is 16,000 Hz (16 kHz).</p>
</section>
<section id="amplitude" class="level3">
<h3 class="anchored" data-anchor-id="amplitude">Amplitude</h3>
<p>When we talk about the sampling rate in digital audio, we refer to how often samples are taken. But what do these samples actually represent?</p>
<p>Sound is produced by variations in air pressure at frequencies that are audible to humans. The <strong>amplitude</strong> of a sound measures the sound pressure level at any given moment and is expressed in decibels (dB). Amplitude is perceived as loudness; for example, a normal speaking voice is typically under 60 dB, while a rock concert can reach around 125 dB, which is near the upper limit of human hearing.</p>
<p>In digital audio, each sample captures the amplitude of the audio wave at a specific point in time. For instance, in our sample data <code>gtzan["train"][0]["audio"]</code>, each value in the array represents the amplitude at a particular timestep. For these songs, the sampling rate is 22,050 Hz, which means there are 22,050 amplitude values recorded per second.</p>
<p>One thing to remember is that all audio examples in your dataset have the same sampling rate for any audio-related task. If you intend to use custom audio data to fine-tune a pre-trained model, the sampling rate of your data should match the sampling rate of the data used to pre-train the model. The sampling rate determines the time interval between successive audio samples, therefore impacting the temporal resolution of the audio data.</p>
<p>To read more on this topic click <a href="https://huggingface.co/learn/audio-course/en/chapter1/audio_data">here</a>.</p>
</section>
<section id="gradio" class="level3">
<h3 class="anchored" data-anchor-id="gradio">Gradio</h3>
<p>Now that we better understand our dataset letâ€™s create a aimple and interactive UI with the <code>Blocks API</code> to visualize some audio samples and their labels.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="im">import</span> gradio <span class="im">as</span> gr</span>
<span id="cb5-2"><a href="#cb5-2"></a></span>
<span id="cb5-3"><a href="#cb5-3"></a><span class="co"># Function to generate an audio sample</span></span>
<span id="cb5-4"><a href="#cb5-4"></a><span class="kw">def</span> generate_audio():</span>
<span id="cb5-5"><a href="#cb5-5"></a>    example <span class="op">=</span> gtzan[<span class="st">"train"</span>].shuffle()[<span class="dv">0</span>]</span>
<span id="cb5-6"><a href="#cb5-6"></a>    audio <span class="op">=</span> example[<span class="st">"audio"</span>]</span>
<span id="cb5-7"><a href="#cb5-7"></a>    <span class="cf">return</span> (audio[<span class="st">"sampling_rate"</span>], audio[<span class="st">"array"</span>]), id2label_fn(example[<span class="st">"genre"</span>])</span>
<span id="cb5-8"><a href="#cb5-8"></a></span>
<span id="cb5-9"><a href="#cb5-9"></a><span class="co"># Create a Gradio interface to display audio samples</span></span>
<span id="cb5-10"><a href="#cb5-10"></a><span class="cf">with</span> gr.Blocks() <span class="im">as</span> demo:</span>
<span id="cb5-11"><a href="#cb5-11"></a>    <span class="cf">with</span> gr.Column():</span>
<span id="cb5-12"><a href="#cb5-12"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>):</span>
<span id="cb5-13"><a href="#cb5-13"></a>            audio, label <span class="op">=</span> generate_audio()</span>
<span id="cb5-14"><a href="#cb5-14"></a>            output <span class="op">=</span> gr.Audio(audio, label<span class="op">=</span>label)</span>
<span id="cb5-15"><a href="#cb5-15"></a></span>
<span id="cb5-16"><a href="#cb5-16"></a><span class="co"># Launch the Gradio demo</span></span>
<span id="cb5-17"><a href="#cb5-17"></a>demo.launch(debug<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="output-2" class="level4">
<h4 class="anchored" data-anchor-id="output-2"><strong>Output</strong></h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Screenshot 2024-06-19 at 10.23.29 PM.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
</section>
<section id="explanation-1" class="level4">
<h4 class="anchored" data-anchor-id="explanation-1"><strong>Explanation</strong></h4>
<ul>
<li><strong>Generating Audio</strong>: <code>generate_audio()</code> randomly selects and returns an audio sample from the training set.</li>
<li><strong>Gradio Interface</strong>: Gradio <code>Blocks</code> and <code>Column</code> create a layout to display audio samples. <code>gr.Audio</code> adds audio players with labels to the interface.</li>
<li><strong>Launching Interface</strong>: <code>demo.launch(debug=True)</code> starts the Gradio interface for interaction.</li>
</ul>
</section>
</section>
</section>
<section id="feature-extraction" class="level2">
<h2 class="anchored" data-anchor-id="feature-extraction">Feature Extraction</h2>
<p>Just as tokenization is essential in NLP, audio and speech models need input encoded in a processable format. In ðŸ¤— Transformers, this is handled by the modelâ€™s <em>feature extractor</em>. The <code>AutoFeatureExtractor</code> class automatically selects the right feature extractor for a given model. Letâ€™s see how to process our audio files by instantiating the feature extractor for DistilHuBERT from the pre-trained checkpoint:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoFeatureExtractor</span>
<span id="cb6-2"><a href="#cb6-2"></a></span>
<span id="cb6-3"><a href="#cb6-3"></a><span class="co"># Load a pre-trained feature extractor</span></span>
<span id="cb6-4"><a href="#cb6-4"></a>model_id <span class="op">=</span> <span class="st">'ntu-spml/distilhubert'</span></span>
<span id="cb6-5"><a href="#cb6-5"></a>feature_extractor <span class="op">=</span> AutoFeatureExtractor.from_pretrained(</span>
<span id="cb6-6"><a href="#cb6-6"></a>    model_id,</span>
<span id="cb6-7"><a href="#cb6-7"></a>    do_normalize<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb6-8"><a href="#cb6-8"></a>    return_attention_mask<span class="op">=</span><span class="va">True</span></span>
<span id="cb6-9"><a href="#cb6-9"></a>)</span>
<span id="cb6-10"><a href="#cb6-10"></a></span>
<span id="cb6-11"><a href="#cb6-11"></a><span class="co"># Get the sampling rate from the feature extractor</span></span>
<span id="cb6-12"><a href="#cb6-12"></a>sampling_rate <span class="op">=</span> feature_extractor.sampling_rate</span>
<span id="cb6-13"><a href="#cb6-13"></a>sampling_rate</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="output-3" class="level4">
<h4 class="anchored" data-anchor-id="output-3"><strong>Output</strong></h4>
<div class="sourceCode" id="cb7"><pre class="sourceCode numberSource bash number-lines code-with-copy"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1"></a><span class="ex">16000</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="explanation-2" class="level4">
<h4 class="anchored" data-anchor-id="explanation-2"><strong>Explanation</strong></h4>
<ul>
<li><strong>Loading Feature Extractor</strong>: <code>AutoFeatureExtractor.from_pretrained</code> loads a pre-trained feature extractor model.</li>
<li><strong>Sampling Rate</strong>: <code>feature_extractor.sampling_rate</code> retrieves the sampling rate needed for the audio data.</li>
</ul>
</section>
</section>
<section id="preprocess-the-dataset" class="level2">
<h2 class="anchored" data-anchor-id="preprocess-the-dataset">Preprocess the Dataset</h2>
<p>We preprocess the audio data to match the input requirements of the model by converting audio samples to the desired format and sampling rate.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a><span class="im">from</span> datasets <span class="im">import</span> Audio</span>
<span id="cb8-2"><a href="#cb8-2"></a></span>
<span id="cb8-3"><a href="#cb8-3"></a><span class="co"># Cast the audio column to match the feature extractor's sampling rate</span></span>
<span id="cb8-4"><a href="#cb8-4"></a>gtzan <span class="op">=</span> gtzan.cast_column(<span class="st">'audio'</span>, Audio(sampling_rate<span class="op">=</span>sampling_rate))</span>
<span id="cb8-5"><a href="#cb8-5"></a></span>
<span id="cb8-6"><a href="#cb8-6"></a>gtzan[<span class="st">"train"</span>][<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Below we can verify that the sampling rate is downsampled to 16 kHz. ðŸ¤— Datasets will resample the audio file in real-time as each audio sample is loaded:</p>
<section id="output-4" class="level4">
<h4 class="anchored" data-anchor-id="output-4"><strong>Output</strong></h4>
<div class="sourceCode" id="cb9"><pre class="sourceCode numberSource bash number-lines code-with-copy"><code class="sourceCode bash"><span id="cb9-1"><a href="#cb9-1"></a><span class="kw">{</span></span>
<span id="cb9-2"><a href="#cb9-2"></a>    <span class="st">"file"</span><span class="ex">:</span> <span class="st">"~/.cache/huggingface/datasets/downloads/extracted/fa06ce46130d3467683100aca945d6deafb642315765a784456e1d81c94715a8/genres/pop/pop.00098.wav"</span>,</span>
<span id="cb9-3"><a href="#cb9-3"></a>    <span class="st">"audio"</span><span class="ex">:</span> {</span>
<span id="cb9-4"><a href="#cb9-4"></a>        <span class="st">"path"</span><span class="ex">:</span> <span class="st">"~/.cache/huggingface/datasets/downloads/extracted/fa06ce46130d3467683100aca945d6deafb642315765a784456e1d81c94715a8/genres/pop/pop.00098.wav"</span>,</span>
<span id="cb9-5"><a href="#cb9-5"></a>        <span class="st">"array"</span><span class="ex">:</span> array<span class="er">(</span></span>
<span id="cb9-6"><a href="#cb9-6"></a>            <span class="bu">[</span></span>
<span id="cb9-7"><a href="#cb9-7"></a>                0.0873509,</span>
<span id="cb9-8"><a href="#cb9-8"></a>                0.20183384,</span>
<span id="cb9-9"><a href="#cb9-9"></a>                0.4790867,</span>
<span id="cb9-10"><a href="#cb9-10"></a>                <span class="er">...,</span></span>
<span id="cb9-11"><a href="#cb9-11"></a>                <span class="er">-0.18743178,</span></span>
<span id="cb9-12"><a href="#cb9-12"></a>                <span class="er">-0.23294401,</span></span>
<span id="cb9-13"><a href="#cb9-13"></a>                <span class="er">-0.13517427,</span></span>
<span id="cb9-14"><a href="#cb9-14"></a>            <span class="er">],</span></span>
<span id="cb9-15"><a href="#cb9-15"></a>            <span class="er">dtype=float32,</span></span>
<span id="cb9-16"><a href="#cb9-16"></a>        <span class="er">),</span></span>
<span id="cb9-17"><a href="#cb9-17"></a>        <span class="er">"sampling_rate":</span> <span class="er">16000,</span></span>
<span id="cb9-18"><a href="#cb9-18"></a>    <span class="er">}</span><span class="ex">,</span></span>
<span id="cb9-19"><a href="#cb9-19"></a>    <span class="st">"genre"</span><span class="ex">:</span> 7,</span>
<span id="cb9-20"><a href="#cb9-20"></a><span class="er">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>What we have just done is that weâ€™ve provided the sampling rate of our audio data to our feature extractor. This is a crucial step as the feature extractor verifies whether the sampling rate of our audio data matches the modelâ€™s expected rate. If there were a mismatch, we would need to up-sample or down-sample the audio data to align with the modelâ€™s required sampling rate.</p>
<p>After processing our resampled audio files, the final step is to create a function that can be applied to all examples in the dataset. Since we want the audio clips to be 30 seconds long, we will truncate any longer clips using the <code>max_length</code> and <code>truncation</code> arguments of the feature extractor.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1"></a><span class="co"># Function to preprocess the audio data</span></span>
<span id="cb10-2"><a href="#cb10-2"></a>max_duration <span class="op">=</span> <span class="fl">30.0</span></span>
<span id="cb10-3"><a href="#cb10-3"></a></span>
<span id="cb10-4"><a href="#cb10-4"></a><span class="kw">def</span> preprocess_function(examples):</span>
<span id="cb10-5"><a href="#cb10-5"></a>    audio_arrays <span class="op">=</span> [x[<span class="st">"array"</span>] <span class="cf">for</span> x <span class="kw">in</span> examples[<span class="st">"audio"</span>]]</span>
<span id="cb10-6"><a href="#cb10-6"></a>    inputs <span class="op">=</span> feature_extractor(</span>
<span id="cb10-7"><a href="#cb10-7"></a>        audio_arrays,</span>
<span id="cb10-8"><a href="#cb10-8"></a>        sampling_rate<span class="op">=</span>feature_extractor.sampling_rate,</span>
<span id="cb10-9"><a href="#cb10-9"></a>        max_length<span class="op">=</span><span class="bu">int</span>(feature_extractor.sampling_rate <span class="op">*</span> max_duration),</span>
<span id="cb10-10"><a href="#cb10-10"></a>        truncation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb10-11"><a href="#cb10-11"></a>        return_attention_mask<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb10-12"><a href="#cb10-12"></a>    )</span>
<span id="cb10-13"><a href="#cb10-13"></a>    <span class="cf">return</span> inputs</span>
<span id="cb10-14"><a href="#cb10-14"></a></span>
<span id="cb10-15"><a href="#cb10-15"></a><span class="co"># Apply the preprocessing function to the dataset</span></span>
<span id="cb10-16"><a href="#cb10-16"></a>gtzan_encoded <span class="op">=</span> gtzan.<span class="bu">map</span>(</span>
<span id="cb10-17"><a href="#cb10-17"></a>    preprocess_function,</span>
<span id="cb10-18"><a href="#cb10-18"></a>    remove_columns<span class="op">=</span>[<span class="st">"audio"</span>, <span class="st">"file"</span>],</span>
<span id="cb10-19"><a href="#cb10-19"></a>    batched<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb10-20"><a href="#cb10-20"></a>    batch_size<span class="op">=</span><span class="dv">100</span>, <span class="co"># by default is 1000</span></span>
<span id="cb10-21"><a href="#cb10-21"></a>    num_proc<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb10-22"><a href="#cb10-22"></a>)</span>
<span id="cb10-23"><a href="#cb10-23"></a>gtzan_encoded</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="output-5" class="level4">
<h4 class="anchored" data-anchor-id="output-5"><strong>Output</strong></h4>
<div class="sourceCode" id="cb11"><pre class="sourceCode numberSource bash number-lines code-with-copy"><code class="sourceCode bash"><span id="cb11-1"><a href="#cb11-1"></a><span class="ex">DatasetDict</span><span class="er">(</span><span class="kw">{</span></span>
<span id="cb11-2"><a href="#cb11-2"></a>    <span class="ex">train:</span> Dataset<span class="er">(</span><span class="kw">{</span></span>
<span id="cb11-3"><a href="#cb11-3"></a>        <span class="ex">features:</span> [<span class="st">'genre'</span>, <span class="st">'input_values'</span>, <span class="st">'attention_mask'</span>],</span>
<span id="cb11-4"><a href="#cb11-4"></a>        <span class="ex">num_rows:</span> 899</span>
<span id="cb11-5"><a href="#cb11-5"></a>    <span class="kw">})</span></span>
<span id="cb11-6"><a href="#cb11-6"></a>    <span class="ex">test:</span> Dataset<span class="er">(</span><span class="kw">{</span></span>
<span id="cb11-7"><a href="#cb11-7"></a>        <span class="ex">features:</span> [<span class="st">'genre'</span>, <span class="st">'input_values'</span>, <span class="st">'attention_mask'</span>],</span>
<span id="cb11-8"><a href="#cb11-8"></a>        <span class="ex">num_rows:</span> 100</span>
<span id="cb11-9"><a href="#cb11-9"></a>    <span class="kw">})</span></span>
<span id="cb11-10"><a href="#cb11-10"></a><span class="kw">})</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="explanation-3" class="level4">
<h4 class="anchored" data-anchor-id="explanation-3"><strong>Explanation</strong></h4>
<ul>
<li><strong>Preprocessing Function</strong>: <code>preprocess_function</code> truncates or pads audio samples to a fixed length, normalizes them, and creates attention masks.</li>
<li><strong>Applying Function</strong>: <code>gtzan.map</code> applies the preprocessing function to the entire dataset.</li>
</ul>
<p><code>feature_extractor</code> provides a dictionary containing two arrays: <code>input_values</code> and <code>attention_mask</code>. That is why we see them as new columns for our <code>features</code>.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a>sample <span class="op">=</span> gtzan[<span class="st">"train"</span>][<span class="dv">0</span>][<span class="st">"audio"</span>]</span>
<span id="cb12-2"><a href="#cb12-2"></a></span>
<span id="cb12-3"><a href="#cb12-3"></a>inputs <span class="op">=</span> feature_extractor(sample[<span class="st">"array"</span>], sampling_rate<span class="op">=</span>sample[<span class="st">"sampling_rate"</span>])</span>
<span id="cb12-4"><a href="#cb12-4"></a></span>
<span id="cb12-5"><a href="#cb12-5"></a><span class="bu">print</span>(<span class="ss">f"inputs keys: </span><span class="sc">{</span><span class="bu">list</span>(inputs.keys())<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb13"><pre class="sourceCode numberSource bash number-lines code-with-copy"><code class="sourceCode bash"><span id="cb13-1"><a href="#cb13-1"></a><span class="ex">inputs</span> keys: [<span class="st">'input_values'</span>, <span class="st">'attention_mask'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>For a simpler training process, weâ€™ve excluded the <code>audio</code> and <code>file</code> columns from the dataset. Instead, the dataset now includes an <code>input_values</code> column with encoded audio files, an <code>attention_mask</code> column with binary masks (0 or 1) indicating padded areas in the audio input, and a <code>genre</code> column with corresponding labels or targets.</p>
</section>
</section>
<section id="prepare-labels" class="level2">
<h2 class="anchored" data-anchor-id="prepare-labels">Prepare Labels</h2>
<p>We need to rename the <code>genre</code> column to <code>label</code> to enable the Trainer to process the class labels.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a>gtzan_encoded <span class="op">=</span> gtzan_encoded.rename_column(<span class="st">"genre"</span>, <span class="st">"label"</span>)</span>
<span id="cb14-2"><a href="#cb14-2"></a></span>
<span id="cb14-3"><a href="#cb14-3"></a><span class="co"># Create mappings from IDs to labels and vice versa</span></span>
<span id="cb14-4"><a href="#cb14-4"></a>id2label <span class="op">=</span> {<span class="bu">str</span>(i): id2label_fn(i) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(gtzan_encoded[<span class="st">"train"</span>].features[<span class="st">"label"</span>].names))}</span>
<span id="cb14-5"><a href="#cb14-5"></a>label2id <span class="op">=</span> {v: k <span class="cf">for</span> k, v <span class="kw">in</span> id2label.items()}</span>
<span id="cb14-6"><a href="#cb14-6"></a></span>
<span id="cb14-7"><a href="#cb14-7"></a>id2label</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="output-6" class="level4">
<h4 class="anchored" data-anchor-id="output-6"><strong>Output</strong></h4>
<div class="sourceCode" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a>{ <span class="st">'0'</span>: <span class="st">'blues'</span>,</span>
<span id="cb15-2"><a href="#cb15-2"></a>  <span class="st">'1'</span>: <span class="st">'classical'</span>,</span>
<span id="cb15-3"><a href="#cb15-3"></a>  <span class="st">'2'</span>: <span class="st">'country'</span>,</span>
<span id="cb15-4"><a href="#cb15-4"></a>  <span class="st">'3'</span>: <span class="st">'disco'</span>,</span>
<span id="cb15-5"><a href="#cb15-5"></a>  <span class="st">'4'</span>: <span class="st">'hiphop'</span>,</span>
<span id="cb15-6"><a href="#cb15-6"></a>  <span class="st">'5'</span>: <span class="st">'jazz'</span>,</span>
<span id="cb15-7"><a href="#cb15-7"></a>  <span class="st">'6'</span>: <span class="st">'metal'</span>,</span>
<span id="cb15-8"><a href="#cb15-8"></a>  <span class="st">'7'</span>: <span class="st">'pop'</span>,</span>
<span id="cb15-9"><a href="#cb15-9"></a>  <span class="st">'8'</span>: <span class="st">'reggae'</span>,</span>
<span id="cb15-10"><a href="#cb15-10"></a>  <span class="st">'9'</span>: <span class="st">'rock'</span></span>
<span id="cb15-11"><a href="#cb15-11"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="explanation-4" class="level4">
<h4 class="anchored" data-anchor-id="explanation-4"><strong>Explanation</strong></h4>
<ul>
<li><strong>Renaming Column</strong>: <code>rename_column("genre", "label")</code> renames the genre column to <code>label</code>.</li>
<li><strong>Creating Mappings</strong>: <code>id2label</code> and <code>label2id</code> create dictionaries to map genre IDs to names and vice versa.</li>
</ul>
</section>
</section>
<section id="load-and-fine-tune-the-model" class="level2">
<h2 class="anchored" data-anchor-id="load-and-fine-tune-the-model">Load and Fine-tune the Model</h2>
<p>We load a pre-trained audio classification model and fine-tune it on the GTZAN dataset.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForAudioClassification</span>
<span id="cb16-2"><a href="#cb16-2"></a></span>
<span id="cb16-3"><a href="#cb16-3"></a><span class="co"># Load a pre-trained audio classification model</span></span>
<span id="cb16-4"><a href="#cb16-4"></a>num_labels <span class="op">=</span> <span class="bu">len</span>(id2label)</span>
<span id="cb16-5"><a href="#cb16-5"></a></span>
<span id="cb16-6"><a href="#cb16-6"></a>model <span class="op">=</span> AutoModelForAudioClassification.from_pretrained(</span>
<span id="cb16-7"><a href="#cb16-7"></a>    model_id,</span>
<span id="cb16-8"><a href="#cb16-8"></a>    num_labels<span class="op">=</span>num_labels,</span>
<span id="cb16-9"><a href="#cb16-9"></a>    label2id<span class="op">=</span>label2id,</span>
<span id="cb16-10"><a href="#cb16-10"></a>    id2label<span class="op">=</span>id2label,</span>
<span id="cb16-11"><a href="#cb16-11"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The next step is optional but advised. We basically link our notebook to the ðŸ¤— Hub. The main advantage of doing so is to ensure that no model checkpoint is lost during the training process. You can get your Hub authentication token (permission: write) from <a href="https://huggingface.co/settings/tokens">here</a> :</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a><span class="co"># Login to Hugging Face Hub (optional)</span></span>
<span id="cb17-2"><a href="#cb17-2"></a><span class="im">from</span> huggingface_hub <span class="im">import</span> notebook_login</span>
<span id="cb17-3"><a href="#cb17-3"></a></span>
<span id="cb17-4"><a href="#cb17-4"></a>notebook_login()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="output-7" class="level4">
<h4 class="anchored" data-anchor-id="output-7"><strong>Output</strong></h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Screenshot 2024-06-20 at 2.47.50 PM.png" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
<p>Next step, we define the training arguments (e.g.&nbsp;batch size, number of epochs, learning rate, etc.)</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a><span class="co"># Define training arguments</span></span>
<span id="cb18-2"><a href="#cb18-2"></a><span class="im">from</span> transformers <span class="im">import</span> TrainingArguments</span>
<span id="cb18-3"><a href="#cb18-3"></a></span>
<span id="cb18-4"><a href="#cb18-4"></a>model_name <span class="op">=</span> model_id.split(<span class="st">"/"</span>)[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb18-5"><a href="#cb18-5"></a>batch_size <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb18-6"><a href="#cb18-6"></a>gradient_accumulation_steps <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb18-7"><a href="#cb18-7"></a>num_train_epochs <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb18-8"><a href="#cb18-8"></a></span>
<span id="cb18-9"><a href="#cb18-9"></a>training_args <span class="op">=</span> TrainingArguments(</span>
<span id="cb18-10"><a href="#cb18-10"></a>    <span class="ss">f"</span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss">-finetuned-gtzan"</span>,</span>
<span id="cb18-11"><a href="#cb18-11"></a>    evaluation_strategy<span class="op">=</span><span class="st">"epoch"</span>,</span>
<span id="cb18-12"><a href="#cb18-12"></a>    save_strategy<span class="op">=</span><span class="st">"epoch"</span>,</span>
<span id="cb18-13"><a href="#cb18-13"></a>    learning_rate<span class="op">=</span><span class="fl">5e-5</span>,</span>
<span id="cb18-14"><a href="#cb18-14"></a>    per_device_train_batch_size<span class="op">=</span>batch_size,</span>
<span id="cb18-15"><a href="#cb18-15"></a>    gradient_accumulation_steps<span class="op">=</span>gradient_accumulation_steps,</span>
<span id="cb18-16"><a href="#cb18-16"></a>    per_device_eval_batch_size<span class="op">=</span>batch_size,</span>
<span id="cb18-17"><a href="#cb18-17"></a>    num_train_epochs<span class="op">=</span>num_train_epochs,</span>
<span id="cb18-18"><a href="#cb18-18"></a>    warmup_ratio<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb18-19"><a href="#cb18-19"></a>    logging_steps<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb18-20"><a href="#cb18-20"></a>    load_best_model_at_end<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb18-21"><a href="#cb18-21"></a>    metric_for_best_model<span class="op">=</span><span class="st">"accuracy"</span>,</span>
<span id="cb18-22"><a href="#cb18-22"></a>    fp16<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb18-23"><a href="#cb18-23"></a>    push_to_hub<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb18-24"><a href="#cb18-24"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="explanation-5" class="level4">
<h4 class="anchored" data-anchor-id="explanation-5"><strong>Explanation</strong></h4>
<ul>
<li><strong>Loading Model</strong>: <code>AutoModelForAudioClassification.from_pretrained</code> loads a pre-trained model for audio classification.</li>
<li><strong>Training Arguments</strong>: <code>TrainingArguments</code> defines parameters for training, such as batch size, learning rate, number of epochs, and strategies for evaluation and saving.</li>
</ul>
</section>
</section>
<section id="training-and-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="training-and-evaluation">Training and Evaluation</h2>
<p>Lastly, we define a function to compute metrics and create a trainer to handle the training process.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a><span class="im">import</span> evaluate</span>
<span id="cb19-2"><a href="#cb19-2"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb19-3"><a href="#cb19-3"></a></span>
<span id="cb19-4"><a href="#cb19-4"></a><span class="co"># Load the accuracy metric</span></span>
<span id="cb19-5"><a href="#cb19-5"></a>metric <span class="op">=</span> evaluate.load(<span class="st">"accuracy"</span>)</span>
<span id="cb19-6"><a href="#cb19-6"></a></span>
<span id="cb19-7"><a href="#cb19-7"></a><span class="co"># Function to compute accuracy</span></span>
<span id="cb19-8"><a href="#cb19-8"></a><span class="kw">def</span> compute_metrics(eval_pred):</span>
<span id="cb19-9"><a href="#cb19-9"></a>    predictions <span class="op">=</span> np.argmax(eval_pred.predictions, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb19-10"><a href="#cb19-10"></a>    <span class="cf">return</span> metric.compute(predictions<span class="op">=</span>predictions, references<span class="op">=</span>eval_pred.label_ids)</span>
<span id="cb19-11"><a href="#cb19-11"></a></span>
<span id="cb19-12"><a href="#cb19-12"></a><span class="co"># Initialize the trainer</span></span>
<span id="cb19-13"><a href="#cb19-13"></a><span class="im">from</span> transformers <span class="im">import</span> Trainer</span>
<span id="cb19-14"><a href="#cb19-14"></a></span>
<span id="cb19-15"><a href="#cb19-15"></a>trainer <span class="op">=</span> Trainer(</span>
<span id="cb19-16"><a href="#cb19-16"></a>    model,</span>
<span id="cb19-17"><a href="#cb19-17"></a>    training_args,</span>
<span id="cb19-18"><a href="#cb19-18"></a>    train_dataset<span class="op">=</span>gtzan_encoded[<span class="st">"train"</span>],</span>
<span id="cb19-19"><a href="#cb19-19"></a>    eval_dataset<span class="op">=</span>gtzan_encoded[<span class="st">"test"</span>],</span>
<span id="cb19-20"><a href="#cb19-20"></a>    tokenizer<span class="op">=</span>feature_extractor,</span>
<span id="cb19-21"><a href="#cb19-21"></a>    compute_metrics<span class="op">=</span>compute_metrics,</span>
<span id="cb19-22"><a href="#cb19-22"></a>)</span>
<span id="cb19-23"><a href="#cb19-23"></a></span>
<span id="cb19-24"><a href="#cb19-24"></a><span class="co"># Train the model</span></span>
<span id="cb19-25"><a href="#cb19-25"></a>trainer.train()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="output-8" class="level4">
<h4 class="anchored" data-anchor-id="output-8"><strong>Output</strong></h4>
<pre><code>| Epoch | Training Loss | Validation Loss | Accuracy |
|:-----:|:-------------:|:---------------:|:--------:|
| 1.0   |   1.950200    |    1.817256     | 0.51     |
| 2.0   |   1.158000    |    1.208284     | 0.66     |
| 3.0   |   1.044900    |    0.998169     | 0.72     |
| 4.0   |   0.655100    |    0.852473     | 0.74     |
| 5.0   |   0.611300    |    0.669133     | 0.79     |
| 6.0   |   0.383300    |    0.565036     | 0.86     |
| 7.0   |   0.329900    |    0.623365     | 0.80     |
| 8.0   |   0.114100    |    0.555879     | 0.81     |
| 9.0   |   0.135600    |    0.572448     | 0.80     |
| 10.0  |   0.105100    |    0.580898     | 0.79     |</code></pre>
<p>Using the free tier GPU on Google Colab, we successfully trained our model in about 1 hour. With just 10 epochs and 899 training examples, we achieved an evaluation accuracy of up to 86%. To further optimize model performance, we could increase the number of epochs or apply regularization techniques such as dropout.</p>
</section>
</section>
<section id="inference" class="level2">
<h2 class="anchored" data-anchor-id="inference">Inference</h2>
<p>Now that we have our trained model, we can automatically submit our checkpoint to the leaderboard. You can modify the following values to fit your dataset, language, and model name:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a>kwargs <span class="op">=</span> {</span>
<span id="cb21-2"><a href="#cb21-2"></a>    <span class="st">"dataset_tags"</span>: <span class="st">"marsyas/gtzan"</span>,</span>
<span id="cb21-3"><a href="#cb21-3"></a>    <span class="st">"dataset"</span>: <span class="st">"GTZAN"</span>,</span>
<span id="cb21-4"><a href="#cb21-4"></a>    <span class="st">"model_name"</span>: <span class="ss">f"</span><span class="sc">{</span>model_name<span class="sc">}</span><span class="ss">-finetuned-gtzan"</span>,</span>
<span id="cb21-5"><a href="#cb21-5"></a>    <span class="st">"finetuned_from"</span>: model_id,</span>
<span id="cb21-6"><a href="#cb21-6"></a>    <span class="st">"tasks"</span>: <span class="st">"audio-classification"</span>,</span>
<span id="cb21-7"><a href="#cb21-7"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The training results can now be uploaded to the Hub through the <code>.push_to_hub</code> command:</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1"></a>trainer.push_to_hub(<span class="op">**</span>kwargs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>By following these steps, you built a complete system for music genre classification using the GTZAN dataset, Gradio for interactive visualization, and Hugging Face Transformers for model training and inference.</p>
</section>
<section id="gradio-demo" class="level2">
<h2 class="anchored" data-anchor-id="gradio-demo">Gradio Demo</h2>
<p>Now that we built our music classification model trained on GTZAN dataset, we can showcase it on <a href="https://www.gradio.app/">Gradio</a>. We first need to load up fine-tuned checkpoint using the <code>pipeline()</code> class:</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb23-2"><a href="#cb23-2"></a></span>
<span id="cb23-3"><a href="#cb23-3"></a>model_id <span class="op">=</span> <span class="st">"toobarah/distilhubert-finetuned-gtzan"</span></span>
<span id="cb23-4"><a href="#cb23-4"></a>pipe <span class="op">=</span> pipeline(<span class="st">"audio-classification"</span>, model<span class="op">=</span>model_id)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Next, we defined a function that processes an audio file through the pipeline. The pipeline handles loading the file, resampling it to the correct rate, and running inference with the model. The modelâ€™s predictions are then formatted as a dictionary for display.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1"></a><span class="kw">def</span> classify_audio(filepath):</span>
<span id="cb24-2"><a href="#cb24-2"></a>    preds <span class="op">=</span> pipe(filepath)</span>
<span id="cb24-3"><a href="#cb24-3"></a>    outputs <span class="op">=</span> {}</span>
<span id="cb24-4"><a href="#cb24-4"></a>    <span class="cf">for</span> p <span class="kw">in</span> preds:</span>
<span id="cb24-5"><a href="#cb24-5"></a>        outputs[p[<span class="st">"label"</span>]] <span class="op">=</span> p[<span class="st">"score"</span>]</span>
<span id="cb24-6"><a href="#cb24-6"></a>    <span class="cf">return</span> outputs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Final step, we launch the Gradio demo by calling the function we just created:</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1"></a><span class="im">import</span> gradio <span class="im">as</span> gr</span>
<span id="cb25-2"><a href="#cb25-2"></a></span>
<span id="cb25-3"><a href="#cb25-3"></a>demo <span class="op">=</span> gr.Interface(</span>
<span id="cb25-4"><a href="#cb25-4"></a>    fn<span class="op">=</span>classify_audio, inputs<span class="op">=</span>gr.Audio(<span class="bu">type</span><span class="op">=</span><span class="st">"filepath"</span>), outputs<span class="op">=</span>gr.Label()</span>
<span id="cb25-5"><a href="#cb25-5"></a>)</span>
<span id="cb25-6"><a href="#cb25-6"></a>demo.launch(debug<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>* If you get an <code>ImportError</code> after running the last cell, try downgrading your Gradio using the following command:</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1"></a>pip install gradio<span class="op">==</span><span class="fl">3.47.1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Otherwise, you should see a window pop up as shown below! Go ahead, upload some music, test your model, and enjoy!</p>
<p><img src="Screenshot 2024-06-20 at 6.17.17 PM.png" class="img-fluid"></p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>This tutorial was a step-by-step guide for fine-tuning the DistilHuBERT model for a music classification task. It has also been a learning journey for me, and I drew much inspiration from the work of the Hugging Face <a href="https://huggingface.co/learn/audio-course/en/chapter4/introduction">audio course</a> as I began this project. I hope I was able to explain the steps clearly and that they were easy for you to follow. Every step shown here can be applied to any audio classification task, so if youâ€™re interested in exploring other datasets or models, I recommend checking out other <a href="https://github.com/huggingface/transformers/tree/main/examples/pytorch/audio-classification">examples</a> in the ðŸ¤— Transformers repository.</p>
<p>For access to all the code shared here in one file, click on this <a href="https://drive.google.com/file/d/1v41Y8vXaJzZjq1fNtNYSR5zN9nG6oA-u/view?usp=sharing">Colab file</a>. Happy coding! :)</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>