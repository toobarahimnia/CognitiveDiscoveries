[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello and welcome to my blog! I’m a tech enthusiast who’s constantly learning and eager to share my journey with you. Here, I share everything I’ve learned about applied machine learning and the latest cloud technologies. Whether you’re just starting out or already an expert, I hope you’ll find something valuable in my posts. Thank you for joining me on this learning adventure!"
  },
  {
    "objectID": "posts/blog3/index.html",
    "href": "posts/blog3/index.html",
    "title": "Navigating the Kaggle U.S. Patent Phrase to Phrase Matching Competition: A Step-by-Step Guide",
    "section": "",
    "text": "Working on the U.S. Patent Phrase to Phrase Matching Problem using Natural Language Processing (NLP) techniques to learn how to develop models that accurately match phrases based on semantic similarity.\nIn recent years, natural Language Processing (NLP) has been quit a transformitive force in the field of artificial intelligence, driving advancements in deep learning models that are bale to understand, interpret, and generate human language with high accuracy and sofistication.\nUndoubtedly, classification stands out as one of the most practical and useful applications of NLP. Countless use cases exist for classification tasks, including:\nAnother less obvious use case is the Kaggle U.S. Patent Phrase to Phrase Matching competition. Here, the task involves comparing two words or phrases and assigning a score based on their relevance to each other. A score of 1 indicates identical meaning, while 0 indicates completely different meanings. For example, abatement and eliminating process might have a score of 0.5, indicating they are somewhat similar but not identical.\nNote that this scenario can be likened to a classification problem, where we question:\nIn this post, we will provide a step-by-step guide on solving the Patent Phrase Matching problem. We will approach this task as a classification problem, representing it in a manner similar to the example described above."
  },
  {
    "objectID": "posts/blog3/index.html#on-kaggle",
    "href": "posts/blog3/index.html#on-kaggle",
    "title": "Navigating the Kaggle U.S. Patent Phrase to Phrase Matching Competition: A Step-by-Step Guide",
    "section": "On Kaggle",
    "text": "On Kaggle\nBefore we dive into the code, there are a few steps to follow. On the competition page, first accept the rules, then click on Code from the top menu, then select + New Notebook. You will see a cell with prewritten code, which you can delete as we will be writing all our code from scratch.\nNext, on the right-hand sidebar, follow these steps:\n\nDownload the Models: We will be using two different models for this task (debertav3base and debertav3small). Click on + Add Input, then under the Datasets category, type in the names of these two models one by one and download them.\n\n(Side note: You can also try deberta-v3-large model later on, however, it requires more computing power and I personally do not recommend using it for the start)\nDownload CPC Dataset: There is another dataset that we need to download. Follow the same steps as before and type cpc-codes to obtain the new CSV file. We will explore the information contained in this dataset in later sections.\n\nTurn on GPU: You will need GPUs for this task… Click on the three dots at the top right corner, then choose Accelerator, and select a GPU option.\n\nTurn off the Internet: Finally, you need to work in offline mode as required by the competition organizer. Once all your files are downloaded, open the left-side window, select Session options, and turn off the internet connection. (screenshot)"
  },
  {
    "objectID": "posts/blog3/index.html#importing-libraries-and-set-required-seed",
    "href": "posts/blog3/index.html#importing-libraries-and-set-required-seed",
    "title": "Navigating the Kaggle U.S. Patent Phrase to Phrase Matching Competition: A Step-by-Step Guide",
    "section": "Importing libraries and Set Required Seed",
    "text": "Importing libraries and Set Required Seed\nNow we are ready to start the exciting part. We’ll be using PyTorch and Transformers, along with Matplotlib for some exploratory data analysis (EDA).\nimport pandas as pd\nimport transformers\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport torch\nfrom torch import optim\nfrom torch.utils.data import DataLoader, Dataset, random_split\nfrom torchmetrics.regression import PearsonCorrCoef\nimport numpy as np\nimport random\nimport timeit\nfrom tqdm import tqdm\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nfrom operator import itemgetter\nAdditionally, we’ll use a seed to ensure that the random numbers generated are reproducible. Reproducibility is essential for controlling randomness in both CPU and GPU operations, ensuring deterministic behavior. This is crucial for debugging and for ensuring that experiments can be replicated exactly.\nRANDOM_SEED = 42\n#MODEL_PATH = '/kaggle/input/debertav3base'\nMODEL_PATH = '/kaggle/input/debertav3small'\nMAX_LENGTH = 256\nBATCH_SIZE = 64\nLEARNING_RATE = 2e-5\nEPOCHS = 2\n\ndef set_random_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_random_seed(RANDOM_SEED)\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nThe code below helps control the amount of logging output by focusing only on error messages, which can be particularly useful for cleaner logs and debugging.\ntransformers.utils.logging.set_verbosity_error()"
  },
  {
    "objectID": "posts/blog3/index.html#reading-datasets",
    "href": "posts/blog3/index.html#reading-datasets",
    "title": "Navigating the Kaggle U.S. Patent Phrase to Phrase Matching Competition: A Step-by-Step Guide",
    "section": "Reading Datasets",
    "text": "Reading Datasets\nNow, let’s read our files:\ntrain_df = pd.read_csv('/kaggle/input/us-patent-phrase-to-phrase-matching/train.csv')\n\ntest_df = pd.read_csv('/kaggle/input/us-patent-phrase-to-phrase-matching/test.csv')\n\nsubmission_df = pd.read_csv('/kaggle/input/us-patent-phrase-to-phrase-matching/sample_submission.csv')\n\npatents_df = pd.read_csv('/kaggle/input/cpc-codes/titles.csv')"
  },
  {
    "objectID": "posts/blog3/index.html#eda-warm-up",
    "href": "posts/blog3/index.html#eda-warm-up",
    "title": "Navigating the Kaggle U.S. Patent Phrase to Phrase Matching Competition: A Step-by-Step Guide",
    "section": "EDA Warm Up",
    "text": "EDA Warm Up\nLet’s explore and extract some useful information from our datasets, first.\ntrain_df.head()\n\nThe above table tells us that the training set has five columns:\nid - a unique identifier for a pair of phrases\nanchor - the first phrase\ntarget - the second phrase\ncontext - the CPC classification, which indicates the subject within which the similarity is to be scored\nscore - the similarity. This is sourced from a combination of one or more manual expert ratings.\nWhat else can we get?\ntrain_df.score.hist()\nplt.title('Histogram of Scores')\n\nNote that there are only 5 potential scores:\n1.0 - Represents a very close match, typically indicating an exact match except for possible differences in conjugation, quantity (e.g., singular vs. plural), and the addition or removal of stopwords (e.g., “the,” “and,” “or”).\n0.75 - Indicates a close synonym, such as “mobile phone” vs. “cellphone.” This also includes abbreviations, such as “TCP” -&gt; “transmission control protocol.”\n0.5 - Denotes synonyms that do not have the same meaning (same function, same properties). This category includes broad-narrow (hyponym) and narrow-broad (hypernym) matches.\n0.25 - Indicates that the two phrases are somewhat related, such as being in the same high-level domain but not being synonyms. This category also includes antonyms.\n0.0 - Represents phrases that are unrelated.\nNow let’s have a closer look at the anchor column:\nax = train_df.groupby('anchor')['id'].count().sort_values(ascending=False).head(10).plot(kind='bar', color=list('gbymcr'))\nfor container in ax.containers:\n    ax.bar_label(container)\nplt.title('Top 10 Anchors')\nplt.show()\n\nHere, we can see the top 10 mostly used anchors, respectively. The total number of entities containing each anchor is written on top of its bar.\nThe test dataset is quite similar to the train dataset, except that it does not have the score column.\ntest_df.head()\n\nAnd our submission file at the end should look like this:\nsubmission_df.head()\n\nThe only dataset left is patents_df. But what information does it have inside? Let’s have a look.\npatents_df.head() \n\nRemember that earlier we suggested that we could represent the input to the model as something like “TEXT1: abatement; TEXT2: eliminating process”. We’ll need to apply that context here.\nTo begin, merge train_df and patent_df on the shared column: notice that train_df['context'] and patent_df['code'] contain similar information on both tables, so they’ll be the point of juncture.\nnext step, we will create a new column (input) on the new training dataset (updated_train_df). It combines updated_train_df['title'] and updated_train_df['anchor'].\nupdated_train_df = train_df.merge(patents_df, left_on='context', right_on='code') \nupdated_train_df['input'] = updated_train_df['title'] + \"; \" + updated_train_df['anchor']\nupdated_train_df.tail()\n\nDo the same steps for test_df:\nupdated_test_df = test_df.merge(patents_df, left_on='context', right_on='code')\nupdated_test_df['input'] = updated_test_df['title'] + \" \" + updated_test_df['anchor']"
  },
  {
    "objectID": "posts/blog3/index.html#tokenization",
    "href": "posts/blog3/index.html#tokenization",
    "title": "Navigating the Kaggle U.S. Patent Phrase to Phrase Matching Competition: A Step-by-Step Guide",
    "section": "Tokenization",
    "text": "Tokenization\nNow it’s time to tokenize our datasets… But what is it exactly?\n\nTokenization, in the realm of Natural Language Processing (NLP) and machine learning, refers to the process of converting a sequence of text into smaller parts, known as tokens.\n\nIn other words, we can’t pass the texts directly into a model. A deep learning model expects numbers as inputs, not English sentences! So we need to do two things:\n\nTokenization: Split each text up into tokens\nNumericalization: Convert each token into a number.\n\nThe details about how this is done depend on the particular model we choose. There are numerous models available, but for the starting point we will be using the two models that we have downloaded earlier.\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH, num_labels=1).to(device)\nThe num_labels=1 basically turns it into a regression problem, as to output only one score.\nYou can safely ignore the warnings here."
  },
  {
    "objectID": "posts/blog3/index.html#loading-data",
    "href": "posts/blog3/index.html#loading-data",
    "title": "Navigating the Kaggle U.S. Patent Phrase to Phrase Matching Competition: A Step-by-Step Guide",
    "section": "Loading Data",
    "text": "Loading Data\nIn the following cells we will build our custom dataset classes for training and testing that inherit from torch.utils.data.Dataset.\nclass TrainDataset(Dataset):\n    def __init__(self, inputs, targets, scores, tokenizer):\n        self.scores = scores\n        self.encodings = tokenizer(inputs, \n                                   targets, \n                                   padding=True, \n                                   truncation=True, \n                                   max_length=MAX_LENGTH)\n    \n    def __getitem__(self, idx):\n        out_dic = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        out_dic['scores'] = self.scores[idx]\n        return out_dic\n        \n    def __len__(self):\n        return len(self.scores)\nclass SubmitDataset(Dataset):\n    def __init__(self, inputs, targets, ids, tokenizer):\n        self.ids = ids\n        self.encodings = tokenizer(inputs, \n                                   targets, \n                                   padding=True, \n                                   truncation=True, \n                                   max_length=MAX_LENGTH)\n    \n    def __getitem__(self, idx):\n        out_dic = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        out_dic['ids'] = self.ids[idx]\n        return out_dic\n        \n    def __len__(self):\n        return len(self.ids)\nTaking TrainDataset for instance, let’s break it down into sections and see what it entails:\n\nInitialization (__init__ method):\n\nParameters:\n\ninputs: The input texts to be tokenized.\ntargets: The target texts or labels to be tokenized.\nscores: A list or array of scores corresponding to the inputs and targets.\ntokenizer: A tokenizer object (such as one from the Hugging Face Transformers library).\n\nAttributes:\n\nself.scores: Stores the scores parameter for later use in __getitem__.\nself.encodings: Stores the tokenized inputs and targets. The tokenizer processes the inputs and targets, applying padding, truncation, and limiting the length to MAX_LENGTH.\n\n\nGetting an Item (__getitem__ method):\n\nParameters: \n\nidx: The index of the item to retrieve.\n\nFunctionality:\n\nCreates a dictionary out_dic where each key-value pair from self.encodings is converted into a tensor. The value at index idx for each key is taken. \nAdds the score at index idx from self.scores to out_dic. \nReturns out_dic, which now contains the tokenized input and target data for the given index, along with the corresponding score.\n\n\nLength of Dataset (__len__ method):\n\nReturns the length of the dataset, which is determined by the number of scores. This assumes that the length of scores corresponds to the number of data points in the dataset.\n\n\nAfter tokenizing, let’s check our datasets:\ndataset = TrainDataset(updated_train_df['input'].to_list(), \n                       updated_train_df['target'].to_list(), \n                       updated_train_df['score'].to_list(), \n                       tokenizer)\n\ntest_dataset = SubmitDataset(updated_test_df['input'].to_list(), \n                             updated_test_df['target'].to_list(), \n                             updated_test_df['id'].to_list(), \n                             tokenizer)\n\nThere it is! Every input/output entity turned into a token and then into a number.\nNow we split our data into train and validation sets.\ngenerator = torch.Generator().manual_seed(RANDOM_SEED)\ntrain_dataset, val_dataset = random_split(dataset, [0.8, 0.2], generator=generator)\nWe use DataLoader after having our datasets to efficiently load and process the data in batches during training, validation, and testing of machine learning models. Therefore, it is easier to handle large datasets that don’t fit into memory all at once. It also provides additional functionalities like shuffling and batching, which are essential for training machine learning models effectively.\n(Tip: Always shuffle=True on the training set and shuffle=False on the validation set and test set. [source])\ntrain_dataloader = DataLoader(dataset=train_dataset, \n                             batch_size=BATCH_SIZE,\n                             shuffle=True)\n\nval_dataloader = DataLoader(dataset=val_dataset, \n                             batch_size=BATCH_SIZE,\n                             shuffle=False)\n\ntest_dataloader = DataLoader(dataset=test_dataset, \n                             batch_size=BATCH_SIZE,\n                             shuffle=False)"
  },
  {
    "objectID": "posts/blog3/index.html#training-the-model",
    "href": "posts/blog3/index.html#training-the-model",
    "title": "Navigating the Kaggle U.S. Patent Phrase to Phrase Matching Competition: A Step-by-Step Guide",
    "section": "Training the Model",
    "text": "Training the Model\nNow it’s time to train our model. Notice that I am using /kaggle/input/debertav3small (let’s call it Model 1) as the model path. We will go ahead with training this model first, then we can change the path to /kaggle/input/debertav3base (we refer to this as Model 2) and compare the results.\nBelow we train our model using the AdamW optimizer and evaluate it on the validation dataset. We then compute the Pearson correlation coefficient between the predicted scores and the ground truth scores.\n\nOptimization Initialization:\n\nInitialize the AdamW optimizer, which is a variant of the Adam optimizer with weight decay (W stands for weight decay).\n\nPearsonCorrCoef Initialization:\n\nInitialize an object of the PearsonCorrCoef class.\n\n\nThe Pearson correlation coefficient (r) is a number between –1 and 1 that measures the strength and direction of the relationship between two variables. When one variable changes, the other variable changes in the same direction.\n\nTraining Loop:\n\nIterate over the specified number of epochs (EPOCHS).\nFor each epoch:\n\nSet the model to training mode (model.train()).\nIterate over batches of data in the training dataset (train_dataloader).\n\nMove input data and targets to the device (e.g., GPU).\nCompute the model outputs and loss.\nCompute gradients and performs a parameter update (optimizer step).\nAccumulate the training loss.\n\nCompute the average training loss for the epoch.\nSet the model to evaluation mode (model.eval()).\nIterate over batches of data in the validation dataset (val_dataloader).\n\nCompute the model outputs without gradient computation (since no training is performed).\nAccumulate the validation loss.\nExtend preds and golds lists with predicted scores and ground truth scores, respectively.\n\nCompute the average validation loss for the epoch.\nCompute and print the Pearson correlation coefficient between predicted scores (preds) and ground truth scores (golds).\n\n\n(Side note: outputs[‘logits’].squeeze() would return the logits with any singleton dimensions removed, converting them into a 1D array if they were originally a 2D array with a singleton dimension.)\nTiming and Printing:\n\nCompute and prints the training time.\n\n\noptimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\npearson = PearsonCorrCoef()\nstart = timeit.default_timer()\n\nfor epoch in tqdm(range(EPOCHS), position=0, leave=True):\n    model.train()\n    train_running_loss = 0\n    \n    for idx, sample in enumerate(tqdm(train_dataloader, position=0, leave=True)):\n        input_ids = sample['input_ids'].to(device) # text1\n        attention_mask = sample['attention_mask'].to(device) # text2\n        targets = sample['scores'].to(device) # relativeness\n        \n        outputs = model(input_ids=input_ids, \n                        attention_mask=attention_mask, \n                        labels=targets)\n        \n        loss = outputs.loss\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        train_running_loss += loss.item()\n        \n    train_loss = train_running_loss / (idx + 1)\n    model.eval()\n    val_running_loss = 0\n    preds = []\n    golds = []\n    \n    with torch.no_grad():\n        for idx, sample in enumerate(tqdm(val_dataloader, position=0, leave=True)):\n            input_ids = sample['input_ids'].to(device)\n            attention_mask = sample['attention_mask'].to(device)\n            targets = sample['scores'].to(device)\n            \n            outputs = model(input_ids=input_ids, \n                            attention_mask=attention_mask, \n                            labels=targets)\n            \n            preds.extend([float(i) for i in outputs['logits'].squeeze()])\n            golds.extend([float(i) for i in targets])\n            \n            val_running_loss += outputs.loss.item()\n        val_loss = val_running_loss / (idx + 1)\n    \n    # printing out results \n    print('-' * 30)\n    print(f'Pearson Score: {float(pearson(torch.tensor(preds), torch.tensor(golds))):.3f}')\n    print(f'Train Loss EPOCH {epoch+1}: {train_loss:.3f}')\n    print(f'Valid Loss EPOCH {epoch+1}: {val_loss:.3f}')\n    print('-' * 30)\n    \nstop = timeit.default_timer()\nprint(f'Training Time: {stop-start: .2f}s')\nLet’s compare the performance of Model 1 (left side) and Model 2 (right side).\n\nAs you can see, Model 2 is more accurate than Model 1. It performs 4.6% better on the first epoch and 2.8% better on the second epoch. The training and validation loss of Model 2 is almost half of Model 1.\nHowever, the accuracy comes with the price of consumed time. Model 2 takes a bit over double the time of Model 1 to train. So, to conclude:\n\nModel 1 is faster and less accurate\nModel 2 is slower and more accurate\n\nWhat other models can we use to improve the performance up to 90% or even more? Perhaps one option would be to use Microsoft’s DEBERa-v3-large. What other suggestions you might have?"
  },
  {
    "objectID": "posts/blog3/index.html#clear-the-memory",
    "href": "posts/blog3/index.html#clear-the-memory",
    "title": "Navigating the Kaggle U.S. Patent Phrase to Phrase Matching Competition: A Step-by-Step Guide",
    "section": "Clear the Memory",
    "text": "Clear the Memory\nWe are close to the end. Make sure to free up your memory at this point.\ntorch.cuda.empty_cache()"
  },
  {
    "objectID": "posts/blog3/index.html#testing-the-model",
    "href": "posts/blog3/index.html#testing-the-model",
    "title": "Navigating the Kaggle U.S. Patent Phrase to Phrase Matching Competition: A Step-by-Step Guide",
    "section": "Testing the Model",
    "text": "Testing the Model\npreds = []\nids = []\nmodel.eval()\nwith torch.no_grad():\n    for idx, sample in enumerate(tqdm(test_dataloader)):\n        input_ids = sample['input_ids'].to(device)\n        attention_mask = sample['attention_mask'].to(device)\n        ids.extend(sample[\"ids\"])\n        \n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        preds.extend([float(i) for i in outputs[\"logits\"].squeeze()])"
  },
  {
    "objectID": "posts/blog3/index.html#submission",
    "href": "posts/blog3/index.html#submission",
    "title": "Navigating the Kaggle U.S. Patent Phrase to Phrase Matching Competition: A Step-by-Step Guide",
    "section": "Submission",
    "text": "Submission\nCongratulations on making it this far! Your code is now ready to be submitted. Save your predicted scores in a .csv file and submit the results. Kaggle will use a hidden testing dataset to evaluate your model. You will then be able to see how well your model performs on these unseen datasets.\nsubmission_df = pd.DataFrame(list(zip(ids, preds)),\n               columns =[\"id\", \"score\"])\nsubmission_df.to_csv(\"submission.csv\", index=False)\nsubmission_df.head()\nThank you for sticking with me up to this point. This has been my first time working on an NLP case competition, and it’s also my first time writing about it. I hope it provided some value to you. Let’s continue practicing and improving together!\nTo access my complete code for this Kaggle competition, click here."
  },
  {
    "objectID": "posts/blog2/index.html",
    "href": "posts/blog2/index.html",
    "title": "Implementing Binary Image Classification Using FastAI, and Flask",
    "section": "",
    "text": "This guide covers the entire process of deploying a dog breed image classification model. The model is locally hosted, and utilizes a RESTful API built with Flask.\nIn this project, we will build a web application that can classify the breed of a dog given an image provided by the user. We will walk through the entire process, from loading the data and fine-tuning our model to designing the back end and front end using Flask, HTML, and CSS.\nSo, without further ado, let’s get started."
  },
  {
    "objectID": "posts/blog2/index.html#data-loading-and-model-fine-tuning-in-jupyter-notebook",
    "href": "posts/blog2/index.html#data-loading-and-model-fine-tuning-in-jupyter-notebook",
    "title": "Implementing Binary Image Classification Using FastAI, and Flask",
    "section": "Data Loading and Model Fine-Tuning in Jupyter Notebook",
    "text": "Data Loading and Model Fine-Tuning in Jupyter Notebook\nFirst, install and load all the packages and dependencies:\n!pip install fastbook\n!pip install timm\nimport timm # must be imported before fastai\nimport fastbook \nfrom fastbook import * \nfrom fastai.vision.widgets import * \nimport requests # for making HTTP requests \nfrom bs4 import BeautifulSoup as bs \nfrom pathlib import Path\nimport os\n\nDownloading Data\nIn the next steps, we will extract our dataset from a webpage using a method called web scraping. Our target website contains an extensive list of dog breed names. We will use the first 50 breed names and then download 150 different images for each breed using the Python library download_images, which retrieves images directly from Google.\n# Fetch the webpage and parse it\nURL = \"&lt;https://www.akc.org/expert-advice/news/most-popular-dog-breeds-full-ranking-list/&gt;\" \npage = requests.get(URL) \nsoup = bs(page.content, \"html.parser\")\nThis Following line of code uses a list comprehension to extract the text from the first cell of each row within the table body of a HTML document parsed using BeautifulSoup (soup).\nTo break it down:\n\nsoup.select(\"tbody tr\"): This part uses the select method of BeautifulSoup to find all (table row) elements that are descendants of a (table body) element in the parsed HTML document. It returns a list of these elements.\n[row.td.get_text(strip=True) for row in ...]: This is a list comprehension that iterates over each element (row) found by select and extracts the text from the first cell within that row. row.td accesses the first element in the row, and get_text(strip=True) gets the text content of that element, stripping any leading or trailing whitespace.\n[:50]: Finally, [:50] at the end limits the resulting list of breed names to the first 50 entries, ensuring that only the top 50 breeds are considered.\n\nbreeds = [row.td.get_text(strip=True) for row in soup.select(\"tbody tr\")][:50]\nNow, let’s download images using our list of breeds as keywords for the search engine:\n# Set up the working directory\nroot = Path.cwd() / \"dog-breed-DS\"\n\n# Query and download images for each breed\nif not root.exists():\n  root.mkdir()\n  for breed in breeds:\n      # duckduckgo_search(root, breed, breed, max_results=100)\n      dest = root/breed\n      dest.mkdir(exist_ok=True)\n      download_images(dest, urls=search_images_ddg(f'{breed} dog', max_images=150))\n      resize_images(dest, dest=dest)\n\n\nData Cleaning\nIf you proceed with training your model on the dataset you collected, you might end up with an error similar to this:\nOSError: Caught OSError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/PIL/ImageFile.py\", line 249, in load\n    s = read(self.decodermaxblock)\n ...\n \nstruct.error: unpack_from requires a buffer of at least 4 bytes for unpacking 4 bytes at offset 0 (actual buffer size is 0)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n...    \n\n  File \"/usr/local/lib/python3.10/dist-packages/PIL/ImageFile.py\", line 256, in load\n    raise OSError(msg) from e\nOSError: image file is truncated\nThis error merely implies that one or more of your image files is corrupted or truncated. This can happen for various reasons, such as incomplete downloads, network issues, or issues with the image source. In order to fix this issue, you can clean the image dataset by deleting all the corrupted images in a given folder so that only the good ones remain in the directory:\ncnt = 0\nfor breedname in os.listdir(root):\n    for filename in os.listdir(os.path.join(root, breedname)):\n        try:\n           img=Image.open(os.path.join(root, breedname, filename))\n        except OSError:\n            print(\"FILE:\", os.path.join(breedname, filename), \"is corrupt!\")\n            cnt += 1\n            os.remove(os.path.join(root, breedname, filename))\nprint(\"Successfully Completed Operation! Files Courrupted are \", cnt)\n\n\nData Augmentation and Loading\nAt this step, we begin to set up a pipeline to process a dataset of dog images, applying necessary transformations and preparing it for training a deep learning model using the fastai library.\nFirst, we need to create the DataBlock. Let’s take it to smaller pieces and see what each part does:\n\nDataBlock: DataBlock is a fastai class that allows you to define how to create your dataset. It provides a flexible and simple way to handle data pipelines.\nblocks: This parameter defines the types of input and output blocks. ImageBlock is for handling images, and CategoryBlock is for categorical labels (the dog breeds in this case).\nget_items: get_image_files is a function that retrieves all the image file paths.\nsplitter: RandomSplitter(valid_pct=0.2, seed=42) is used to split the dataset into training and validation sets. Here, 20% of the data is used for validation, and the split is reproducible due to the set seed.\nget_y: parent_label is a function that gets the label for each image, typically by extracting it from the parent directory name.\nitem_tfms: Resize(129) is an item transformation that resizes each image to 129x129 pixels.\n\ndogs = DataBlock(\n      blocks=(ImageBlock, CategoryBlock),\n      get_items=get_image_files,\n      splitter=RandomSplitter(valid_pct=0.2, seed=42),\n      get_y=parent_label,\n      item_tfms=Resize(129)\n)\nIn the following cell we are trying to do the data augmentation to enhance the performance of our model:\n\ndogs.new: This creates a new DataBlock object with updated transformations.\nitem_tfms: RandomResizedCrop(129, min_scale=0.5) replaces the previous Resize(129). This transformation randomly crops the image to 129x129 pixels with a minimum scale of 0.5, which helps with data augmentation.\nbatch_tfms: aug_transforms(mult=2) applies a set of data augmentation transformations to the entire batch. The mult=2 argument increases the intensity of these transformations.\n\nYou can try to change the numbers to find the right adjustment for your model design.\ndogs = dogs.new(\n      item_tfms = RandomResizedCrop(129, min_scale=0.5),\n      batch_tfms = aug_transforms(mult=2)\n)\nThe next method creates the DataLoaders object, which is used to load the data in batches. root is the root directory where the images are stored.\ndls = dogs.dataloaders(root)\nFinally, we show a batch of images from the training set for inspection.\ndls.train.show_batch(max_n=4, nrows=1, unique=True)\n\n\n\nTraining the Model\nLet’s go ahead and train our model\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(4)\nAs you can see we get a training time of 3:53 min on the last epoch.\n\n\n\n\n\nThanks to the great work that’s been done by Ross Wightman, we can import the powerful timm library which provides state-of-the-art pre-trained computer vision models. This allows us to improve our model performance. You can learn more about the library through this analysis. \nHere are some of the models covered by timm:\ntimm.list_models('convnext*')\n\n\n\n\n\nNow, training on we get a better result!\nlearn = vision_learner(dls, 'convnext_tiny_in22k', metrics=error_rate).to_fp16()\nlearn.fine_tune(4)\n\n\n\n\n\nNotice the time is much faster and the accuracy increased from 81% to almost 86%!\nLastly, we download our model and we’re done with this part:\nlearn.export('/content/model.pkl')"
  },
  {
    "objectID": "posts/blog2/index.html#building-a-restful-api-with-flask",
    "href": "posts/blog2/index.html#building-a-restful-api-with-flask",
    "title": "Implementing Binary Image Classification Using FastAI, and Flask",
    "section": "Building a RESTful API with Flask",
    "text": "Building a RESTful API with Flask\nREST APIs have become essential to modern software applications, enabling seamless communication between systems and ensuring data consistency. They facilitate the easy exchange of information between various websites, databases, and SaaS (Software as a Service) applications. Whether you’re developing a mobile app, a web application, or integrating third-party services, understanding how to create a robust REST API is crucial. \nThere are numerous ways to build a REST API, and leveraging existing frameworks and tools can streamline the process. One such example is Flask, a web framework in Python. With the Flask-RESTful extension, developers can quickly and efficiently develop REST APIs. \nNow, let’s dive into the implementation and see how we can create a REST API using Flask and the Flask-RESTful extension.\n\nSetup Your Flask Project\n\nOpen VSCode\nCreate a new directory for your project and navigate into it.\nCreate a virtual environment: python -m venv .venv\nActivate the virtual environment:\n\nOn Windows: .venv\\Scripts\\activate\nOn macOS/Linux: source .venv/bin/activate\n\nInstall Flask and fastbook if you haven’t already:\n\npip install fastbook\npip install Flask\n\nCreate the following project structure:\n├── app.py\n├── templates\n│   └── index.html\n|   └── result.html\n└── static\n|   └── style.css\n├── model.pkl\n\n\n\n\n\n\n\n\nCreate Your Flask App (app.py)\n\nImport necessary libraries and set up your Flask app.\nDefine routes for the home page (index.html) and result page (result.html).\nUse a form to upload an image on the home page and process the uploaded image on the result page.\nfrom flask import Flask, render_template, request, jsonify, url_for\nfrom fastbook import *\nfrom fastai import *\nfrom fastai.vision import *\nimport os\n\n\n# Define a flask app\napp = Flask(__name__)\nroot = os.getcwd()\nfilename = 'model.pkl'\nlearn = load_learner(os.path.join(root, filename))\n\n\n@app.route('/')\ndef index():\n    # Main page\n    return render_template('index.html')\n\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    prediction = single_image_predict(request.files['uploaded-file'])\n    prediction_dict = json.loads(prediction.data.decode('utf-8'))  # Convert bytes to string and then to dict\n\n    return render_template('result.html', percentage=prediction_dict['probability'], prediction=prediction_dict['dog_type'])\n\n\n#function to predict image\ndef single_image_predict(image):\n    img_PIL = Image.open(image)\n    img = tensor(img_PIL) #converting PIL image to tensor\n    learn_inf = learn.predict(img)\n    return jsonify({'dog_type': learn_inf[0][0:],\n                    'probability': str(round(max(learn_inf[2].numpy()*100), 2))})\n\n\nif __name__ == '__main__':\n    app.run(host=\"0.0.0.0\", port=8000)\n\nCreate Your HTML Templates (index.html and result.html)\n\nindex.html for the home page with a form to upload the image.\nresult.html to display the result (breed of the dog).\n&lt;!-- index.html --&gt;\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;Home&lt;/title&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"/static/style.css\"&gt;\n    &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"{{ url_for('static', filename='/styles.css') }}\"&gt;\n&lt;/head&gt;\n\n&lt;body&gt;\n\n    &lt;div class=\"container main\"&gt;&lt;/div&gt;\n        &lt;div class=\"row\"&gt;\n          &lt;div class=\"col\"&gt;\n            &lt;h1 class=\"header\"&gt;Dog Breed Image Classifier&lt;/h1&gt;\n            &lt;form action=\"{{ url_for('predict')}}\" method=\"post\" enctype=\"multipart/form-data\"&gt;\n                &lt;label for=\"uploaded-file\" class=\"upload-frame\"&gt;\n                    Upload your photo here :)\n                    &lt;input type=\"file\" id=\"uploaded-file\" class=\"form-control\" name=\"uploaded-file\" style=\"display: none;\" onchange=\"displayImage(event)\"&gt;\n                &lt;/label&gt;\n                &lt;input type=\"hidden\" name=\"image_path\" id=\"image_path\"&gt;\n                &lt;div class=\"button-group\"&gt;\n                    &lt;!-- &lt;button class=\"upload-photo-btn\" type=\"button\" onclick=\"document.getElementById('file-input').click()\"&gt;Upload Photo&lt;/button&gt; --&gt;\n                    &lt;button class=\"submit-btn\" type=\"submit\"&gt;Submit&lt;/button&gt;\n                &lt;/div&gt;\n            &lt;/form&gt;\n\n            &lt;script&gt;\n                function displayImage(event) {\n                    var file = event.target.files[0];\n                    var reader = new FileReader();\n\n                    reader.onload = function(e) {\n                        var imageUrl = e.target.result;\n                        var uploadFrame = document.querySelector('.upload-frame');\n                        uploadFrame.style.backgroundImage = 'url(' + imageUrl + ')';\n                        document.getElementById('image_path').value = imageUrl; // Set the value of the hidden input field\n                    };\n\n                    reader.readAsDataURL(file);\n                }\n\n                function validateForm(event) {\n                    var fileInput = document.getElementById('uploaded-file');\n                    if (fileInput.files.length === 0) {\n                        alert('Please upload a photo first');\n                        event.preventDefault(); // Prevent the form from submitting\n                    }\n                }\n\n                document.querySelector('form').addEventListener('submit', validateForm);\n            &lt;/script&gt;\n\n        &lt;/div&gt;\n    &lt;/div&gt;\n\n    &lt;/body&gt;\n&lt;/html&gt; \n&lt;!-- result.html --&gt;\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;&lt;/title&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"/static/style.css\"&gt;\n    &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"{{ url_for('static', filename='/styles.css') }}\"&gt;\n&lt;/head&gt;\n&lt;body&gt;\n\n    &lt;header&gt;\n        &lt;div class=\"container\"&gt;\n            &lt;h1 class=\"header\"&gt;Classification Results&lt;/h1&gt;\n        &lt;/div&gt;\n    &lt;/header&gt;\n\n    &lt;div class=\"results\"&gt;\n        &lt;h3&gt;This is {{ percentage }}% {{ prediction }}&lt;/h3&gt;\n        &lt;p&gt;Let's play again! Hit the Home link below :)&lt;/p&gt;\n        &lt;div style=\"text-align: center;\"&gt;\n            &lt;a href=\"{{ url_for('index') }}\" class=\"back-btn\"&gt;Home&lt;/a&gt;\n        &lt;/div&gt;\n    &lt;/div&gt;\n\n&lt;/body&gt;\n&lt;/html&gt;\n\nAdd Pretrained Model for Dog Breed Classification\n\nUse a the downloaded pretrained model for dog breed classification.\nLoad the model in your Flask app and use it to classify the uploaded image.\n\nRun and Test Your Flask App\n\nRun your Flask app: python app.py\nAccess your web app in a browser at http://192.168.4.218:8000 (or any link that shows up on your terminal) to upload a dog photo and see the result!"
  },
  {
    "objectID": "posts/blog2/index.html#next-step-deployment",
    "href": "posts/blog2/index.html#next-step-deployment",
    "title": "Implementing Binary Image Classification Using FastAI, and Flask",
    "section": "Next Step: Deployment",
    "text": "Next Step: Deployment\nSo far, we have learned how to create our deep learning-based web app. We designed both the backend and the frontend. The final step is to deploy our model so everyone can access our application on their own devices.\nThere are many options for choosing the right platform, but one of the most widely used deployment environments is a cloud service provider. Cloud services are ubiquitous these days, making it essential for machine learning practitioners and tech experts to stay ahead with a solid understanding of these services. One of the most important cloud service providers in the job market is AWS.\nDeploying a project of this scale on AWS can be tricky (and costly). If you have never worked with the platform before and want to do some warm-up practices with DL models, the best alternative options would be using Heroku, Hugging Face, or Render.\nAlright, we will stop the project at this point, but you can keep experimenting with the code and perhaps create your own image classification app with your chosen data!\nFinally, I want to leave you with some sources that inspired me for this project. You can also find my code in this repository. Let me know what you think, and as always, all feedbacks are welcome! :)\n\nBinary Image Classifier Deployed on Heroku\nDeploy Machine Learning Model API on AWS EC2\nHow to Make a REST API"
  },
  {
    "objectID": "posts/blog1/index.html",
    "href": "posts/blog1/index.html",
    "title": "How to Build a Web App Using HuggingFace and Gradio Within an Hour",
    "section": "",
    "text": "Walkthrough of an end to end dog vs cat image classification model deployed on HuggingFace Spaces, supported by FastAI and Gradio.\nIt’s been a few weeks that I have started taking the fantastic deep learning course, fast.ai, by Jeremy Howard and it has been an amazing learning journey so far. I come from a STEM background but Howard’s style of teaching deep learning has brought a fresh perspective into learning this field. As I go through each lecture, I made the decision of documenting and sharing my experiences and learning outcomes publicly, so that it could inspire, help or encourage someone along the same path. This course has been a game-changer in my learning path so far, and I’m thrilled to talk about my first project in image classification.\nIn this very first blog, I will guide you through deploying an image classification model using HuggingFace and Gradio. This method is beginner-friendly, straightforward and completely free. Whether you are a newcomer or looking to refine your deployment skills, I’ll walk you through each step, ensuring that by the end, you’ll be able to deploy your own models effortlessly. So, let’s get started on the exciting journey!"
  },
  {
    "objectID": "posts/blog1/index.html#prerequisites",
    "href": "posts/blog1/index.html#prerequisites",
    "title": "How to Build a Web App Using HuggingFace and Gradio Within an Hour",
    "section": "Prerequisites",
    "text": "Prerequisites\nBefore we begin, make sure you have the followings:\n\nA basic understanding of Python\nA HuggingFace account (sign up here)"
  },
  {
    "objectID": "posts/blog1/index.html#getting-started",
    "href": "posts/blog1/index.html#getting-started",
    "title": "How to Build a Web App Using HuggingFace and Gradio Within an Hour",
    "section": "Getting Started",
    "text": "Getting Started\nFirst thing, open the Google Colab and let’s make sure we have the necessary tools set up. Since we’re using the popular fastai library, you might need to install or upgrade it first with the following command.\n!pip install -Uqq fastai\nNow, install the one necessary package needed at this step.\nfrom fastai.vision.all import *"
  },
  {
    "objectID": "posts/blog1/index.html#gathering-data",
    "href": "posts/blog1/index.html#gathering-data",
    "title": "How to Build a Web App Using HuggingFace and Gradio Within an Hour",
    "section": "Gathering Data",
    "text": "Gathering Data\nFastai makes it incredibly easy for us to work with datasets thanks to its built-in function: untar_data(). This function streamlines the process of downloading and extracting datasets.\nIn our case, we use untar_data(URLs.PETS) to download a compressed dataset of pet images from a specified URL and extract it to a specific location on our local machine (path).\npath = untar_data(URLs.PETS)/'images'\nThe PETS dataset includes images of 37 breeds of pets along with annotations, which are perfect for training an image classification model."
  },
  {
    "objectID": "posts/blog1/index.html#loading-data",
    "href": "posts/blog1/index.html#loading-data",
    "title": "How to Build a Web App Using HuggingFace and Gradio Within an Hour",
    "section": "Loading Data",
    "text": "Loading Data\nNext, we need to load our data. ImageDataLoaders.from_name_func is a method we use that is provided by the fastai library. It is designed to help you create a DataLoader for training and validating image classification models. This method is particularly useful when you have a dataset of images and the labels can be inferred from the filenames of those images, which is basically what we have here.\nThe primary purpose of ImageDataLoaders.from_name_func is to create a data loader that can feed images and their corresponding labels to a machine learning model during training and validation phases. It streamlines the process of preparing data, handling transformations, and ensuring that the data is fed in batches to the model.\nBut how does it work? Let’s break it down into smaller pieces:\n\npath: The path to your dataset directory.\nget_image_files(path): A function from fastai that returns a list of all image file paths in the directory.\nlabel_func: A function that takes a file path and returns the label. In this example, it selects the label according to the first letter of the file’s name. If the name starts with a capital letter, the image belongs to a cat, and a dog otherwise.\nitem_tfms: Transformations applied to each image individually, such as resizing.\n\ndls = ImageDataLoaders.from_name_func('.', \n            get_image_files(path), valid_pct=0.2, seed=42, \n            label_func=is_cat, \n            item_tfms=Resize(192))\n\n\ndef is_cat(x): return x[0].isupper()          \nLet’s take a look at some randomly picked photos from our dataset.\ndls.show_batch()"
  },
  {
    "objectID": "posts/blog1/index.html#training-the-model",
    "href": "posts/blog1/index.html#training-the-model",
    "title": "How to Build a Web App Using HuggingFace and Gradio Within an Hour",
    "section": "Training the Model",
    "text": "Training the Model\nAfter setting up the DataLoader, the next step is to create and train a model using the fastai library. Here’s how you can do it:\nFirst, we initialize a vision_learner with our data loaders (dls), specifying the architecture we want to use—in this case, ResNet-152, a powerful and accurate convolutional neural network. We also specify the metrics we want to monitor during training, such as the error rate.\nlearn = vision_learner(dls, resnet152, metrics=error_rate)\nNext, we fine-tune the model using the fine_tune method:\nlearn.fine_tune(3)\nThis method fine-tunes the model for a specified number of epochs—in this case, 3 epochs. Fine-tuning involves training the model’s top layers, which are typically initialized with random weights, while gradually unfreezing and training the deeper layers of the pre-trained network. This process allows the model to adapt to our specific dataset while leveraging the powerful features learned by ResNet-152 on a much larger dataset.\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.051338\n0.039052\n0.010825\n01:58\n\n\n1\n0.026246\n0.006559\n0.002030\n01:58\n\n\n2\n0.010546\n0.004937\n0.001353\n01:57"
  },
  {
    "objectID": "posts/blog1/index.html#downloading-the-model",
    "href": "posts/blog1/index.html#downloading-the-model",
    "title": "How to Build a Web App Using HuggingFace and Gradio Within an Hour",
    "section": "Downloading the Model",
    "text": "Downloading the Model\nLet’s go ahead and download our model.\nlearn.export('model.pkl')"
  },
  {
    "objectID": "posts/blog1/index.html#deployment",
    "href": "posts/blog1/index.html#deployment",
    "title": "How to Build a Web App Using HuggingFace and Gradio Within an Hour",
    "section": "Deployment",
    "text": "Deployment\nFor the deployment, it’s very straightforward. We will be using HuggingFace Spaces. It is a platform that allows you to host and share machine learning models and demos easily.\nBefore going onto HuggingFace, we need to create our app.py and requirements.txt files so that we can upload them to the HF repo.\nIn the app.py file, we load the model that we saved previously, using load_learner('model.pkl'), and then classify our new unseen images using the Gradio interface. Gradio is a Python library that allows you to quickly create customizable web apps for your machine learning models and data processing pipelines.\nWhat we are basically doing here is setting up a web interface where users can upload images, and our model will classify these images and return the results. We call our classify_image function, specify our inputs and outputs, and optionally include some example images for users to test.\nfrom fastai.vision.all import *\nimport gradio as gr\n\ndef is_cat(x): return x[0].isupper() \n\nlearn = load_learner('model.pkl')\n\ncategories = ('Dog', 'Cat')\n\ndef classify_image(img):\n    pred,idx,probs = learn.predict(img)\n    return dict(zip(categories, map(float,probs)))\n\nimage = gr.Image()\nlabel = gr.Label()\nexamples = ['dog.jpg', 'cat.jpg', 'cat_dog.jpg']\n\nintf = gr.Interface(fn=classify_image, inputs=image, outputs=label, examples=examples)\nintf.launch(inline=False)\nWe need to create a requirements.txt file as mentioned before, and all we need to mention is the fastai library because HF doesn’t automatically include it. This file ensures that the necessary dependencies are installed when the app is deployed on HuggingFace Spaces.\n\nSetting up HuggingFace Spaces\nSo how do you set up the HF Space? First, go to the HuggingFace Spaces website. Once you’re there, click on “Create new Space”.\nChoose a Space name of your own. For the License, choose “apache-2.0”. Select Gradio as the Space SDK. Leave the rest as is and click on “Create Space”.\n\nOpen your local terminal and navigate to the directory where your required files, that you created previously, are saved. Once there, do the following:\ngit clone &lt;https://huggingface.co/spaces/&gt;&lt;your_login&gt;/&lt;your_app_name&gt;\ncd &lt;your_app_name&gt;\nHave your three files ready:\n\napp.py\nrequirements.txt\nmodel.pkl\n\nYou can commit and push the first two files as follow:\ngit add &lt;your_file_name&gt; \ngit commit -m 'Add application file'\ngit push\nHowever, to upload your model, you need a different approach since it’s a large file (over 25 MB). You need to first. Follow these steps:\n\nInstall Git: If you do not have Git installed locally, please download from here.\nDownload and install Git Large File Storage (LFS): Git LFS is an extension for Git that allows you to handle large files. Download and install it from here.\nSet up Git LFS: Type the following commands in your terminal:\ngit lfs install \ngit lfs track \"*.pkl\"\ngit add .gitattributes \nAdd and commit your model file:\ngit add model.pkl \ngit commit -m \"Add model\" \nPush your changes to the repository:\ngit push -u origin master"
  },
  {
    "objectID": "posts/blog1/index.html#final-result",
    "href": "posts/blog1/index.html#final-result",
    "title": "How to Build a Web App Using HuggingFace and Gradio Within an Hour",
    "section": "Final Result",
    "text": "Final Result\nAfter following all these steps, your app will show up on the screen in a few moments! You’ll see your HuggingFace Space with your deployed image classification model, ready to use and share with others!\n\nThanks for sticking with me up to this point! I hope you found this guide useful. Feel free to check out my app and all the source code by clicking here. Happy coding, and keep pushing boundaries!"
  },
  {
    "objectID": "posts/blog4/index.html",
    "href": "posts/blog4/index.html",
    "title": "How to Build a Music Genre Classifier Using Hugging Face Transformers and Gradio",
    "section": "",
    "text": "In this tutorial, we will build a music genre classification system using the GTZAN dataset to identify the genre of a given audio track.\nHave you ever been curious about how machine learning models classify music genres? What features in the dataset are useful for the model’s understanding? And how can you deploy your trained model for users? If these questions have crossed your mind, then keep reading as I guide you through everything you need to quickly deploy a music classification app. By the end of this tutorial, you will have a fully functional music genre classification model capable of predicting the genre of any audio track. You will also have a Gradio-based interactive interface to test and visualize the model’s predictions, and the model will be ready for deployment using the Hugging Face Hub."
  },
  {
    "objectID": "posts/blog4/index.html#load-and-prepare-the-dataset",
    "href": "posts/blog4/index.html#load-and-prepare-the-dataset",
    "title": "How to Build a Music Genre Classifier Using Hugging Face Transformers and Gradio",
    "section": "Load and Prepare the Dataset",
    "text": "Load and Prepare the Dataset\nWe start by loading the GTZAN dataset using the datasets library and split it into training and test sets. The reason for using GTZAN is that it’s a popular dataset containing 1,000 songs for music genre classification. Each song is a 30-second clip from one of 10 genres of music, spanning blues to rock.\nfrom datasets import load_dataset\n\n# Load the GTZAN dataset\ngtzan = load_dataset('marsyas/gtzan', 'all')\n\n# Split the dataset into training and test sets\ngtzan = gtzan['train'].train_test_split(seed=42, shuffle=True, test_size=0.1)\n\n# How does the dataset look like\nprint(gtzan)\n\n# Create a function to convert genre ID to genre name\nid2label_fn = gtzan['train'].features['genre'].int2str\n\n# Example of converting a genre ID to genre name\nprint(id2label_fn(gtzan['train']['genre'][1]))\n\nOutput\nDatasetDict({\n    train: Dataset({\n        features: ['file', 'audio', 'genre'],\n        num_rows: 899\n    })\n    test: Dataset({\n        features: ['file', 'audio', 'genre'],\n        num_rows: 100\n    })\n})\nclassical\n\n\nExplanation\n\nLoading Dataset: load_dataset('marsyas/gtzan', 'all') loads the GTZAN dataset.\nSplitting Dataset: train_test_split splits the dataset into training and validation sets with 90% training and 10% validation.\nLabel Conversion Function: int2str() maps numeric genre IDs to their corresponding genre names (human-readable names)."
  },
  {
    "objectID": "posts/blog4/index.html#generate-audio-samples-with-gradio",
    "href": "posts/blog4/index.html#generate-audio-samples-with-gradio",
    "title": "How to Build a Music Genre Classifier Using Hugging Face Transformers and Gradio",
    "section": "Generate Audio Samples with Gradio",
    "text": "Generate Audio Samples with Gradio\nAs you have seen in the previous section, our dataset contains three types of features: file, audio, and genre. We learned about genre and now let’s have a closer look at audio and figure out what’s inside of it.\ngtzan[\"train\"][0][\"audio\"]\n\nOutput\n{'path': '/root/.cache/huggingface/datasets/downloads/extracted/5022b0984afa7334ff9a3c60566280b08b5179d4ac96a628052bada7d8940244/genres/pop/pop.00098.wav',\n 'array': array([ 0.10720825,  \n                  0.16122437,  \n                  0.28585815, \n                  ..., \n                  -0.22924805,\n                  -0.20629883, \n                  -0.11334229]\n                ),\n 'sampling_rate': 22050}\nAs you can see, the audio file is represented as 1-dimensional NumPy array. But what does the value of array represent? And what is sampling_rate?\n\n\nSampling and Sampling Rate\nIn signal processing, sampling refers to the process of converting a continuous signal (such as sound) into a discrete signal by taking periodic samples.\n\n\n\n\n\n\n\nWikipedia article: Sampling (signal_processing)\n\n\nIn our example of audio sampling, sampling rate (or sampling frequency) refers to the number of samples of audio carried per second. It is usually measured in Hertz (Hz). To put it in perspective, standard media consumption has a sampling rate of 44,100 Hz, meaning it takes 44,100 samples per second. In comparison, high-resolution audio has a sampling rate of 192,000 Hz (192 kHz). For training speech models, a commonly used sampling rate is 16,000 Hz (16 kHz).\n\n\nAmplitude\nWhen we talk about the sampling rate in digital audio, we refer to how often samples are taken. But what do these samples actually represent?\nSound is produced by variations in air pressure at frequencies that are audible to humans. The amplitude of a sound measures the sound pressure level at any given moment and is expressed in decibels (dB). Amplitude is perceived as loudness; for example, a normal speaking voice is typically under 60 dB, while a rock concert can reach around 125 dB, which is near the upper limit of human hearing.\nIn digital audio, each sample captures the amplitude of the audio wave at a specific point in time. For instance, in our sample data gtzan[\"train\"][0][\"audio\"], each value in the array represents the amplitude at a particular timestep. For these songs, the sampling rate is 22,050 Hz, which means there are 22,050 amplitude values recorded per second.\nOne thing to remember is that all audio examples in your dataset have the same sampling rate for any audio-related task. If you intend to use custom audio data to fine-tune a pre-trained model, the sampling rate of your data should match the sampling rate of the data used to pre-train the model. The sampling rate determines the time interval between successive audio samples, therefore impacting the temporal resolution of the audio data.\nTo read more on this topic click here.\n\n\nGradio\nNow that we better understand our dataset let’s create a aimple and interactive UI with the Blocks API to visualize some audio samples and their labels.\nimport gradio as gr\n\n# Function to generate an audio sample\ndef generate_audio():\n    example = gtzan[\"train\"].shuffle()[0]\n    audio = example[\"audio\"]\n    return (audio[\"sampling_rate\"], audio[\"array\"]), id2label_fn(example[\"genre\"])\n\n# Create a Gradio interface to display audio samples\nwith gr.Blocks() as demo:\n    with gr.Column():\n        for _ in range(4):\n            audio, label = generate_audio()\n            output = gr.Audio(audio, label=label)\n\n# Launch the Gradio demo\ndemo.launch(debug=True)\n\nOutput\n\n\n\n\n\n\n\nExplanation\n\nGenerating Audio: generate_audio() randomly selects and returns an audio sample from the training set.\nGradio Interface: Gradio Blocks and Column create a layout to display audio samples. gr.Audio adds audio players with labels to the interface.\nLaunching Interface: demo.launch(debug=True) starts the Gradio interface for interaction."
  },
  {
    "objectID": "posts/blog4/index.html#feature-extraction",
    "href": "posts/blog4/index.html#feature-extraction",
    "title": "How to Build a Music Genre Classifier Using Hugging Face Transformers and Gradio",
    "section": "Feature Extraction",
    "text": "Feature Extraction\nJust as tokenization is essential in NLP, audio and speech models need input encoded in a processable format. In 🤗 Transformers, this is handled by the model’s feature extractor. The AutoFeatureExtractor class automatically selects the right feature extractor for a given model. Let’s see how to process our audio files by instantiating the feature extractor for DistilHuBERT from the pre-trained checkpoint:\nfrom transformers import AutoFeatureExtractor\n\n# Load a pre-trained feature extractor\nmodel_id = 'ntu-spml/distilhubert'\nfeature_extractor = AutoFeatureExtractor.from_pretrained(\n    model_id,\n    do_normalize=True,\n    return_attention_mask=True\n)\n\n# Get the sampling rate from the feature extractor\nsampling_rate = feature_extractor.sampling_rate\nsampling_rate\n\nOutput\n16000\n\n\nExplanation\n\nLoading Feature Extractor: AutoFeatureExtractor.from_pretrained loads a pre-trained feature extractor model.\nSampling Rate: feature_extractor.sampling_rate retrieves the sampling rate needed for the audio data."
  },
  {
    "objectID": "posts/blog4/index.html#preprocess-the-dataset",
    "href": "posts/blog4/index.html#preprocess-the-dataset",
    "title": "How to Build a Music Genre Classifier Using Hugging Face Transformers and Gradio",
    "section": "Preprocess the Dataset",
    "text": "Preprocess the Dataset\nWe preprocess the audio data to match the input requirements of the model by converting audio samples to the desired format and sampling rate.\nfrom datasets import Audio\n\n# Cast the audio column to match the feature extractor's sampling rate\ngtzan = gtzan.cast_column('audio', Audio(sampling_rate=sampling_rate))\n\ngtzan[\"train\"][0]\nBelow we can verify that the sampling rate is downsampled to 16 kHz. 🤗 Datasets will resample the audio file in real-time as each audio sample is loaded:\n\nOutput\n{\n    \"file\": \"~/.cache/huggingface/datasets/downloads/extracted/fa06ce46130d3467683100aca945d6deafb642315765a784456e1d81c94715a8/genres/pop/pop.00098.wav\",\n    \"audio\": {\n        \"path\": \"~/.cache/huggingface/datasets/downloads/extracted/fa06ce46130d3467683100aca945d6deafb642315765a784456e1d81c94715a8/genres/pop/pop.00098.wav\",\n        \"array\": array(\n            [\n                0.0873509,\n                0.20183384,\n                0.4790867,\n                ...,\n                -0.18743178,\n                -0.23294401,\n                -0.13517427,\n            ],\n            dtype=float32,\n        ),\n        \"sampling_rate\": 16000,\n    },\n    \"genre\": 7,\n}\nWhat we have just done is that we’ve provided the sampling rate of our audio data to our feature extractor. This is a crucial step as the feature extractor verifies whether the sampling rate of our audio data matches the model’s expected rate. If there were a mismatch, we would need to up-sample or down-sample the audio data to align with the model’s required sampling rate.\nAfter processing our resampled audio files, the final step is to create a function that can be applied to all examples in the dataset. Since we want the audio clips to be 30 seconds long, we will truncate any longer clips using the max_length and truncation arguments of the feature extractor.\n# Function to preprocess the audio data\nmax_duration = 30.0\n\ndef preprocess_function(examples):\n    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n    inputs = feature_extractor(\n        audio_arrays,\n        sampling_rate=feature_extractor.sampling_rate,\n        max_length=int(feature_extractor.sampling_rate * max_duration),\n        truncation=True,\n        return_attention_mask=True,\n    )\n    return inputs\n\n# Apply the preprocessing function to the dataset\ngtzan_encoded = gtzan.map(\n    preprocess_function,\n    remove_columns=[\"audio\", \"file\"],\n    batched=True,\n    batch_size=100, # by default is 1000\n    num_proc=1,\n)\ngtzan_encoded\n\n\nOutput\nDatasetDict({\n    train: Dataset({\n        features: ['genre', 'input_values', 'attention_mask'],\n        num_rows: 899\n    })\n    test: Dataset({\n        features: ['genre', 'input_values', 'attention_mask'],\n        num_rows: 100\n    })\n})\n\n\nExplanation\n\nPreprocessing Function: preprocess_function truncates or pads audio samples to a fixed length, normalizes them, and creates attention masks.\nApplying Function: gtzan.map applies the preprocessing function to the entire dataset.\n\nfeature_extractor provides a dictionary containing two arrays: input_values and attention_mask. That is why we see them as new columns for our features.\nsample = gtzan[\"train\"][0][\"audio\"]\n\ninputs = feature_extractor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"])\n\nprint(f\"inputs keys: {list(inputs.keys())}\")\ninputs keys: ['input_values', 'attention_mask']\nFor a simpler training process, we’ve excluded the audio and file columns from the dataset. Instead, the dataset now includes an input_values column with encoded audio files, an attention_mask column with binary masks (0 or 1) indicating padded areas in the audio input, and a genre column with corresponding labels or targets."
  },
  {
    "objectID": "posts/blog4/index.html#prepare-labels",
    "href": "posts/blog4/index.html#prepare-labels",
    "title": "How to Build a Music Genre Classifier Using Hugging Face Transformers and Gradio",
    "section": "Prepare Labels",
    "text": "Prepare Labels\nWe need to rename the genre column to label to enable the Trainer to process the class labels.\ngtzan_encoded = gtzan_encoded.rename_column(\"genre\", \"label\")\n\n# Create mappings from IDs to labels and vice versa\nid2label = {str(i): id2label_fn(i) for i in range(len(gtzan_encoded[\"train\"].features[\"label\"].names))}\nlabel2id = {v: k for k, v in id2label.items()}\n\nid2label\n\nOutput\n{ '0': 'blues',\n  '1': 'classical',\n  '2': 'country',\n  '3': 'disco',\n  '4': 'hiphop',\n  '5': 'jazz',\n  '6': 'metal',\n  '7': 'pop',\n  '8': 'reggae',\n  '9': 'rock'\n}\n\n\nExplanation\n\nRenaming Column: rename_column(\"genre\", \"label\") renames the genre column to label.\nCreating Mappings: id2label and label2id create dictionaries to map genre IDs to names and vice versa."
  },
  {
    "objectID": "posts/blog4/index.html#load-and-fine-tune-the-model",
    "href": "posts/blog4/index.html#load-and-fine-tune-the-model",
    "title": "How to Build a Music Genre Classifier Using Hugging Face Transformers and Gradio",
    "section": "Load and Fine-tune the Model",
    "text": "Load and Fine-tune the Model\nWe load a pre-trained audio classification model and fine-tune it on the GTZAN dataset.\nfrom transformers import AutoModelForAudioClassification\n\n# Load a pre-trained audio classification model\nnum_labels = len(id2label)\n\nmodel = AutoModelForAudioClassification.from_pretrained(\n    model_id,\n    num_labels=num_labels,\n    label2id=label2id,\n    id2label=id2label,\n)\nThe next step is optional but advised. We basically link our notebook to the 🤗 Hub. The main advantage of doing so is to ensure that no model checkpoint is lost during the training process. You can get your Hub authentication token (permission: write) from here :\n# Login to Hugging Face Hub (optional)\nfrom huggingface_hub import notebook_login\n\nnotebook_login()\n\nOutput\n\n\n\n\n\nNext step, we define the training arguments (e.g. batch size, number of epochs, learning rate, etc.)\n# Define training arguments\nfrom transformers import TrainingArguments\n\nmodel_name = model_id.split(\"/\")[-1]\nbatch_size = 8\ngradient_accumulation_steps = 1\nnum_train_epochs = 10\n\ntraining_args = TrainingArguments(\n    f\"{model_name}-finetuned-gtzan\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=5e-5,\n    per_device_train_batch_size=batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=num_train_epochs,\n    warmup_ratio=0.1,\n    logging_steps=5,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    fp16=True,\n    push_to_hub=True,\n)\n\n\nExplanation\n\nLoading Model: AutoModelForAudioClassification.from_pretrained loads a pre-trained model for audio classification.\nTraining Arguments: TrainingArguments defines parameters for training, such as batch size, learning rate, number of epochs, and strategies for evaluation and saving."
  },
  {
    "objectID": "posts/blog4/index.html#training-and-evaluation",
    "href": "posts/blog4/index.html#training-and-evaluation",
    "title": "How to Build a Music Genre Classifier Using Hugging Face Transformers and Gradio",
    "section": "Training and Evaluation",
    "text": "Training and Evaluation\nLastly, we define a function to compute metrics and create a trainer to handle the training process.\nimport evaluate\nimport numpy as np\n\n# Load the accuracy metric\nmetric = evaluate.load(\"accuracy\")\n\n# Function to compute accuracy\ndef compute_metrics(eval_pred):\n    predictions = np.argmax(eval_pred.predictions, axis=1)\n    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n\n# Initialize the trainer\nfrom transformers import Trainer\n\ntrainer = Trainer(\n    model,\n    training_args,\n    train_dataset=gtzan_encoded[\"train\"],\n    eval_dataset=gtzan_encoded[\"test\"],\n    tokenizer=feature_extractor,\n    compute_metrics=compute_metrics,\n)\n\n# Train the model\ntrainer.train()\n\nOutput\n| Epoch | Training Loss | Validation Loss | Accuracy |\n|:-----:|:-------------:|:---------------:|:--------:|\n| 1.0   |   1.950200    |    1.817256     | 0.51     |\n| 2.0   |   1.158000    |    1.208284     | 0.66     |\n| 3.0   |   1.044900    |    0.998169     | 0.72     |\n| 4.0   |   0.655100    |    0.852473     | 0.74     |\n| 5.0   |   0.611300    |    0.669133     | 0.79     |\n| 6.0   |   0.383300    |    0.565036     | 0.86     |\n| 7.0   |   0.329900    |    0.623365     | 0.80     |\n| 8.0   |   0.114100    |    0.555879     | 0.81     |\n| 9.0   |   0.135600    |    0.572448     | 0.80     |\n| 10.0  |   0.105100    |    0.580898     | 0.79     |\nUsing the free tier GPU on Google Colab, we successfully trained our model in about 1 hour. With just 10 epochs and 899 training examples, we achieved an evaluation accuracy of up to 86%. To further optimize model performance, we could increase the number of epochs or apply regularization techniques such as dropout."
  },
  {
    "objectID": "posts/blog4/index.html#inference",
    "href": "posts/blog4/index.html#inference",
    "title": "How to Build a Music Genre Classifier Using Hugging Face Transformers and Gradio",
    "section": "Inference",
    "text": "Inference\nNow that we have our trained model, we can automatically submit our checkpoint to the leaderboard. You can modify the following values to fit your dataset, language, and model name:\nkwargs = {\n    \"dataset_tags\": \"marsyas/gtzan\",\n    \"dataset\": \"GTZAN\",\n    \"model_name\": f\"{model_name}-finetuned-gtzan\",\n    \"finetuned_from\": model_id,\n    \"tasks\": \"audio-classification\",\n}\nThe training results can now be uploaded to the Hub through the .push_to_hub command:\ntrainer.push_to_hub(**kwargs)\nBy following these steps, you built a complete system for music genre classification using the GTZAN dataset, Gradio for interactive visualization, and Hugging Face Transformers for model training and inference."
  },
  {
    "objectID": "posts/blog4/index.html#demo-with-gradio",
    "href": "posts/blog4/index.html#demo-with-gradio",
    "title": "How to Build a Music Genre Classifier Using Hugging Face Transformers and Gradio",
    "section": "Demo with Gradio",
    "text": "Demo with Gradio"
  },
  {
    "objectID": "posts/blog4/index.html#conclusion",
    "href": "posts/blog4/index.html#conclusion",
    "title": "How to Build a Music Genre Classifier Using Hugging Face Transformers and Gradio",
    "section": "Conclusion",
    "text": "Conclusion\nIn this tutorial, we provided a step-by-step guide for fine-tuning the DistilHuBERT model for a music classification task. It has also been a learning journey for me, and I drew much inspiration from the work of the Hugging Face audio course as I began this project. Every step shown here can be applied to any audio classification task. If you’re interested in exploring other datasets or models, I recommend checking out other examples in the 🤗 Transformers repository.\nFor access to all the code shared here in one file, click on this Colab file. Happy coding! :)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CognitiveDiscoveries",
    "section": "",
    "text": "How to Build a Music Genre Classifier Using Hugging Face Transformers and Gradio\n\n\n\n\n\n\nCode\n\n\nAnalysis\n\n\nAudio Signal Processing\n\n\n\n\n\n\n\n\n\nJun 19, 2024\n\n\nTooba Rahimnia\n\n\n\n\n\n\n\n\n\n\n\n\nNavigating the Kaggle U.S. Patent Phrase to Phrase Matching Competition: A Step-by-Step Guide\n\n\n\n\n\n\nCode\n\n\nAnalysis\n\n\nNLP\n\n\n\n\n\n\n\n\n\nJun 5, 2024\n\n\nTooba Rahimnia\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Binary Image Classification Using FastAI, and Flask\n\n\n\n\n\n\nCode\n\n\nAnalysis\n\n\nComputer Vision\n\n\n\n\n\n\n\n\n\nMay 28, 2024\n\n\nTooba Rahimnia\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Build a Web App Using HuggingFace and Gradio Within an Hour\n\n\n\n\n\n\nCode\n\n\nAnalysis\n\n\nComputer Vision\n\n\n\n\n\n\n\n\n\nMay 20, 2024\n\n\nTooba Rahimnia\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/blog4/index.html#gradio-demo",
    "href": "posts/blog4/index.html#gradio-demo",
    "title": "How to Build a Music Genre Classifier Using Hugging Face Transformers and Gradio",
    "section": "Gradio Demo",
    "text": "Gradio Demo\nNow that we built our music classification model trained on GTZAN dataset, we can showcase it on Gradio. We first need to load up fine-tuned checkpoint using the pipeline() class:\nfrom transformers import pipeline\n\nmodel_id = \"toobarah/distilhubert-finetuned-gtzan\"\npipe = pipeline(\"audio-classification\", model=model_id)\nNext, we defined a function that processes an audio file through the pipeline. The pipeline handles loading the file, resampling it to the correct rate, and running inference with the model. The model’s predictions are then formatted as a dictionary for display.\ndef classify_audio(filepath):\n    preds = pipe(filepath)\n    outputs = {}\n    for p in preds:\n        outputs[p[\"label\"]] = p[\"score\"]\n    return outputs\nFinal step, we launch the Gradio demo by calling the function we just created:\nimport gradio as gr\n\ndemo = gr.Interface(\n    fn=classify_audio, inputs=gr.Audio(type=\"filepath\"), outputs=gr.Label()\n)\ndemo.launch(debug=True)\n* If you get an ImportError after running the last cell, try downgrading your Gradio using the following command:\npip install gradio==3.47.1\nOtherwise, you should see a window pop up as shown below! Go ahead, upload some music, test your model, and enjoy!"
  }
]