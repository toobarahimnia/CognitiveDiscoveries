[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello and welcome to my blog! My name is Tooba. I’m a tech enthusiast who’s constantly learning and eager to share my journey with you. Here, I share everything I’ve learned about applied machine learning and the latest cloud technologies. Whether you’re just starting out or already an expert, I hope you’ll find something valuable in my posts. Thank you for joining me on this learning adventure!"
  },
  {
    "objectID": "posts/article5/index.html",
    "href": "posts/article5/index.html",
    "title": "How to Setup a Machine for Deep Learning Environment (Mac Version)",
    "section": "",
    "text": "Create a Deep Learning environment on your own system using mamba, and fastai.\nRecently, I discovered a fantastic series of recorded live coding sessions by Jeremy Howard from fastai. These sessions are a treasure trove for machine learning and deep learning enthusiasts, offering practical insights and cutting-edge techniques. Inspired by Jeremy’s work, I’ve decided to document my learnings through writing articles. My goal is to understand, remember, and share some of the valuable tips and tricks from these sessions with you.\nYou can explore these amazing sessions here.\nI’m using a MacBook Pro, so some steps might differ slightly from Jeremy’s Windows setup. However, the overall process is quite similar, as we’ll be working within a Linux distribution."
  },
  {
    "objectID": "posts/article5/index.html#definitions",
    "href": "posts/article5/index.html#definitions",
    "title": "How to Setup a Machine for Deep Learning Environment (Mac Version)",
    "section": "Definitions",
    "text": "Definitions\n\nTerminal and Shell\nLet’s start with the terminal. A terminal, also known as a command line interface (CLI), is a text-based interface used to interact with the operating system. It allows us to execute text commands, run scripts, and manage system processes. This is done through a shell, which is a command-line interpreter that understands and executes the commands.\nFor Mac users, here is the Terminal User Guide to help you get started.\n\n\nPrompt\nWhen you open the terminal, you’ll see the prompt, which includes your username and the tilde (~) sign. The prompt is a text indicator or symbol that shows the system is ready to accept commands. It usually contains information about the current directory, username, and hostname of the machine. The prompt serves as a visual cue that the terminal is awaiting input.\n\n\n\n\n\n\n\nMac vs Windows\nOn a Mac, the terminal is called “Terminal” and is a Unix-based shell, typically using Bash or Zsh, which are powerful for development and system management tasks. Windows traditionally used Command Prompt, which is less feature-rich compared to Unix shells. However, Windows now also includes PowerShell and the Windows Subsystem for Linux (WSL), which provides a more Unix-like environment."
  },
  {
    "objectID": "posts/article5/index.html#system-setup",
    "href": "posts/article5/index.html#system-setup",
    "title": "How to Setup a Machine for Deep Learning Environment (Mac Version)",
    "section": "System Setup",
    "text": "System Setup\nIf you have a Mac, you can skip the following step as everything is already integrated.\nFor Windows users, you need to first install Linux with WSL:\nwsl --install\nOn Windows, we use a Linux distribution called Ubuntu, and the shell used is Bash. When you open your terminal for the first time, you need to create a username and password. Then, Ubuntu sets up a new virtual machine."
  },
  {
    "objectID": "posts/article5/index.html#understanding-conda-and-mamba",
    "href": "posts/article5/index.html#understanding-conda-and-mamba",
    "title": "How to Setup a Machine for Deep Learning Environment (Mac Version)",
    "section": "Understanding Conda and Mamba",
    "text": "Understanding Conda and Mamba\nConda and Mamba are package managers that simplify the process of installing and managing software packages, particularly for data science and machine learning projects. They create isolated environments, ensuring that dependencies for different projects do not conflict. For example, when you type which python in your terminal, it typically shows your system’s Python version. Modifying this version of Python is not recommended as it’s used by your computer to run programs on your operating system. Instead, you should install a different version of Python for your use.\nIn a nutshell, the difference between Conda and Mamba is that Mamba is a faster version of Conda, addressing some of the speed and performance issues related to Conda."
  },
  {
    "objectID": "posts/article5/index.html#setting-up-local-machine",
    "href": "posts/article5/index.html#setting-up-local-machine",
    "title": "How to Setup a Machine for Deep Learning Environment (Mac Version)",
    "section": "Setting Up Local Machine",
    "text": "Setting Up Local Machine\nAs of September 2023, Mambaforge has been discouraged in favor of Miniforge. So, we will use Miniforge for our setup.\n\nWith the release of Miniforge3-23.3.1-0, that incorporated the changes in #277, the packages and configuration of Mambaforge and Miniforge3 are now identical. The only difference between the two is the name of the installer and, subsequently, the default installation directory.\n\n\nInstall Mamba\n\nVisit this link to select the installer for your operating system, copy its link, and move on to the next step.\n\nOpen your terminal and run the following commands one after the other:\nmkdir Projects # in your home directory create your project\n\nwget https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-MacOSX-arm64.sh # paste the link you copied from prev step here\n\nbash Miniforge3-MacOSX-arm64.sh\n\nOnce the Miniforge installer is complete, you are asked to close and re-open your shell.\nWhen you do that, you’ll see (base) before the prompt, indicating that you’re inside the environment. To confirm, you can type which python, which will show that you’re now in the Miniforge directory.\n\n\n\n\n\nThe file with the .sh extension is a shell script with commands listed one after another. To see the content of the shell script, type:\nless Miniforge3-MacOSX-arm64.sh\nTo list the contents of your directory in a human-readable format, type:\nls -lh\nAnd to delete a directory, you can use:\nrm -rf miniforge\nWhere rm means to remove, -r means recursively, and -f means force. If you want to install everything from scratch, remove the directory and start again.\n\n\nCreate a new Environment\nYou can create a new environment by typing the following command into the Command Prompt:\nmamba create -n my_env devbio-napari python=3.9 pyqt -c conda-forge\nThis will create a new environment with the name my_first_env and Python version 3.9 installed. Additionally, the latest version of devbio-napari will be installed in this environment. Devbio-napari is a collection of Python libraries and Napari plugins maintained by the BiAPoL team, useful for processing fluorescent microscopy image data. Conda will ask for your permission to download the needed packages with Proceed [y]/n. By hitting Enter, you confirm this, and Mamba will download and install the necessary packages.\nTo activate the environment, type:\nmamba activate my_env"
  },
  {
    "objectID": "posts/article5/index.html#install-packages-with-mamba",
    "href": "posts/article5/index.html#install-packages-with-mamba",
    "title": "How to Setup a Machine for Deep Learning Environment (Mac Version)",
    "section": "Install Packages with Mamba",
    "text": "Install Packages with Mamba\n\nIPython\nNow you need to install libraries. One such library is IPython (Interactive Python), which works on the model known as REPL — Read-Eval-Print-Loop.\nmamba install ipython\n\n\nPytorch\nFor Pytorch you can select your preferences from here and then run the corresponding install command.\n\n\n\n\n\nmamba install pytorch torchvision torchaudio -c pytorch\n\n\nJupyter\nmamba install jupyterlab\nTo run Jupyter Notebook, simply type:\njupyter lab\nThe above command will work just fine on Mac. On Windows you might face with an error. To fix that, you can Ctrl + click on the link or use the — no-browser.\n\n\n\n\n\n\n\nInstalling New Packages\nIf you want to install new packages while working in your Jupyter notebook, you can:\n\nOpen a new Command Prompt window.\nActivate your current environment.\nInstall packages by specifying the channel with -c conda-forge.\n\nFor example, if you want to install seaborn, type:\nmamba install seaborn -c conda-forge"
  },
  {
    "objectID": "posts/article5/index.html#final-words",
    "href": "posts/article5/index.html#final-words",
    "title": "How to Setup a Machine for Deep Learning Environment (Mac Version)",
    "section": "Final Words",
    "text": "Final Words\nIn this quick tutorial, we have successfully installed Miniforge. I hope the instructions were clear and easy to follow. If you need more detailed steps and additional tips on using the terminal, you can always refer back to the original video linked above.\nOn another note, I should add that Jeremy is a fantastic instructor, and I highly recommend watching his videos to deepen your understanding of data science and machine learning concepts. His hands-on approach and clear explanations make complex topics accessible and engaging. Dive in and start learning from one of the best!\nNow, it’s time for you to start experimenting with Conda environments, familiarize yourself with some essential packages, and build some awesome projects. Happy coding!"
  },
  {
    "objectID": "posts/article3/index.html",
    "href": "posts/article3/index.html",
    "title": "Navigating the Kaggle U.S. Patent Phrase to Phrase Matching Competition: A Step-by-Step Guide",
    "section": "",
    "text": "Working on the U.S. Patent Phrase to Phrase Matching Problem using Natural Language Processing (NLP) techniques to learn how to develop models that accurately match phrases based on semantic similarity.\nIn recent years, natural Language Processing (NLP) has been quit a transformitive force in the field of artificial intelligence, driving advancements in deep learning models that are bale to understand, interpret, and generate human language with high accuracy and sofistication.\nUndoubtedly, classification stands out as one of the most practical and useful applications of NLP. Countless use cases exist for classification tasks, including:\nAnother less obvious use case is the Kaggle U.S. Patent Phrase to Phrase Matching competition. Here, the task involves comparing two words or phrases and assigning a score based on their relevance to each other. A score of 1 indicates identical meaning, while 0 indicates completely different meanings. For example, abatement and eliminating process might have a score of 0.5, indicating they are somewhat similar but not identical.\nNote that this scenario can be likened to a classification problem, where we question:\nIn this post, we will provide a step-by-step guide on solving the Patent Phrase Matching problem. We will approach this task as a classification problem, representing it in a manner similar to the example described above."
  },
  {
    "objectID": "posts/article3/index.html#on-kaggle",
    "href": "posts/article3/index.html#on-kaggle",
    "title": "Navigating the Kaggle U.S. Patent Phrase to Phrase Matching Competition: A Step-by-Step Guide",
    "section": "On Kaggle",
    "text": "On Kaggle\nBefore we dive into the code, there are a few steps to follow. On the competition page, first accept the rules, then click on Code from the top menu, then select + New Notebook. You will see a cell with prewritten code, which you can delete as we will be writing all our code from scratch.\nNext, on the right-hand sidebar, follow these steps:\n\nDownload the Models: We will be using two different models for this task (debertav3base and debertav3small). Click on + Add Input, then under the Datasets category, type in the names of these two models one by one and download them.\n\n(Side note: You can also try deberta-v3-large model later on, however, it requires more computing power and I personally do not recommend using it for the start)\nDownload CPC Dataset: There is another dataset that we need to download. Follow the same steps as before and type cpc-codes to obtain the new CSV file. We will explore the information contained in this dataset in later sections.\n\nTurn on GPU: You will need GPUs for this task… Click on the three dots at the top right corner, then choose Accelerator, and select a GPU option.\n\nTurn off the Internet: Finally, you need to work in offline mode as required by the competition organizer. Once all your files are downloaded, open the left-side window, select Session options, and turn off the internet connection. (screenshot)"
  },
  {
    "objectID": "posts/article3/index.html#importing-libraries-and-set-required-seed",
    "href": "posts/article3/index.html#importing-libraries-and-set-required-seed",
    "title": "Navigating the Kaggle U.S. Patent Phrase to Phrase Matching Competition: A Step-by-Step Guide",
    "section": "Importing libraries and Set Required Seed",
    "text": "Importing libraries and Set Required Seed\nNow we are ready to start the exciting part. We’ll be using PyTorch and Transformers, along with Matplotlib for some exploratory data analysis (EDA).\nimport pandas as pd\nimport transformers\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport torch\nfrom torch import optim\nfrom torch.utils.data import DataLoader, Dataset, random_split\nfrom torchmetrics.regression import PearsonCorrCoef\nimport numpy as np\nimport random\nimport timeit\nfrom tqdm import tqdm\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nfrom operator import itemgetter\nAdditionally, we’ll use a seed to ensure that the random numbers generated are reproducible. Reproducibility is essential for controlling randomness in both CPU and GPU operations, ensuring deterministic behavior. This is crucial for debugging and for ensuring that experiments can be replicated exactly.\nRANDOM_SEED = 42\n#MODEL_PATH = '/kaggle/input/debertav3base'\nMODEL_PATH = '/kaggle/input/debertav3small'\nMAX_LENGTH = 256\nBATCH_SIZE = 64\nLEARNING_RATE = 2e-5\nEPOCHS = 2\n\ndef set_random_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_random_seed(RANDOM_SEED)\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nThe code below helps control the amount of logging output by focusing only on error messages, which can be particularly useful for cleaner logs and debugging.\ntransformers.utils.logging.set_verbosity_error()"
  },
  {
    "objectID": "posts/article3/index.html#reading-datasets",
    "href": "posts/article3/index.html#reading-datasets",
    "title": "Navigating the Kaggle U.S. Patent Phrase to Phrase Matching Competition: A Step-by-Step Guide",
    "section": "Reading Datasets",
    "text": "Reading Datasets\nNow, let’s read our files:\ntrain_df = pd.read_csv('/kaggle/input/us-patent-phrase-to-phrase-matching/train.csv')\n\ntest_df = pd.read_csv('/kaggle/input/us-patent-phrase-to-phrase-matching/test.csv')\n\nsubmission_df = pd.read_csv('/kaggle/input/us-patent-phrase-to-phrase-matching/sample_submission.csv')\n\npatents_df = pd.read_csv('/kaggle/input/cpc-codes/titles.csv')"
  },
  {
    "objectID": "posts/article3/index.html#eda-warm-up",
    "href": "posts/article3/index.html#eda-warm-up",
    "title": "Navigating the Kaggle U.S. Patent Phrase to Phrase Matching Competition: A Step-by-Step Guide",
    "section": "EDA Warm Up",
    "text": "EDA Warm Up\nLet’s explore and extract some useful information from our datasets, first.\ntrain_df.head()\n\nThe above table tells us that the training set has five columns:\nid - a unique identifier for a pair of phrases\nanchor - the first phrase\ntarget - the second phrase\ncontext - the CPC classification, which indicates the subject within which the similarity is to be scored\nscore - the similarity. This is sourced from a combination of one or more manual expert ratings.\nWhat else can we get?\ntrain_df.score.hist()\nplt.title('Histogram of Scores')\n\nNote that there are only 5 potential scores:\n1.0 - Represents a very close match, typically indicating an exact match except for possible differences in conjugation, quantity (e.g., singular vs. plural), and the addition or removal of stopwords (e.g., “the,” “and,” “or”).\n0.75 - Indicates a close synonym, such as “mobile phone” vs. “cellphone.” This also includes abbreviations, such as “TCP” -&gt; “transmission control protocol.”\n0.5 - Denotes synonyms that do not have the same meaning (same function, same properties). This category includes broad-narrow (hyponym) and narrow-broad (hypernym) matches.\n0.25 - Indicates that the two phrases are somewhat related, such as being in the same high-level domain but not being synonyms. This category also includes antonyms.\n0.0 - Represents phrases that are unrelated.\nNow let’s have a closer look at the anchor column:\nax = train_df.groupby('anchor')['id'].count().sort_values(ascending=False).head(10).plot(kind='bar', color=list('gbymcr'))\nfor container in ax.containers:\n    ax.bar_label(container)\nplt.title('Top 10 Anchors')\nplt.show()\n\nHere, we can see the top 10 mostly used anchors, respectively. The total number of entities containing each anchor is written on top of its bar.\nThe test dataset is quite similar to the train dataset, except that it does not have the score column.\ntest_df.head()\n\nAnd our submission file at the end should look like this:\nsubmission_df.head()\n\nThe only dataset left is patents_df. But what information does it have inside? Let’s have a look.\npatents_df.head() \n\nRemember that earlier we suggested that we could represent the input to the model as something like “TEXT1: abatement; TEXT2: eliminating process”. We’ll need to apply that context here.\nTo begin, merge train_df and patent_df on the shared column: notice that train_df['context'] and patent_df['code'] contain similar information on both tables, so they’ll be the point of juncture.\nnext step, we will create a new column (input) on the new training dataset (updated_train_df). It combines updated_train_df['title'] and updated_train_df['anchor'].\nupdated_train_df = train_df.merge(patents_df, left_on='context', right_on='code') \nupdated_train_df['input'] = updated_train_df['title'] + \"; \" + updated_train_df['anchor']\nupdated_train_df.tail()\n\nDo the same steps for test_df:\nupdated_test_df = test_df.merge(patents_df, left_on='context', right_on='code')\nupdated_test_df['input'] = updated_test_df['title'] + \" \" + updated_test_df['anchor']"
  },
  {
    "objectID": "posts/article3/index.html#tokenization",
    "href": "posts/article3/index.html#tokenization",
    "title": "Navigating the Kaggle U.S. Patent Phrase to Phrase Matching Competition: A Step-by-Step Guide",
    "section": "Tokenization",
    "text": "Tokenization\nNow it’s time to tokenize our datasets… But what is it exactly?\n\nTokenization, in the realm of Natural Language Processing (NLP) and machine learning, refers to the process of converting a sequence of text into smaller parts, known as tokens.\n\nIn other words, we can’t pass the texts directly into a model. A deep learning model expects numbers as inputs, not English sentences! So we need to do two things:\n\nTokenization: Split each text up into tokens\nNumericalization: Convert each token into a number.\n\nThe details about how this is done depend on the particular model we choose. There are numerous models available, but for the starting point we will be using the two models that we have downloaded earlier.\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH, num_labels=1).to(device)\nThe num_labels=1 basically turns it into a regression problem, as to output only one score.\nYou can safely ignore the warnings here."
  },
  {
    "objectID": "posts/article3/index.html#loading-data",
    "href": "posts/article3/index.html#loading-data",
    "title": "Navigating the Kaggle U.S. Patent Phrase to Phrase Matching Competition: A Step-by-Step Guide",
    "section": "Loading Data",
    "text": "Loading Data\nIn the following cells we will build our custom dataset classes for training and testing that inherit from torch.utils.data.Dataset.\nclass TrainDataset(Dataset):\n    def __init__(self, inputs, targets, scores, tokenizer):\n        self.scores = scores\n        self.encodings = tokenizer(inputs, \n                                   targets, \n                                   padding=True, \n                                   truncation=True, \n                                   max_length=MAX_LENGTH)\n    \n    def __getitem__(self, idx):\n        out_dic = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        out_dic['scores'] = self.scores[idx]\n        return out_dic\n        \n    def __len__(self):\n        return len(self.scores)\nclass SubmitDataset(Dataset):\n    def __init__(self, inputs, targets, ids, tokenizer):\n        self.ids = ids\n        self.encodings = tokenizer(inputs, \n                                   targets, \n                                   padding=True, \n                                   truncation=True, \n                                   max_length=MAX_LENGTH)\n    \n    def __getitem__(self, idx):\n        out_dic = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        out_dic['ids'] = self.ids[idx]\n        return out_dic\n        \n    def __len__(self):\n        return len(self.ids)\nTaking TrainDataset for instance, let’s break it down into sections and see what it entails:\n\nInitialization (__init__ method):\n\nParameters:\n\ninputs: The input texts to be tokenized.\ntargets: The target texts or labels to be tokenized.\nscores: A list or array of scores corresponding to the inputs and targets.\ntokenizer: A tokenizer object (such as one from the Hugging Face Transformers library).\n\nAttributes:\n\nself.scores: Stores the scores parameter for later use in __getitem__.\nself.encodings: Stores the tokenized inputs and targets. The tokenizer processes the inputs and targets, applying padding, truncation, and limiting the length to MAX_LENGTH.\n\n\nGetting an Item (__getitem__ method):\n\nParameters: \n\nidx: The index of the item to retrieve.\n\nFunctionality:\n\nCreates a dictionary out_dic where each key-value pair from self.encodings is converted into a tensor. The value at index idx for each key is taken. \nAdds the score at index idx from self.scores to out_dic. \nReturns out_dic, which now contains the tokenized input and target data for the given index, along with the corresponding score.\n\n\nLength of Dataset (__len__ method):\n\nReturns the length of the dataset, which is determined by the number of scores. This assumes that the length of scores corresponds to the number of data points in the dataset.\n\n\nAfter tokenizing, let’s check our datasets:\ndataset = TrainDataset(updated_train_df['input'].to_list(), \n                       updated_train_df['target'].to_list(), \n                       updated_train_df['score'].to_list(), \n                       tokenizer)\n\ntest_dataset = SubmitDataset(updated_test_df['input'].to_list(), \n                             updated_test_df['target'].to_list(), \n                             updated_test_df['id'].to_list(), \n                             tokenizer)\n\nThere it is! Every input/output entity turned into a token and then into a number.\nNow we split our data into train and validation sets.\ngenerator = torch.Generator().manual_seed(RANDOM_SEED)\ntrain_dataset, val_dataset = random_split(dataset, [0.8, 0.2], generator=generator)\nWe use DataLoader after having our datasets to efficiently load and process the data in batches during training, validation, and testing of machine learning models. Therefore, it is easier to handle large datasets that don’t fit into memory all at once. It also provides additional functionalities like shuffling and batching, which are essential for training machine learning models effectively.\n(Tip: Always shuffle=True on the training set and shuffle=False on the validation set and test set. [source])\ntrain_dataloader = DataLoader(dataset=train_dataset, \n                             batch_size=BATCH_SIZE,\n                             shuffle=True)\n\nval_dataloader = DataLoader(dataset=val_dataset, \n                             batch_size=BATCH_SIZE,\n                             shuffle=False)\n\ntest_dataloader = DataLoader(dataset=test_dataset, \n                             batch_size=BATCH_SIZE,\n                             shuffle=False)"
  },
  {
    "objectID": "posts/article3/index.html#training-the-model",
    "href": "posts/article3/index.html#training-the-model",
    "title": "Navigating the Kaggle U.S. Patent Phrase to Phrase Matching Competition: A Step-by-Step Guide",
    "section": "Training the Model",
    "text": "Training the Model\nNow it’s time to train our model. Notice that I am using /kaggle/input/debertav3small (let’s call it Model 1) as the model path. We will go ahead with training this model first, then we can change the path to /kaggle/input/debertav3base (we refer to this as Model 2) and compare the results.\nBelow we train our model using the AdamW optimizer and evaluate it on the validation dataset. We then compute the Pearson correlation coefficient between the predicted scores and the ground truth scores.\n\nOptimization Initialization:\n\nInitialize the AdamW optimizer, which is a variant of the Adam optimizer with weight decay (W stands for weight decay).\n\nPearsonCorrCoef Initialization:\n\nInitialize an object of the PearsonCorrCoef class.\n\n\nThe Pearson correlation coefficient (r) is a number between –1 and 1 that measures the strength and direction of the relationship between two variables. When one variable changes, the other variable changes in the same direction.\n\nTraining Loop:\n\nIterate over the specified number of epochs (EPOCHS).\nFor each epoch:\n\nSet the model to training mode (model.train()).\nIterate over batches of data in the training dataset (train_dataloader).\n\nMove input data and targets to the device (e.g., GPU).\nCompute the model outputs and loss.\nCompute gradients and performs a parameter update (optimizer step).\nAccumulate the training loss.\n\nCompute the average training loss for the epoch.\nSet the model to evaluation mode (model.eval()).\nIterate over batches of data in the validation dataset (val_dataloader).\n\nCompute the model outputs without gradient computation (since no training is performed).\nAccumulate the validation loss.\nExtend preds and golds lists with predicted scores and ground truth scores, respectively.\n\nCompute the average validation loss for the epoch.\nCompute and print the Pearson correlation coefficient between predicted scores (preds) and ground truth scores (golds).\n\n\n(Side note: outputs[‘logits’].squeeze() would return the logits with any singleton dimensions removed, converting them into a 1D array if they were originally a 2D array with a singleton dimension.)\nTiming and Printing:\n\nCompute and prints the training time.\n\n\noptimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\npearson = PearsonCorrCoef()\nstart = timeit.default_timer()\n\nfor epoch in tqdm(range(EPOCHS), position=0, leave=True):\n    model.train()\n    train_running_loss = 0\n    \n    for idx, sample in enumerate(tqdm(train_dataloader, position=0, leave=True)):\n        input_ids = sample['input_ids'].to(device) # text1\n        attention_mask = sample['attention_mask'].to(device) # text2\n        targets = sample['scores'].to(device) # relativeness\n        \n        outputs = model(input_ids=input_ids, \n                        attention_mask=attention_mask, \n                        labels=targets)\n        \n        loss = outputs.loss\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        train_running_loss += loss.item()\n        \n    train_loss = train_running_loss / (idx + 1)\n    model.eval()\n    val_running_loss = 0\n    preds = []\n    golds = []\n    \n    with torch.no_grad():\n        for idx, sample in enumerate(tqdm(val_dataloader, position=0, leave=True)):\n            input_ids = sample['input_ids'].to(device)\n            attention_mask = sample['attention_mask'].to(device)\n            targets = sample['scores'].to(device)\n            \n            outputs = model(input_ids=input_ids, \n                            attention_mask=attention_mask, \n                            labels=targets)\n            \n            preds.extend([float(i) for i in outputs['logits'].squeeze()])\n            golds.extend([float(i) for i in targets])\n            \n            val_running_loss += outputs.loss.item()\n        val_loss = val_running_loss / (idx + 1)\n    \n    # printing out results \n    print('-' * 30)\n    print(f'Pearson Score: {float(pearson(torch.tensor(preds), torch.tensor(golds))):.3f}')\n    print(f'Train Loss EPOCH {epoch+1}: {train_loss:.3f}')\n    print(f'Valid Loss EPOCH {epoch+1}: {val_loss:.3f}')\n    print('-' * 30)\n    \nstop = timeit.default_timer()\nprint(f'Training Time: {stop-start: .2f}s')\nLet’s compare the performance of Model 1 (left side) and Model 2 (right side).\n\nAs you can see, Model 2 is more accurate than Model 1. It performs 4.6% better on the first epoch and 2.8% better on the second epoch. The training and validation loss of Model 2 is almost half of Model 1.\nHowever, the accuracy comes with the price of consumed time. Model 2 takes a bit over double the time of Model 1 to train. So, to conclude:\n\nModel 1 is faster and less accurate\nModel 2 is slower and more accurate\n\nWhat other models can we use to improve the performance up to 90% or even more? Perhaps one option would be to use Microsoft’s DEBERa-v3-large. What other suggestions you might have?"
  },
  {
    "objectID": "posts/article3/index.html#clear-the-memory",
    "href": "posts/article3/index.html#clear-the-memory",
    "title": "Navigating the Kaggle U.S. Patent Phrase to Phrase Matching Competition: A Step-by-Step Guide",
    "section": "Clear the Memory",
    "text": "Clear the Memory\nWe are close to the end. Make sure to free up your memory at this point.\ntorch.cuda.empty_cache()"
  },
  {
    "objectID": "posts/article3/index.html#testing-the-model",
    "href": "posts/article3/index.html#testing-the-model",
    "title": "Navigating the Kaggle U.S. Patent Phrase to Phrase Matching Competition: A Step-by-Step Guide",
    "section": "Testing the Model",
    "text": "Testing the Model\npreds = []\nids = []\nmodel.eval()\nwith torch.no_grad():\n    for idx, sample in enumerate(tqdm(test_dataloader)):\n        input_ids = sample['input_ids'].to(device)\n        attention_mask = sample['attention_mask'].to(device)\n        ids.extend(sample[\"ids\"])\n        \n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        preds.extend([float(i) for i in outputs[\"logits\"].squeeze()])"
  },
  {
    "objectID": "posts/article3/index.html#submission",
    "href": "posts/article3/index.html#submission",
    "title": "Navigating the Kaggle U.S. Patent Phrase to Phrase Matching Competition: A Step-by-Step Guide",
    "section": "Submission",
    "text": "Submission\nCongratulations on making it this far! Your code is now ready to be submitted. Save your predicted scores in a .csv file and submit the results. Kaggle will use a hidden testing dataset to evaluate your model. You will then be able to see how well your model performs on these unseen datasets.\nsubmission_df = pd.DataFrame(list(zip(ids, preds)),\n               columns =[\"id\", \"score\"])\nsubmission_df.to_csv(\"submission.csv\", index=False)\nsubmission_df.head()\nThank you for sticking with me up to this point. This has been my first time working on an NLP case competition, and it’s also my first time writing about it. I hope it provided some value to you. Let’s continue practicing and improving together!\nTo access my complete code for this Kaggle competition, click here."
  },
  {
    "objectID": "posts/article1/index.html",
    "href": "posts/article1/index.html",
    "title": "How to Build a Web App Using HuggingFace and Gradio Within an Hour",
    "section": "",
    "text": "Walkthrough of an end to end dog vs cat image classification model deployed on HuggingFace Spaces, supported by FastAI and Gradio.\nIt’s been a few weeks that I have started taking the fantastic deep learning course, fast.ai, by Jeremy Howard and it has been an amazing learning journey so far. I come from a STEM background but Howard’s style of teaching deep learning has brought a fresh perspective into learning this field. As I go through each lecture, I made the decision of documenting and sharing my experiences and learning outcomes publicly, so that it could inspire, help or encourage someone along the same path. This course has been a game-changer in my learning path so far, and I’m thrilled to talk about my first project in image classification.\nIn this very first article, I will guide you through deploying an image classification model using HuggingFace and Gradio. This method is beginner-friendly, straightforward and completely free. Whether you are a newcomer or looking to refine your deployment skills, I’ll walk you through each step, ensuring that by the end, you’ll be able to deploy your own models effortlessly. So, let’s get started on the exciting journey!"
  },
  {
    "objectID": "posts/article1/index.html#prerequisites",
    "href": "posts/article1/index.html#prerequisites",
    "title": "How to Build a Web App Using HuggingFace and Gradio Within an Hour",
    "section": "Prerequisites",
    "text": "Prerequisites\nBefore we begin, make sure you have the followings:\n\nA basic understanding of Python\nA HuggingFace account (sign up here)"
  },
  {
    "objectID": "posts/article1/index.html#getting-started",
    "href": "posts/article1/index.html#getting-started",
    "title": "How to Build a Web App Using HuggingFace and Gradio Within an Hour",
    "section": "Getting Started",
    "text": "Getting Started\nFirst thing, open the Google Colab and let’s make sure we have the necessary tools set up. Since we’re using the popular fastai library, you might need to install or upgrade it first with the following command.\n!pip install -Uqq fastai\nNow, install the one necessary package needed at this step.\nfrom fastai.vision.all import *"
  },
  {
    "objectID": "posts/article1/index.html#gathering-data",
    "href": "posts/article1/index.html#gathering-data",
    "title": "How to Build a Web App Using HuggingFace and Gradio Within an Hour",
    "section": "Gathering Data",
    "text": "Gathering Data\nFastai makes it incredibly easy for us to work with datasets thanks to its built-in function: untar_data(). This function streamlines the process of downloading and extracting datasets.\nIn our case, we use untar_data(URLs.PETS) to download a compressed dataset of pet images from a specified URL and extract it to a specific location on our local machine (path).\npath = untar_data(URLs.PETS)/'images'\nThe PETS dataset includes images of 37 breeds of pets along with annotations, which are perfect for training an image classification model."
  },
  {
    "objectID": "posts/article1/index.html#loading-data",
    "href": "posts/article1/index.html#loading-data",
    "title": "How to Build a Web App Using HuggingFace and Gradio Within an Hour",
    "section": "Loading Data",
    "text": "Loading Data\nNext, we need to load our data. ImageDataLoaders.from_name_func is a method we use that is provided by the fastai library. It is designed to help you create a DataLoader for training and validating image classification models. This method is particularly useful when you have a dataset of images and the labels can be inferred from the filenames of those images, which is basically what we have here.\nThe primary purpose of ImageDataLoaders.from_name_func is to create a data loader that can feed images and their corresponding labels to a machine learning model during training and validation phases. It streamlines the process of preparing data, handling transformations, and ensuring that the data is fed in batches to the model.\nBut how does it work? Let’s break it down into smaller pieces:\n\npath: The path to your dataset directory.\nget_image_files(path): A function from fastai that returns a list of all image file paths in the directory.\nlabel_func: A function that takes a file path and returns the label. In this example, it selects the label according to the first letter of the file’s name. If the name starts with a capital letter, the image belongs to a cat, and a dog otherwise.\nitem_tfms: Transformations applied to each image individually, such as resizing.\n\ndls = ImageDataLoaders.from_name_func('.', \n            get_image_files(path), valid_pct=0.2, seed=42, \n            label_func=is_cat, \n            item_tfms=Resize(192))\n\n\ndef is_cat(x): return x[0].isupper()          \nLet’s take a look at some randomly picked photos from our dataset.\ndls.show_batch()"
  },
  {
    "objectID": "posts/article1/index.html#training-the-model",
    "href": "posts/article1/index.html#training-the-model",
    "title": "How to Build a Web App Using HuggingFace and Gradio Within an Hour",
    "section": "Training the Model",
    "text": "Training the Model\nAfter setting up the DataLoader, the next step is to create and train a model using the fastai library. Here’s how you can do it:\nFirst, we initialize a vision_learner with our data loaders (dls), specifying the architecture we want to use—in this case, ResNet-152, a powerful and accurate convolutional neural network. We also specify the metrics we want to monitor during training, such as the error rate.\nlearn = vision_learner(dls, resnet152, metrics=error_rate)\nNext, we fine-tune the model using the fine_tune method:\nlearn.fine_tune(3)\nThis method fine-tunes the model for a specified number of epochs—in this case, 3 epochs. Fine-tuning involves training the model’s top layers, which are typically initialized with random weights, while gradually unfreezing and training the deeper layers of the pre-trained network. This process allows the model to adapt to our specific dataset while leveraging the powerful features learned by ResNet-152 on a much larger dataset.\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.051338\n0.039052\n0.010825\n01:58\n\n\n1\n0.026246\n0.006559\n0.002030\n01:58\n\n\n2\n0.010546\n0.004937\n0.001353\n01:57"
  },
  {
    "objectID": "posts/article1/index.html#downloading-the-model",
    "href": "posts/article1/index.html#downloading-the-model",
    "title": "How to Build a Web App Using HuggingFace and Gradio Within an Hour",
    "section": "Downloading the Model",
    "text": "Downloading the Model\nLet’s go ahead and download our model.\nlearn.export('model.pkl')"
  },
  {
    "objectID": "posts/article1/index.html#deployment",
    "href": "posts/article1/index.html#deployment",
    "title": "How to Build a Web App Using HuggingFace and Gradio Within an Hour",
    "section": "Deployment",
    "text": "Deployment\nFor the deployment, it’s very straightforward. We will be using HuggingFace Spaces. It is a platform that allows you to host and share machine learning models and demos easily.\nBefore going onto HuggingFace, we need to create our app.py and requirements.txt files so that we can upload them to the HF repo.\nIn the app.py file, we load the model that we saved previously, using load_learner('model.pkl'), and then classify our new unseen images using the Gradio interface. Gradio is a Python library that allows you to quickly create customizable web apps for your machine learning models and data processing pipelines.\nWhat we are basically doing here is setting up a web interface where users can upload images, and our model will classify these images and return the results. We call our classify_image function, specify our inputs and outputs, and optionally include some example images for users to test.\nfrom fastai.vision.all import *\nimport gradio as gr\n\ndef is_cat(x): return x[0].isupper() \n\nlearn = load_learner('model.pkl')\n\ncategories = ('Dog', 'Cat')\n\ndef classify_image(img):\n    pred,idx,probs = learn.predict(img)\n    return dict(zip(categories, map(float,probs)))\n\nimage = gr.Image()\nlabel = gr.Label()\nexamples = ['dog.jpg', 'cat.jpg', 'cat_dog.jpg']\n\nintf = gr.Interface(fn=classify_image, inputs=image, outputs=label, examples=examples)\nintf.launch(inline=False)\nWe need to create a requirements.txt file as mentioned before, and all we need to mention is the fastai library because HF doesn’t automatically include it. This file ensures that the necessary dependencies are installed when the app is deployed on HuggingFace Spaces.\n\nSetting up HuggingFace Spaces\nSo how do you set up the HF Space? First, go to the HuggingFace Spaces website. Once you’re there, click on “Create new Space”.\nChoose a Space name of your own. For the License, choose “apache-2.0”. Select Gradio as the Space SDK. Leave the rest as is and click on “Create Space”.\n\nOpen your local terminal and navigate to the directory where your required files, that you created previously, are saved. Once there, do the following:\ngit clone &lt;https://huggingface.co/spaces/&gt;&lt;your_login&gt;/&lt;your_app_name&gt;\ncd &lt;your_app_name&gt;\nHave your three files ready:\n\napp.py\nrequirements.txt\nmodel.pkl\n\nYou can commit and push the first two files as follow:\ngit add &lt;your_file_name&gt; \ngit commit -m 'Add application file'\ngit push\nHowever, to upload your model, you need a different approach since it’s a large file (over 25 MB). You need to first. Follow these steps:\n\nInstall Git: If you do not have Git installed locally, please download from here.\nDownload and install Git Large File Storage (LFS): Git LFS is an extension for Git that allows you to handle large files. Download and install it from here.\nSet up Git LFS: Type the following commands in your terminal:\ngit lfs install \ngit lfs track \"*.pkl\"\ngit add .gitattributes \nAdd and commit your model file:\ngit add model.pkl \ngit commit -m \"Add model\" \nPush your changes to the repository:\ngit push -u origin master"
  },
  {
    "objectID": "posts/article1/index.html#final-result",
    "href": "posts/article1/index.html#final-result",
    "title": "How to Build a Web App Using HuggingFace and Gradio Within an Hour",
    "section": "Final Result",
    "text": "Final Result\nAfter following all these steps, your app will show up on the screen in a few moments! You’ll see your HuggingFace Space with your deployed image classification model, ready to use and share with others!\n\nThanks for sticking with me up to this point! I hope you found this guide useful. Feel free to check out my app and all the source code by clicking here. Happy coding, and keep pushing boundaries!"
  },
  {
    "objectID": "posts/article7/index.html",
    "href": "posts/article7/index.html",
    "title": "Detecting Fraudulent Transactions Using Random Forest: A Step-by-Step Guide",
    "section": "",
    "text": "In this blog post, we’ll explore how to harness the power of Random Forest for effective fraud detection, especially when dealing with large and imbalanced datasets. We’ll break down the essentials of this algorithm and walk through its application in a Kaggle competition, offering practical insights and hands-on experience."
  },
  {
    "objectID": "posts/article7/index.html#kaggle-competition",
    "href": "posts/article7/index.html#kaggle-competition",
    "title": "Detecting Fraudulent Transactions Using Random Forest: A Step-by-Step Guide",
    "section": "Kaggle Competition",
    "text": "Kaggle Competition\nFrom here on out, we’ll be diving into the IEEE-CIS Fraud Detection Kaggle Competition. Our goal is to use a Random Forest classifier to predict the probability of an online transaction being fraudulent.\nThe data is split into two datasets: “identity” and “transaction,” which are linked by TransactionID. Keep in mind that not all transactions have matching identity information. You can check out more details about the competition here. Now, let’s get started with the implementation!"
  },
  {
    "objectID": "posts/article7/index.html#importing-libraries",
    "href": "posts/article7/index.html#importing-libraries",
    "title": "Detecting Fraudulent Transactions Using Random Forest: A Step-by-Step Guide",
    "section": "Importing Libraries",
    "text": "Importing Libraries\nWe begin by importing the necessary libraries. In this case, we will use the scikit-learn package for training our model and performing preprocessing on the dataset.\n# Data Analysis\nimport numpy as np\nimport pandas as pd\n\n# Data Visualization\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n# Machine Learning\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import confusion_matrix as cm\n\n# Warnings\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "posts/article7/index.html#loading-and-cleaning-data",
    "href": "posts/article7/index.html#loading-and-cleaning-data",
    "title": "Detecting Fraudulent Transactions Using Random Forest: A Step-by-Step Guide",
    "section": "Loading and cleaning data",
    "text": "Loading and cleaning data\nThe dataset we are given is very large, so we will create a function to reduce its memory usage by changing the data types of its columns to more memory-efficient types. This will help us achieve faster processing and consume less storage.\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    \n    start_mem = df.memory_usage().sum() / 1024**2    \n    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min &gt; np.iinfo(np.int8).min and c_max &lt; np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min &gt; np.iinfo(np.int16).min and c_max &lt; np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n      ...\n    \n    end_mem = df.memory_usage().sum() / 1024**2\n    \n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df\nNow, we are ready to load our data.\n# Loading train_transaction data\ntrain_transaction = pd.read_csv('../input/ieee-fraud-detection/train_transaction.csv')\nprint(train_transaction.shape)\ntrain_transaction = reduce_mem_usage(train_transaction)\ntrain_transaction.head()\n(590540, 394)\nMem. usage decreased to 542.35 Mb (69.4% reduction)\n\n\n\n\n\n# Loading train_identity data\ntrain_identity = pd.read_csv('../input/ieee-fraud-detection/train_identity.csv')\nprint(train_identity.shape)\ntrain_identity = reduce_mem_usage(train_identity)\ntrain_identity.head()\n(144233, 41)\nMem. usage decreased to 25.86 Mb (42.7% reduction)\n\n# Merging transaction and identity train data\ntrain_df = pd.merge(train_transaction, train_identity, how='left')\nprint(train_df.shape)\nlen_train_df = len(train_df)\ndel train_transaction, train_identity\ntrain_df.head()\n(590540, 434)\n\nYou can load and merge your testing dataset using a similar pattern.\nLastly, we need to create our submission DataFrame.\n# Creating a submission file\nsubmission = pd.DataFrame({'TransactionID' : test_df.TransactionID})\n# Duplicates check in train data\ntrain_df.duplicated().sum()\n0\nLuckily, there are no duplicates in our dataset!"
  },
  {
    "objectID": "posts/article7/index.html#preprocessing",
    "href": "posts/article7/index.html#preprocessing",
    "title": "Detecting Fraudulent Transactions Using Random Forest: A Step-by-Step Guide",
    "section": "Preprocessing",
    "text": "Preprocessing\nIn the previous section, we merged our transaction and identity training datasets, giving us a total of 434 columns, including the isFraud label. Our training dataset has 590,540 samples, and the testing dataset has 506,691 records. We also checked for any duplicates in the data.\nWe’ve noticed that there are quite a few missing values in our datasets. To handle this, we’ll keep columns with at least 80% of the data and drop those with more than 20% missing values. To make things easier, we’ll combine the training and testing datasets while we clean up the data.\ncombined_df = pd.concat([train_df.drop(columns=['isFraud', 'TransactionID']), test_df.drop(columns='TransactionID')])\n\n# Dependant variable\ny = train_df.isFraud\n\n# Dropping columns with more than 20% missing values\nmiss_val = combined_df.isnull().sum() / len(combined_df)\ncombined_pruned_df = combined_df.drop(columns=miss_val[miss_val &gt; 0.2].index)\n\ndel combined_df, train_df, test_df\nAs a result, we are left with only 180 columns out of 434 after removing those with more than 20% missing values. We have also removed the TransactionID column, as it does not provide useful information for prediction.\nThe next step is to address the missing values in the remaining columns. For numerical columns, we will impute missing values with the median, while for categorical columns, we will use the most frequent category.\n# Filtering numerical data\nnum_pruned_df = combined_pruned_df.select_dtypes(include=np.number)\nprint(num_pruned_df.shape)\n\n# Filtering categorical data\ncat_pruned_df = combined_pruned_df.select_dtypes(exclude=np.number)\nprint(cat_pruned_df.shape)\ndel combined_pruned_df\n\n# Filling missing values for numerical columns\nimp_median = SimpleImputer(missing_values=np.nan, strategy='median')\nnum_df = pd.DataFrame(imp_median.fit_transform(num_pruned_df), columns=num_pruned_df.columns)\ndel num_pruned_df\nprint(num_df.shape)\n\n# Filling missing values for categorical columns\nimp_max = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\ncat_df = pd.DataFrame(imp_max.fit_transform(cat_pruned_df), columns=cat_pruned_df.columns)\ndel cat_pruned_df\nprint(cat_df.shape)\n\n# Concatinating numerical and categorical columns\ncombined_df = pd.concat([num_df, cat_df], axis=1)\ndel num_df, cat_df\n\n# Checking for missing values\nprint(f'Total missing values: {combined_df.isnull().sum().sum()}')\nprint(combined_df.shape)\n(1097231, 176)\n(1097231, 4)\n(1097231, 176)\n(1097231, 4)\nTotal missing values: 0\n(1097231, 180)\nNext, we need to convert our categorical columns into numerical representations. We can accomplish this using the get_dummies method.\n# One-hot encoding\ncombined_df_encoded = pd.get_dummies(combined_df, drop_first=True)\nprint(combined_df_encoded.shape)\ndel combined_df\ncombined_df_encoded.head()\n(1097231, 245)\n\nAs you recall, we combined the training and testing datasets to speed up data cleaning. Now, we need to separate them again as we move forward with data manipulation on the training set.\n# Separating train and test data\nX = combined_df_encoded.iloc[:len_train_df]\nprint(X.shape)\ntest = combined_df_encoded.iloc[len_train_df:]\nprint(test.shape)\ndel combined_df_encoded\n(590540, 245)\n(506691, 245)\ntrain = pd.concat([X, y], axis=1)\ntrain.sort_values('TransactionDT', inplace=True)\nX = train.drop(columns=['isFraud'])\n# or X = train.drop(['isFraud'], axis=1)\ny = train['isFraud']\nsplitting_index = int(0.8 * len(X))\nX_train = X.iloc[:splitting_index]\nX_val = X.iloc[splitting_index:]\ny_train = y.iloc[:splitting_index]\ny_val = y.iloc[splitting_index:]\nprint(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\ndel train, y\n(472432, 245) (118108, 245) (472432,) (118108,)\nOne noteworthy point to mention is that you do not need to normalize or scale your data features for Random Forest. Scaling is primarily important in algorithms that are distance-based and rely on Euclidean distance. Random Forest is a tree-based model and, therefore, does not require feature scaling.\nFurthermore, I would argue that it’s not only unnecessary but also highly discouraged to scale features for Random Forest. By not scaling features, I was able to boost the accuracy by more than 7%.\nThe next thing we need to look into is the distribution of classes. We observed a significant class imbalance. This pattern is also evident in the validation set.\n# Class imbalance check\nprint(pd.value_counts(y_train))\nplt.pie(y_train.value_counts(), labels=['not Fraud', 'Fraud'], autopct='%0.1f%%', shadow='dict')\nplt.axis('equal')\nplt.show()\nisFraud\n0    455833\n1     16599\nName: count, dtype: int64\n\n\n\nPie chart for training dataset.\n\n\nplt.pie(y_val.value_counts(), labels=['not Fraud', 'Fraud'], autopct='%0.1f%%', shadow='dict')\nplt.axis('equal')\nplt.show()\n\n\n\nPie chart for validation dataset.\n\n\nThe class not Fraud is much more frequent than Fraud. Detecting the minority class (fraudulent transactions) is more important than the majority class (non-fraudulent transactions). Our first attempt was to use SMOTE (Synthetic Minority Over-sampling Technique), to over sample our minority class.\nAlthough this approach works well, it takes quite a long time to train. Instead, we will use the class_weight='balanced' argument in our RandomForestClassifier to improve training efficiency.\nIf you’re interested in checking out the SMOTE approach, I’ll share the link to the Kaggle notebook I worked on at the end. Feel free to take a look there!"
  },
  {
    "objectID": "posts/article7/index.html#machine-learning-model",
    "href": "posts/article7/index.html#machine-learning-model",
    "title": "Detecting Fraudulent Transactions Using Random Forest: A Step-by-Step Guide",
    "section": "Machine Learning model",
    "text": "Machine Learning model\nAs we mentioned earlier, we’ll be using a random forest to train our model. The main reason we’re choosing this algorithm is for practice. We’ll dive into its performance using ROC curves, a confusion matrix, and other metrics. We’ll also pinpoint any issues our model runs into and suggest some improvements for future work.\n\nTraining Model\nHere, we use 50% of the samples to train each base estimator and require a minimum of 80 samples to split an internal node. Instead of using SMOTE to approach class imbalance issue, we use the class_weight parameter to give more weight to the minority class. So far, this approach has yielded the best results. Another idea could be to reduce dimensionality using PCA, which is an interesting question to explore.\n# Random forest classifier\nrfc = RandomForestClassifier(class_weight='balanced', criterion='entropy', max_samples=0.5, min_samples_split=80)\nmodel = rfc.fit(X_train, y_train)\ny_probs = model.predict_proba(X_val)[:, 1] # probabilities for the positive class\n\n\nROC Curve\nThe ROC curve, which stands for Receiver Operating Characteristic curve, is a graphical representation of a binary classifier’s performance (in our case fraud/ not fraud) across different classification thresholds. It plots the True Positive Rate (TPR) against the False Positive Rate (FPR). You can see our ROC blue curve below.\nOn the other hand, ROC AUC, or Receiver Operating Characteristic Area Under the Curve, is a single metric that summarizes a classifier’s performance over all possible classification thresholds. The ROC AUC score is obtained by measuring the area under the ROC curve. In our example it is 0.89, which is the area under the blue curve [source].\nThe ROC AUC score indicates how well a classifier distinguishes between positive and negative classes, with values ranging from 0 to 1. A higher ROC AUC score signifies better performance: a perfect model achieves an AUC of 1, whereas a random model scores 0.5.\nDo you see why? If the model is performing poorly, the AUC would be equal to the area under the red dotted line, which is 0.5 ( = 1 * 1 / 2). The more concave the blue line appears, the more it resembles a rectangle, resulting in an area closer to 1 (since the units of the sides are one as well).\nprint(f'Validation AUC = {roc_auc_score(y_val, y_probs)}')\n\n# plotting ROC Curve\nfpr, tpr, threshold = metrics.roc_curve(y_val, y_probs[:])\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.figure(figsize=(12,6))\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label='AUC=%0.2f' % roc_auc)\n...\nplt.show()\nValidation AUC = 0.8904434324215795\n\n\n\n\n\n\n\nPrecision and Recall\nLet’s get back to basics. When you’re evaluating how well a classification model is performing, you need to be familiar with three key metrics: precision, recall, and F1 score. These are especially important when your data is imbalanced, or when the cost of different types of errors varies.\n\nPrecision is all about accuracy. Of all the items that the model labeled as positive, how many are actually positive?\nRecall is about coverage. Of all the actual positive items, how many were correctly identified by the model?\nAnd the F1 score? It’s like the perfect combo of both precision and recall, providing a single metric that balances the two. It’s super useful when you need a balance between them.\n\nWhen I was digging through different articles online to get a better grip on these terms, I stumbled upon a comprehensive article by Teemu Kanstrén. He points out that the basis of precision, recall, and F1 score comes from understanding concepts like True Positive, True Negative, False Positive, and False Negative.\nHere’s a quick table to break down what each term means:\n\nTrue/False Positive and Negative Definitions\n\n\n\n\n\n\n\n\nPrediction\nActual value\nType\nExplanation\n\n\n\n\n1\n1\nTrue Positive\nPredicted Positive and was Positive\n\n\n0\n0\nTrue Negative\nPredicted Negative and was Negative\n\n\n1\n0\nFalse Positive\nPredicted Positive but was Negative\n\n\n0\n1\nFalse Negative\nPredicted Negative but was Positive\n\n\n\nTo really get a handle on what each term means, I’ve put together a confusion matrix to illustrate it below (and don’t worry, we’ll dive into the details of the confusion matrix soon).\n\n\n\nConfusion Matrix. Image by Author.\n\n\nThese terms give us some pretty useful insights. For example, when the F1 score is at its peak, it means we have the best balance between precision and recall. It’s a good sign that the model is doing well at spotting positive cases without missing too many or making a lot of false positives.\nAt this maximum F1 score, we also get a specific threshold value. The threshold is used to decide if a predicted probability should be classified as positive or negative in classification models.\nFor example, if the threshold is set to 0.5, any prediction with a probability above 0.5 is classified as positive (fraudulent), and anything below 0.5 is classified as negative (not fraudulent).\n# Calculate precision and recall for different thresholds\nprecision, recall, thresholds = precision_recall_curve(y_val, y_probs)\n\n# Calculate F1 score\nf1_scores = 2 * (precision * recall) / (precision + recall)\n\n# Find the threshold that gives the maximum F1 score\noptimal_threshold = thresholds[np.argmax(f1_scores)]\n\nprint(f\"Optimal Threshold: {optimal_threshold}\")\nOptimal Threshold: 0.650943583810778\n\n\nConfusion Matrix\nIn our case, the threshold is set to 0.65. We’ll use this value to label our predicted probabilities as 0 or 1 because the confusion matrix only works with discrete numbers, not probabilities.\nA confusion matrix is a table that helps us evaluate how well our classification model is performing. It shows counts for true positives, true negatives, false positives, and false negatives. We’ve already covered these terms in a table in the previous section.\n# Confusion matrix\ny_probs_labels = (y_probs[:] &gt;= optimal_threshold).astype(int)\n\n# y_val contains the true binary labels\nconf_matrix = cm(y_val, y_probs_labels)\n\ncategories = ['Zero', 'One']\n\n# Display the confusion matrix\nsns.heatmap(\n    conf_matrix / np.sum(conf_matrix),  # Normalized by the total sum for percentages\n    annot=True,  # Show the annotations\n    annot_kws={'size': 16},  # Size of annotations\n    cmap='Blues',  # Color map\n    fmt='.2%',  # Format the annotation to show percentage\n    xticklabels=categories,  # Set x-axis labels\n    yticklabels=categories,  # Set y-axis labels\n    cbar_kws={'label': 'Percentage of Total'}  # Add color bar with label\n)\n\nplt.xlabel('Predicted Labels')  # X-axis label\nplt.ylabel('True Labels')  # Y-axis label\nplt.title('Normalized Confusion Matrix')  # Title of the plot\nplt.show()\nLooking at the figure below, we can get some useful insights. The total of all the percentages should add up to 100. Ideally, the diagonal boxes (representing correct predictions) should have the highest percentages. The 95.46% portion refers to True Negatives, which are the non-fraudulent transactions. Our model did a pretty good job here, as this aligns closely with the original 96.6% of non-fraudulent transactions (or 114,092 in total) in the validation dataset (take a look at the pie charts above again).\nHowever, the model only detected 1.41% of the 3.4% fraudulent transactions, missing almost half of them. This is concerning because we need a system that can reliably spot any fraudulent transactions. Since there are far more “normal” transactions than “fraud” ones, the model tends to generalize that almost everything is safe. So, while our model didn’t perform well in this area, there’s definitely room for improvement!\n\n\n\n\n\n\n\nFeature Importance\nWe’ve already talked about why feature importance is important and what it’s used for. Now, let’s take a look at the top 20 most important features in our dataset.\n# Create a pandas Series from the feature importances\nfeature_series = pd.Series(rfc.feature_importances_, index=X.columns)\n\n# Get the top 20 features in descending order\ntop_features = feature_series.nlargest(20)\n\n# Create a DataFrame for seaborn\ntop_features_df = top_features.reset_index()\ntop_features_df.columns = ['Feature', 'Importance']\n...\n\nplt.show()"
  },
  {
    "objectID": "posts/article7/index.html#predict-and-submit",
    "href": "posts/article7/index.html#predict-and-submit",
    "title": "Detecting Fraudulent Transactions Using Random Forest: A Step-by-Step Guide",
    "section": "Predict and submit",
    "text": "Predict and submit\nAnd finally, we need to test our unseen dataset and get ready to submit our results to the leaderboard!\n# Predicting the test data\npredictions = rfc.predict_proba(test)\nprint(predictions.shape)\nsubmission['isFraud'] = predictions[:, 1]\nprint(submission.shape)\nsubmission.head()\n(506691, 2)\n(506691, 2)\n\n\n\n\n\n# Submitting results\nsubmission.to_csv('submission.csv', index=False)\nprint('Submission is successful !')\nSubmission is successful !\nAnd that’s a wrap! If you could follow along without any issues, it means I’ve done a good job of sharing my knowledge! And if there was any ambiguity or you just want to share any comments or suggestions, let me know!\nLastly, I wanted to share some useful links that helped me along the way as well as my written notebook on Kaggle.\nCheers :)\n\nIEEE-CIS Fraud Detection: Random Forest Classifier\nieee-cis-fraud-detection-random-forest\nExtensive EDA and Modeling XGB Hyperopt\nHow to explain gradient boosting\nMy Notebook"
  },
  {
    "objectID": "posts/article6/index.html",
    "href": "posts/article6/index.html",
    "title": "How to Create a Web App with Django and YOLOv10 Models on Your Custom Dataset",
    "section": "",
    "text": "In this blog, we will learn about YOLOv10 models and how to train one on a custom dataset to develop a real-time object detection web application using Django.\nMachine learning, a subset of artificial intelligence, has revolutionized data handling and prediction making. Among its various applications, computer vision is particularly noteworthy, allowing machines to interpret and comprehend visual data similarly to humans.\nThe influence of computer vision is profound, revolutionizing healthcare, advancing automation, and bolstering security measures. Real-time object detection enables accurate identification of object categories and positions in images with minimal delay. The YOLO series has led the charge in this area, striking a perfect balance between performance and efficiency."
  },
  {
    "objectID": "posts/article6/index.html#yolov10-overivew",
    "href": "posts/article6/index.html#yolov10-overivew",
    "title": "How to Create a Web App with Django and YOLOv10 Models on Your Custom Dataset",
    "section": "YOLOv10 Overivew",
    "text": "YOLOv10 Overivew\nYOLO stands for You Only Look Once. It’s a family of models designed for real-time object detection. From image pixels to bounding box coordinates, the main idea behind YOLO is to frame object detection as a single regression task.\nA few months ago, researchers at Tsinghua University unveiled YOLOv10, a novel approach to real-time object detection. This new model addresses the post-processing and architectural shortcomings of previous YOLO versions. By eliminating non-maximum suppression (NMS), YOLOv10 achieves state-of-the-art performance with significantly reduced computational overhead.\n\nModel Variants\nYOLOv10 comes in various model scales to address different application needs:\n\nYOLOv10-N: Nano version for extremely resource-constrained environments.\nYOLOv10-S: Small version balancing speed and accuracy.\nYOLOv10-M: Medium version for general-purpose use.\nYOLOv10-B: Balanced version with increased width for higher accuracy.\nYOLOv10-L: Large version for higher accuracy at the cost of increased computational resources.\nYOLOv10-X: Extra-large version for maximum accuracy and performance.\n\n\n\nArchitecture Design\nThe YOLOv10 architecture builds upon the strengths of previous YOLO models while introducing key innovations for even better performance.\n\n\n\n\n\nThe components of the architecture shown in Figure 2 above are as follows:\n\nInput Image:\n\nThe model takes an image as input. This image is usually resized to a fixed size (e.g., 640x640 pixels).\n\nBackbone:\n\nThis is a convolutional neural network (CNN) that extracts features from the input image. The backbone is often a modified version of a well-known CNN like CSPDarknet or a more efficient variant.\nThe backbone consists of several convolutional layers that help detect edges, textures, patterns, and more complex structures.\n\nNeck:\n\nThe neck connects the backbone to the head of the model. It consists of additional layers that further process the features extracted by the backbone.\nYOLOv10 uses techniques like PANet (Path Aggregation Network) to improve feature fusion from different layers, enhancing the detection capability for objects at different scales.\n\nHead:\n\nThe head is responsible for predicting bounding boxes and class probabilities.\nOne-to-Many Head: Generates multiple predictions per object during training to provide rich supervisory signals and improve learning accuracy.\nOne-to-One Head: Generates a single best prediction per object during inference to eliminate the need for NMS, thereby reducing latency and improving efficiency.\nYOLOv10 uses anchor boxes to predict the location and size of objects. Anchor boxes are predefined boxes of different shapes and sizes that help the model generalize better to various object shapes.\nThe head outputs three types of information for each anchor box: the coordinates of the bounding box, the objectness score (how likely it is that an object is present), and the class probabilities (what kind of object it is).\n\nOutput:\n\nThe model outputs a grid of predictions, where each cell in the grid predicts multiple bounding boxes and their corresponding class probabilities.\nNon-Maximum Suppression (NMS) is used to filter out overlapping boxes, keeping only the best ones to avoid duplicate detections.\n\n\n\n\nKey Features\n\n\nNo NMS Training: Uses a new method to avoid non-maximum suppression (NMS), speeding up the inference (prediction) process.\nComplete Model Design: Optimizes different parts of the model for better speed and accuracy, including lighter classification heads and improved downsampling techniques.\nImproved Model Features: Adds larger convolutions and self-attention techniques to boost performance without adding much extra computing power.\n\n\n\nPerformance Comparison\n\n\n\n\n\nYOLOv10 models, including YOLOv10-S and YOLOv10-X, offer significant speed advantages, being 1.8× and 1.3× faster than RT-DETR-R18 and R101 while maintaining comparable accuracy. Additionally, YOLOv10-B has 25% fewer parameters and 46% lower latency than YOLOv9-C, achieving the same level of accuracy. Furthermore, YOLOv10-L and YOLOv10-X exceed the performance of YOLOv8-L and YOLOv8-X by 0.3 AP and 0.5 AP, respectively, while requiring 1.8× and 2.3× fewer parameters. Overall, YOLOv10 demonstrates a compelling balance of efficiency and performance compared to other state-of-the-art detectors."
  },
  {
    "objectID": "posts/article6/index.html#custom-model",
    "href": "posts/article6/index.html#custom-model",
    "title": "How to Create a Web App with Django and YOLOv10 Models on Your Custom Dataset",
    "section": "Custom Model",
    "text": "Custom Model\n\nNow, let’s dive into the exciting part—implementation!\nFor our web application’s implementation, we will start by creating our custom model. For this first part, we will use Google Colab. Make sure to set the runtime to T4 GPU in the Runtime section. Once we have our new set of weights, we can download them and move on to Visual Studio Code to set up our Django environment. Let’s begin!\n\nInstall Ultralytics packages\nBefore we start coding, we need to install the YOLOv10 package directly from the GitHub repository. We will also need to install Roboflow to access our dataset.\n!pip install -q git+https://github.com/THU-MIG/yolov10.git\n!pip install -q roboflow\n\n\nDownload Pre-trained Model Weights\nIn this part, we are creating a new repository in our Colab environment called weights. This repository will store all the pre-trained model weights for all YOLOv10 models.\nimport os\nimport urllib.request\n\n# create a directory for weights in the current working directory\nweights_dir = os.path.join(os.getcwd(), 'weights')\nos.makedirs(weights_dir, exist_ok=True)\n\n# URLs of the weight files\nurls = [\n    'https://github.com/THU-MIG/yolov10/releases/download/v1.1/yolov10n.pt',\n    'https://github.com/THU-MIG/yolov10/releases/download/v1.1/yolov10s.pt',\n    'https://github.com/THU-MIG/yolov10/releases/download/v1.1/yolov10m.pt',\n    'https://github.com/THU-MIG/yolov10/releases/download/v1.1/yolov10b.pt',\n    'https://github.com/THU-MIG/yolov10/releases/download/v1.1/yolov10x.pt',\n    'https://github.com/THU-MIG/yolov10/releases/download/v1.1/yolov10l.pt'\n]\n\n# download files\nfor url in urls:\n    file_name = os.path.join(weights_dir, os.path.basename(url))\n    urllib.request.urlretrieve(url, file_name)\n    print(f'Downloaded {file_name}')\n\n\nDownload datasets from Roboflow\nFor the custom dataset collection, I am using Roboflow Universe. It contains over 500,000 datasets and supports our model type. You can search for any keyword to find images in the search bar. An example of a dataset I used for training my model includes over 2,900 images of people and cars.\nOnce you’ve chosen your dataset, click on the Download Dataset button in the top right corner. Next, select YOLOv8 as the format and press Continue. You should now see a code snippet similar to the one shown in the picture below. Copy this code into your Colab notebook and run it.\n\n\n\n\n\n\n\nFine-tune the Model\nOnce your dataset and pre-trained weights are downloaded, they will appear in the Files menu on the left sidebar. To find the path to your model, open the weights directory, select the model of your choice, copy its path, and paste it into your model’s path. For example, the path to the model yolov10m.pt is /content/weights/yolov10m.pt.\n!yolo task=detect mode=train epochs=100 batch=64 \\\nmodel='/path/to/your/model' \\\ndata='/path/to/data.yaml'\nFor the data, click on the file name specific to your dataset. You will see a data.yaml file. Copy its path into the data’s path in your code. Before running the cell, you need to modify this data.yaml file. Double-click on the file. It should look like this:\n\n\n\n\n\nYou need to modify the paths to test, train, and val as shown above. Inside your dataset folder, there are test, train, and valid folders each containing images and labels. Copy the path for images for each of these three and paste them into their corresponding name.\n\n\n\n\n\nNow save your changes and run the code to start training!\n\n\nVisualization and Analysis\nUpon completion of your training process, a new directory called runs will be created. This folder contains information about the confusion matrix, recall, precision, and mAP, allowing us to evaluate our model’s performance. Let’s take a closer look:\nTo find the path to the confusion matrix, navigate to the runs directory, then go to the detect folder and the train subfolder. You will see a file named confusion_matrix.png; copy its path below.\nfrom IPython.display import Image\n\nImage(filename='/path/to/confusion_matrix.png', width=600)\nIdeally, we want all the diagonal cells from the top left to the bottom right to be dark blue, indicating that the predicted objects match the actual ones. The figure below shows that, given our number of iterations, the model nearly perfectly identified all people in most images (2006 out of 2387). However, it only predicted cars correctly in 208 images.\n\n\n\n\n\nNext, let’s look at the results.png file, which can also be found in the same directory as the confusion matrix.\nImage(filename='/path/to/results.png', width=600)\n\n\n\n\n\nAt a glance, we can see that our results align well with the smooth lines, which is promising! Let’s delve into the graphs for more insights.\nFor instance, the metrics/recall (B) indicates that most of the actual positive instances were correctly identified by the model.\nSimilarly, the metrics/precision (B) graph reveals that a significant majority of the items labeled as positive by the model are indeed accurate.\nOverall, these are solid results!\nAnother key graph is the mAP (mean Average Precision), which plays a crucial role in model selection. While we’re currently looking at the results for YOLOv10m, previous comparisons showed that YOLOv10 outperformed all other state-of-the-art models in terms of COCO AP %.\n\n\nTesting the Model\nThe final step before we close our Google Colab notebook is testing our model’s performance on the unseen dataset. inside the runs/detect/train/weights directory choose your best performing model, best.pt, copy its path into the model_path. And the path to the test image files is in your dataset folder.\nfrom ultralytics import YOLOv10\n\nmodel_path = '/path/to/model/best.pt'\nmodel = YOLOv10(model_path)\nresults = model(source='/path/to/test/images', conf=0.25, save=True)\nYou can open a few of the resulting images to check if the detections were correct. Before proceeding to the next step, download the yolov8n.pt file and save it in a local directory where you plan to create your web app."
  },
  {
    "objectID": "posts/article6/index.html#web-app",
    "href": "posts/article6/index.html#web-app",
    "title": "How to Create a Web App with Django and YOLOv10 Models on Your Custom Dataset",
    "section": "Web App",
    "text": "Web App\nA few blogs ago, we learned how to use the Flask framework for a binary image classification web app. Now, we are going to use a new web framework called Django to develop a computer vision app.\nDjango’s scalability, versatility, rapid development, and security advantages make it a great platform to learn and a valuable skill to have in the software development world.\nIf you are new to Django, this tutorial is a great reference for creating apps. Give it a look, and once you’re ready, come back and continue here with me.\n\nSetting up Virtual Environment\nCreate your web app folder and open it with VS Code. The next step is to create a virtual environment. I suggest using Mamba, and the process is as follows:\nmamba create -n your_env_name\nmamba activate your_env_name\nNow, in your terminal, type which python. The path for your Python interpreter should match the one used by your virtual environment. If it doesn’t, go to View -&gt; Command Palette…, and select the Python version from the path of your virtual environment.\n\n\nInstall Django\nLet’s go ahead and install Django:\npip install django\n\n\nStart project\nHere, we create our app inside our project’s directory:\nPython3 -m django startproject mysite\ncd mysite\nThis will create all the necessary, basic website files such as settings and urls routing.\n\n\nAdd Application\nHere we create our computer vision app called object_detection:\npython manage.py startapp object_detection\nWhile in the outer mysite directory, create a media folder. This folder will store all input and output files. Place the model you downloaded earlier (yolovv8n.pt) in mysite folder as well. Your project’s structure should now look like this:\n├── myproject\n|   └── mysite\n│       └── mysite\n|       └── object_detection\n|       └── manage.py\n|       └── yolov8n.pt\n|       └── media\n|\n\n\nCreate Templates - Frontend\nWe need to create the templates folder inside our object_detection. It contains base.html and obj_dtc.html. The base.html contains the main navigation bar, where in our case is the only object detection task.\n&lt;nav class=\"navbar navbar-expand-sm bg-dark navbar-dark\"&gt;\n  &lt;div class=\"collapse navbar-collapse\" id=\"collapsibleNavbar\"&gt;\n    &lt;ul class=\"navbar-nav\"&gt;\n      &lt;li class=\"nav-item\"&gt;\n        &lt;a class=\"nav-link\" href=\"obj_dtc\"&gt;Object Detection&lt;/a&gt;\n      &lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/div&gt;\n&lt;/nav&gt;\nthe href will route to the configured url. We will set the configurations in object_detection/urls.py.\nobj_dtc.html will display the input and output media requested and received by the user.\n{% if original_video or original_image %}\n    &lt;div class=\"row justify-content-center mt-4\"&gt;\n        &lt;div class=\"col-md-5\"&gt;\n            &lt;h2&gt;Original Media:&lt;/h2&gt;\n            {% if original_video %}\n            &lt;video width=\"450\" height=\"283\" controls src=\"{{ original_video }}\"&gt;\n            &lt;/video&gt; \n            {% elif original_image %}\n                &lt;img src=\"{{ original_image }}\" alt=\"Original\" width=\"450\" height=\"283\"&gt;\n            {% endif %}\n        &lt;/div&gt;\n        &lt;div class=\"col-md-5\"&gt;\n            &lt;h2&gt;Result Media:&lt;/h2&gt;\n            {% if original_video %}\n                &lt;video width=\"450\" height=\"283\" controls src=\"{{ result_media }}\"&gt;\n                &lt;/video&gt; \n            {% else %}\n                &lt;img src=\"{{ result_media }}\" alt=\"result media\" width=\"450\" height=\"283\"&gt;\n            {% endif %}\n        &lt;/div&gt;\n    &lt;/div&gt;\n{% endif %}\nFor Django to be able to find our custom templates, we need to add the TEMPLATES path in settings.py as follows:\nTEMPLATES = [\n    {\n        ...\n        \"DIRS\": [BASE_DIR / 'object_detection/templates'],\n        ...\n    },\n]\n\n\nURLs Routing\nIn object_detection/urls.py we need to fetch the base and obj_dtc functions (which we’ll discuss in the backend section).\nfrom django.urls import path\nfrom . import views\n\nurlpatterns = [\n    path('', views.base, name='base'), \n    path('obj_dtc', views.obj_dtc, name='obj_dtc') \n    \n]\nWe need to let Django know of the existence of these functions’ requests. So, we need to specify their routes in mysite/urls.py.\nfrom django.contrib import admin \nfrom django.urls import include, path\nfrom django.conf.urls.static import static\nfrom django.conf import settings\nfrom object_detection import views as object_detection_views\n\n\n\nurlpatterns = [\n    path('admin/', admin.site.urls),\n    path('', object_detection_views.base, name='base'), \n    path('object_detection/', include('object_detection.urls'))\n    \n] + static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT)\nMEDIA_URL refers to the path for the /media directory, which contains all the image files (both input and output) used by the code. It’s essential to configure this URL so that the templates can locate these images when they are sent from the backend. In other words, the /media directory serves as the connection point that links the backend and the frontend.\n\n\nBackend\nWe write all our backend code in object_detection/views.py. It contains four important functions. Let’s take a look at each of them one by one.\n\nbase : This function get the request from the user and renders the results in base.html.\n\ndef base(request):\n    return render(request, 'base.html')\n\nload_mode: In this function we are creating a YOLO object that gets in our custom model as input.\n\ndef load_model():\n    model_name='yolov8n.pt'\n    model = YOLO(model_name)\n    return model\n\nget_prediction: here we pass in the path to the input image/video file uploaded by the user, as well as the model, which is YOLOv10 in our case. The output of this function is the relative path to our output image/video file which will be saved inside our /media directory.\n\ndef get_prediction(media_path, model):\n    \n    result = model(media_path, conf=0.25, save=True, project=settings.MEDIA_ROOT) \n    relative_results_dir = os.path.relpath(result[0].save_dir, settings.MEDIA_ROOT)\n    \n    return relative_results_dir\n\nobj_dtc: A lot is happening in this function. Let’s break it down:\n\n\nFirst, get the request from the user. Retrieve the file name and its full path, and save it in media_full_path.\nEnsure the path exists; if not, inform the user.\nDepending on whether the input file is an image or a video, update the context dictionary. The keys in the dictionary are used to distinguish inputs and outputs, and the values are their corresponding paths (where they have been saved).\nRender the result inside the obj_dtc.html file.\n\ndef obj_dtc(request):\n    if request.method == 'POST' and request.FILES['myfile']:\n        myfile = request.FILES['myfile']\n        fs = FileSystemStorage()\n        filename = fs.save(myfile.name, myfile)\n        media_path = fs.url(filename)\n\n        model = load_model()\n        media_full_path = os.path.join(settings.MEDIA_ROOT, filename)\n        \n        results_dir = os.path.join(get_prediction(media_full_path, model), filename) \n\n        if not os.path.exists(os.path.join(settings.MEDIA_ROOT, results_dir)):\n            return render(request, 'obj_dtc.html', {'error': 'Error in saving predictions'})\n\n        \n        if myfile.name.endswith(('.mp4', '.avi')):\n            context = {'original_video': media_path, 'result_media': os.path.join(settings.MEDIA_URL, results_dir)}\n        else:\n            context = {'original_image': media_path, 'result_media': os.path.join(settings.MEDIA_URL, results_dir)}\n\n        return render(request, 'obj_dtc.html', context)\n      \n    return render(request, 'obj_dtc.html')\n\n\n\n\n\nDeploy Locally\nLocate to the directory where manage.py is saved, and run the following command:\npython manage.py runserver\nThere you’ll see your app up and running!\n\nThis was a really fun project, and I absolutely enjoyed playing around with the code. The successful deployment of this idea was made possible by the diligent work of the researchers at Tsinghua University. You can read their paper to learn more about their model design and methodology. For a more hands-on approach to deployment, here is the link to their repository."
  },
  {
    "objectID": "posts/article4/index.html",
    "href": "posts/article4/index.html",
    "title": "How to Build a Music Genre Classifier Using Hugging Face Transformers and Gradio",
    "section": "",
    "text": "In this blog, we will build a music genre classification system using the GTZAN dataset to identify the genre of a given audio track.\nHave you ever been curious about how machine learning models classify music genres? What features in the dataset are useful for the model’s understanding? And how can you deploy your trained model for users? If these questions have crossed your mind, then keep reading as I guide you through everything you need to quickly deploy a music classification app. By the end of this post, you will have a fully functional music genre classification model capable of predicting the genre of any audio track. You will also have a Gradio-based interactive interface to test and visualize the model’s predictions, and the model will be ready for deployment using the Hugging Face Hub."
  },
  {
    "objectID": "posts/article4/index.html#load-and-prepare-the-dataset",
    "href": "posts/article4/index.html#load-and-prepare-the-dataset",
    "title": "How to Build a Music Genre Classifier Using Hugging Face Transformers and Gradio",
    "section": "Load and Prepare the Dataset",
    "text": "Load and Prepare the Dataset\nWe start by loading the GTZAN dataset using the datasets library and split it into training and test sets. The reason for using GTZAN is that it’s a popular dataset containing 1,000 songs for music genre classification. Each song is a 30-second clip from one of 10 genres of music, spanning blues to rock.\nfrom datasets import load_dataset\n\n# Load the GTZAN dataset\ngtzan = load_dataset('marsyas/gtzan', 'all')\n\n# Split the dataset into training and test sets\ngtzan = gtzan['train'].train_test_split(seed=42, shuffle=True, test_size=0.1)\n\n# How does the dataset look like\nprint(gtzan)\n\n# Create a function to convert genre ID to genre name\nid2label_fn = gtzan['train'].features['genre'].int2str\n\n# Example of converting a genre ID to genre name\nprint(id2label_fn(gtzan['train']['genre'][1]))\n\nOutput\nDatasetDict({\n    train: Dataset({\n        features: ['file', 'audio', 'genre'],\n        num_rows: 899\n    })\n    test: Dataset({\n        features: ['file', 'audio', 'genre'],\n        num_rows: 100\n    })\n})\nclassical\n\n\nExplanation\n\nLoading Dataset: load_dataset('marsyas/gtzan', 'all') loads the GTZAN dataset.\nSplitting Dataset: train_test_split splits the dataset into training and validation sets with 90% training and 10% validation.\nLabel Conversion Function: int2str() maps numeric genre IDs to their corresponding genre names (human-readable names)."
  },
  {
    "objectID": "posts/article4/index.html#generate-audio-samples-with-gradio",
    "href": "posts/article4/index.html#generate-audio-samples-with-gradio",
    "title": "How to Build a Music Genre Classifier Using Hugging Face Transformers and Gradio",
    "section": "Generate Audio Samples with Gradio",
    "text": "Generate Audio Samples with Gradio\nAs you have seen in the previous section, our dataset contains three types of features: file, audio, and genre. We learned about genre and now let’s have a closer look at audio and figure out what’s inside of it.\ngtzan[\"train\"][0][\"audio\"]\n\nOutput\n{'path': '/root/.cache/huggingface/datasets/downloads/extracted/5022b0984afa7334ff9a3c60566280b08b5179d4ac96a628052bada7d8940244/genres/pop/pop.00098.wav',\n 'array': array([ 0.10720825,  \n                  0.16122437,  \n                  0.28585815, \n                  ..., \n                  -0.22924805,\n                  -0.20629883, \n                  -0.11334229]\n                ),\n 'sampling_rate': 22050}\nAs you can see, the audio file is represented as 1-dimensional NumPy array. But what does the value of array represent? And what is sampling_rate?\n\n\nSampling and Sampling Rate\nIn signal processing, sampling refers to the process of converting a continuous signal (such as sound) into a discrete signal by taking periodic samples.\n\n\n\n\n\n\n\nWikipedia article: Sampling (signal_processing)\n\n\nIn our example of audio sampling, sampling rate (or sampling frequency) refers to the number of samples of audio carried per second. It is usually measured in Hertz (Hz). To put it in perspective, standard media consumption has a sampling rate of 44,100 Hz, meaning it takes 44,100 samples per second. In comparison, high-resolution audio has a sampling rate of 192,000 Hz (192 kHz). For training speech models, a commonly used sampling rate is 16,000 Hz (16 kHz).\n\n\nAmplitude\nWhen we talk about the sampling rate in digital audio, we refer to how often samples are taken. But what do these samples actually represent?\nSound is produced by variations in air pressure at frequencies that are audible to humans. The amplitude of a sound measures the sound pressure level at any given moment and is expressed in decibels (dB). Amplitude is perceived as loudness; for example, a normal speaking voice is typically under 60 dB, while a rock concert can reach around 125 dB, which is near the upper limit of human hearing.\nIn digital audio, each sample captures the amplitude of the audio wave at a specific point in time. For instance, in our sample data gtzan[\"train\"][0][\"audio\"], each value in the array represents the amplitude at a particular timestep. For these songs, the sampling rate is 22,050 Hz, which means there are 22,050 amplitude values recorded per second.\nOne thing to remember is that all audio examples in your dataset have the same sampling rate for any audio-related task. If you intend to use custom audio data to fine-tune a pre-trained model, the sampling rate of your data should match the sampling rate of the data used to pre-train the model. The sampling rate determines the time interval between successive audio samples, therefore impacting the temporal resolution of the audio data.\nTo read more on this topic click here.\n\n\nGradio\nNow that we better understand our dataset let’s create a aimple and interactive UI with the Blocks API to visualize some audio samples and their labels.\nimport gradio as gr\n\n# Function to generate an audio sample\ndef generate_audio():\n    example = gtzan[\"train\"].shuffle()[0]\n    audio = example[\"audio\"]\n    return (audio[\"sampling_rate\"], audio[\"array\"]), id2label_fn(example[\"genre\"])\n\n# Create a Gradio interface to display audio samples\nwith gr.Blocks() as demo:\n    with gr.Column():\n        for _ in range(4):\n            audio, label = generate_audio()\n            output = gr.Audio(audio, label=label)\n\n# Launch the Gradio demo\ndemo.launch(debug=True)\n\nOutput\n\n\n\n\n\n\n\nExplanation\n\nGenerating Audio: generate_audio() randomly selects and returns an audio sample from the training set.\nGradio Interface: Gradio Blocks and Column create a layout to display audio samples. gr.Audio adds audio players with labels to the interface.\nLaunching Interface: demo.launch(debug=True) starts the Gradio interface for interaction."
  },
  {
    "objectID": "posts/article4/index.html#feature-extraction",
    "href": "posts/article4/index.html#feature-extraction",
    "title": "How to Build a Music Genre Classifier Using Hugging Face Transformers and Gradio",
    "section": "Feature Extraction",
    "text": "Feature Extraction\nJust as tokenization is essential in NLP, audio and speech models need input encoded in a processable format. In 🤗 Transformers, this is handled by the model’s feature extractor. The AutoFeatureExtractor class automatically selects the right feature extractor for a given model. Let’s see how to process our audio files by instantiating the feature extractor for DistilHuBERT from the pre-trained checkpoint:\nfrom transformers import AutoFeatureExtractor\n\n# Load a pre-trained feature extractor\nmodel_id = 'ntu-spml/distilhubert'\nfeature_extractor = AutoFeatureExtractor.from_pretrained(\n    model_id,\n    do_normalize=True,\n    return_attention_mask=True\n)\n\n# Get the sampling rate from the feature extractor\nsampling_rate = feature_extractor.sampling_rate\nsampling_rate\n\nOutput\n16000\n\n\nExplanation\n\nLoading Feature Extractor: AutoFeatureExtractor.from_pretrained loads a pre-trained feature extractor model.\nSampling Rate: feature_extractor.sampling_rate retrieves the sampling rate needed for the audio data."
  },
  {
    "objectID": "posts/article4/index.html#preprocess-the-dataset",
    "href": "posts/article4/index.html#preprocess-the-dataset",
    "title": "How to Build a Music Genre Classifier Using Hugging Face Transformers and Gradio",
    "section": "Preprocess the Dataset",
    "text": "Preprocess the Dataset\nWe preprocess the audio data to match the input requirements of the model by converting audio samples to the desired format and sampling rate.\nfrom datasets import Audio\n\n# Cast the audio column to match the feature extractor's sampling rate\ngtzan = gtzan.cast_column('audio', Audio(sampling_rate=sampling_rate))\n\ngtzan[\"train\"][0]\nBelow we can verify that the sampling rate is downsampled to 16 kHz. 🤗 Datasets will resample the audio file in real-time as each audio sample is loaded:\n\nOutput\n{\n    \"file\": \"~/.cache/huggingface/datasets/downloads/extracted/fa06ce46130d3467683100aca945d6deafb642315765a784456e1d81c94715a8/genres/pop/pop.00098.wav\",\n    \"audio\": {\n        \"path\": \"~/.cache/huggingface/datasets/downloads/extracted/fa06ce46130d3467683100aca945d6deafb642315765a784456e1d81c94715a8/genres/pop/pop.00098.wav\",\n        \"array\": array(\n            [\n                0.0873509,\n                0.20183384,\n                0.4790867,\n                ...,\n                -0.18743178,\n                -0.23294401,\n                -0.13517427,\n            ],\n            dtype=float32,\n        ),\n        \"sampling_rate\": 16000,\n    },\n    \"genre\": 7,\n}\nWhat we have just done is that we’ve provided the sampling rate of our audio data to our feature extractor. This is a crucial step as the feature extractor verifies whether the sampling rate of our audio data matches the model’s expected rate. If there were a mismatch, we would need to up-sample or down-sample the audio data to align with the model’s required sampling rate.\nAfter processing our resampled audio files, the final step is to create a function that can be applied to all examples in the dataset. Since we want the audio clips to be 30 seconds long, we will truncate any longer clips using the max_length and truncation arguments of the feature extractor.\n# Function to preprocess the audio data\nmax_duration = 30.0\n\ndef preprocess_function(examples):\n    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n    inputs = feature_extractor(\n        audio_arrays,\n        sampling_rate=feature_extractor.sampling_rate,\n        max_length=int(feature_extractor.sampling_rate * max_duration),\n        truncation=True,\n        return_attention_mask=True,\n    )\n    return inputs\n\n# Apply the preprocessing function to the dataset\ngtzan_encoded = gtzan.map(\n    preprocess_function,\n    remove_columns=[\"audio\", \"file\"],\n    batched=True,\n    batch_size=100, # by default is 1000\n    num_proc=1,\n)\ngtzan_encoded\n\n\nOutput\nDatasetDict({\n    train: Dataset({\n        features: ['genre', 'input_values', 'attention_mask'],\n        num_rows: 899\n    })\n    test: Dataset({\n        features: ['genre', 'input_values', 'attention_mask'],\n        num_rows: 100\n    })\n})\n\n\nExplanation\n\nPreprocessing Function: preprocess_function truncates or pads audio samples to a fixed length, normalizes them, and creates attention masks.\nApplying Function: gtzan.map applies the preprocessing function to the entire dataset.\n\nfeature_extractor provides a dictionary containing two arrays: input_values and attention_mask. That is why we see them as new columns for our features.\nsample = gtzan[\"train\"][0][\"audio\"]\n\ninputs = feature_extractor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"])\n\nprint(f\"inputs keys: {list(inputs.keys())}\")\ninputs keys: ['input_values', 'attention_mask']\nFor a simpler training process, we’ve excluded the audio and file columns from the dataset. Instead, the dataset now includes an input_values column with encoded audio files, an attention_mask column with binary masks (0 or 1) indicating padded areas in the audio input, and a genre column with corresponding labels or targets."
  },
  {
    "objectID": "posts/article4/index.html#prepare-labels",
    "href": "posts/article4/index.html#prepare-labels",
    "title": "How to Build a Music Genre Classifier Using Hugging Face Transformers and Gradio",
    "section": "Prepare Labels",
    "text": "Prepare Labels\nWe need to rename the genre column to label to enable the Trainer to process the class labels.\ngtzan_encoded = gtzan_encoded.rename_column(\"genre\", \"label\")\n\n# Create mappings from IDs to labels and vice versa\nid2label = {str(i): id2label_fn(i) for i in range(len(gtzan_encoded[\"train\"].features[\"label\"].names))}\nlabel2id = {v: k for k, v in id2label.items()}\n\nid2label\n\nOutput\n{ '0': 'blues',\n  '1': 'classical',\n  '2': 'country',\n  '3': 'disco',\n  '4': 'hiphop',\n  '5': 'jazz',\n  '6': 'metal',\n  '7': 'pop',\n  '8': 'reggae',\n  '9': 'rock'\n}\n\n\nExplanation\n\nRenaming Column: rename_column(\"genre\", \"label\") renames the genre column to label.\nCreating Mappings: id2label and label2id create dictionaries to map genre IDs to names and vice versa."
  },
  {
    "objectID": "posts/article4/index.html#load-and-fine-tune-the-model",
    "href": "posts/article4/index.html#load-and-fine-tune-the-model",
    "title": "How to Build a Music Genre Classifier Using Hugging Face Transformers and Gradio",
    "section": "Load and Fine-tune the Model",
    "text": "Load and Fine-tune the Model\nWe load a pre-trained audio classification model and fine-tune it on the GTZAN dataset.\nfrom transformers import AutoModelForAudioClassification\n\n# Load a pre-trained audio classification model\nnum_labels = len(id2label)\n\nmodel = AutoModelForAudioClassification.from_pretrained(\n    model_id,\n    num_labels=num_labels,\n    label2id=label2id,\n    id2label=id2label,\n)\nThe next step is optional but advised. We basically link our notebook to the 🤗 Hub. The main advantage of doing so is to ensure that no model checkpoint is lost during the training process. You can get your Hub authentication token (permission: write) from here :\n# Login to Hugging Face Hub (optional)\nfrom huggingface_hub import notebook_login\n\nnotebook_login()\n\nOutput\n\n\n\n\n\nNext step, we define the training arguments (e.g. batch size, number of epochs, learning rate, etc.)\n# Define training arguments\nfrom transformers import TrainingArguments\n\nmodel_name = model_id.split(\"/\")[-1]\nbatch_size = 8\ngradient_accumulation_steps = 1\nnum_train_epochs = 10\n\ntraining_args = TrainingArguments(\n    f\"{model_name}-finetuned-gtzan\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=5e-5,\n    per_device_train_batch_size=batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=num_train_epochs,\n    warmup_ratio=0.1,\n    logging_steps=5,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    fp16=True,\n    push_to_hub=True,\n)\n\n\nExplanation\n\nLoading Model: AutoModelForAudioClassification.from_pretrained loads a pre-trained model for audio classification.\nTraining Arguments: TrainingArguments defines parameters for training, such as batch size, learning rate, number of epochs, and strategies for evaluation and saving."
  },
  {
    "objectID": "posts/article4/index.html#training-and-evaluation",
    "href": "posts/article4/index.html#training-and-evaluation",
    "title": "How to Build a Music Genre Classifier Using Hugging Face Transformers and Gradio",
    "section": "Training and Evaluation",
    "text": "Training and Evaluation\nLastly, we define a function to compute metrics and create a trainer to handle the training process.\nimport evaluate\nimport numpy as np\n\n# Load the accuracy metric\nmetric = evaluate.load(\"accuracy\")\n\n# Function to compute accuracy\ndef compute_metrics(eval_pred):\n    predictions = np.argmax(eval_pred.predictions, axis=1)\n    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n\n# Initialize the trainer\nfrom transformers import Trainer\n\ntrainer = Trainer(\n    model,\n    training_args,\n    train_dataset=gtzan_encoded[\"train\"],\n    eval_dataset=gtzan_encoded[\"test\"],\n    tokenizer=feature_extractor,\n    compute_metrics=compute_metrics,\n)\n\n# Train the model\ntrainer.train()\n\nOutput\n| Epoch | Training Loss | Validation Loss | Accuracy |\n|:-----:|:-------------:|:---------------:|:--------:|\n| 1.0   |   1.950200    |    1.817256     | 0.51     |\n| 2.0   |   1.158000    |    1.208284     | 0.66     |\n| 3.0   |   1.044900    |    0.998169     | 0.72     |\n| 4.0   |   0.655100    |    0.852473     | 0.74     |\n| 5.0   |   0.611300    |    0.669133     | 0.79     |\n| 6.0   |   0.383300    |    0.565036     | 0.86     |\n| 7.0   |   0.329900    |    0.623365     | 0.80     |\n| 8.0   |   0.114100    |    0.555879     | 0.81     |\n| 9.0   |   0.135600    |    0.572448     | 0.80     |\n| 10.0  |   0.105100    |    0.580898     | 0.79     |\nUsing the free tier GPU on Google Colab, we successfully trained our model in about 1 hour. With just 10 epochs and 899 training examples, we achieved an evaluation accuracy of up to 86%. To further optimize model performance, we could increase the number of epochs or apply regularization techniques such as dropout."
  },
  {
    "objectID": "posts/article4/index.html#inference",
    "href": "posts/article4/index.html#inference",
    "title": "How to Build a Music Genre Classifier Using Hugging Face Transformers and Gradio",
    "section": "Inference",
    "text": "Inference\nNow that we have our trained model, we can automatically submit our checkpoint to the leaderboard. You can modify the following values to fit your dataset, language, and model name:\nkwargs = {\n    \"dataset_tags\": \"marsyas/gtzan\",\n    \"dataset\": \"GTZAN\",\n    \"model_name\": f\"{model_name}-finetuned-gtzan\",\n    \"finetuned_from\": model_id,\n    \"tasks\": \"audio-classification\",\n}\nThe training results can now be uploaded to the Hub through the .push_to_hub command:\ntrainer.push_to_hub(**kwargs)\nBy following these steps, you built a complete system for music genre classification using the GTZAN dataset, Gradio for interactive visualization, and Hugging Face Transformers for model training and inference."
  },
  {
    "objectID": "posts/article4/index.html#gradio-demo",
    "href": "posts/article4/index.html#gradio-demo",
    "title": "How to Build a Music Genre Classifier Using Hugging Face Transformers and Gradio",
    "section": "Gradio Demo",
    "text": "Gradio Demo\nNow that we built our music classification model trained on GTZAN dataset, we can showcase it on Gradio. We first need to load up fine-tuned checkpoint using the pipeline() class:\nfrom transformers import pipeline\n\nmodel_id = \"toobarah/distilhubert-finetuned-gtzan\"\npipe = pipeline(\"audio-classification\", model=model_id)\nNext, we defined a function that processes an audio file through the pipeline. The pipeline handles loading the file, resampling it to the correct rate, and running inference with the model. The model’s predictions are then formatted as a dictionary for display.\ndef classify_audio(filepath):\n    preds = pipe(filepath)\n    outputs = {}\n    for p in preds:\n        outputs[p[\"label\"]] = p[\"score\"]\n    return outputs\nFinal step, we launch the Gradio demo by calling the function we just created:\nimport gradio as gr\n\ndemo = gr.Interface(\n    fn=classify_audio, inputs=gr.Audio(type=\"filepath\"), outputs=gr.Label()\n)\ndemo.launch(debug=True)\n* If you get an ImportError after running the last cell, try downgrading your Gradio using the following command:\npip install gradio==3.47.1\nOtherwise, you should see a window pop up as shown below! Go ahead, upload some music, test your model, and enjoy!"
  },
  {
    "objectID": "posts/article4/index.html#conclusion",
    "href": "posts/article4/index.html#conclusion",
    "title": "How to Build a Music Genre Classifier Using Hugging Face Transformers and Gradio",
    "section": "Conclusion",
    "text": "Conclusion\nThis tutorial was a step-by-step guide for fine-tuning the DistilHuBERT model for a music classification task. It has also been a learning journey for me, and I drew much inspiration from the work of the Hugging Face audio course as I began this project. I hope I was able to explain the steps clearly and that they were easy for you to follow. Every step shown here can be applied to any audio classification task, so if you’re interested in exploring other datasets or models, I recommend checking out other examples in the 🤗 Transformers repository.\nFor access to all the code shared here in one file, click on this Colab file. Happy coding! :)"
  },
  {
    "objectID": "posts/article2/index.html",
    "href": "posts/article2/index.html",
    "title": "Implementing Binary Image Classification Using FastAI, and Flask",
    "section": "",
    "text": "This guide covers the entire process of deploying a dog breed image classification model. The model is locally hosted, and utilizes a RESTful API built with Flask.\nIn this project, we will build a web application that can classify the breed of a dog given an image provided by the user. We will walk through the entire process, from loading the data and fine-tuning our model to designing the back end and front end using Flask, HTML, and CSS.\nSo, without further ado, let’s get started."
  },
  {
    "objectID": "posts/article2/index.html#data-loading-and-model-fine-tuning-in-jupyter-notebook",
    "href": "posts/article2/index.html#data-loading-and-model-fine-tuning-in-jupyter-notebook",
    "title": "Implementing Binary Image Classification Using FastAI, and Flask",
    "section": "Data Loading and Model Fine-Tuning in Jupyter Notebook",
    "text": "Data Loading and Model Fine-Tuning in Jupyter Notebook\nFirst, install and load all the packages and dependencies:\n!pip install fastbook\n!pip install timm\nimport timm # must be imported before fastai\nimport fastbook \nfrom fastbook import * \nfrom fastai.vision.widgets import * \nimport requests # for making HTTP requests \nfrom bs4 import BeautifulSoup as bs \nfrom pathlib import Path\nimport os\n\nDownloading Data\nIn the next steps, we will extract our dataset from a webpage using a method called web scraping. Our target website contains an extensive list of dog breed names. We will use the first 50 breed names and then download 150 different images for each breed using the Python library download_images, which retrieves images directly from Google.\n# Fetch the webpage and parse it\nURL = \"&lt;https://www.akc.org/expert-advice/news/most-popular-dog-breeds-full-ranking-list/&gt;\" \npage = requests.get(URL) \nsoup = bs(page.content, \"html.parser\")\nThis Following line of code uses a list comprehension to extract the text from the first cell of each row within the table body of a HTML document parsed using BeautifulSoup (soup).\nTo break it down:\n\nsoup.select(\"tbody tr\"): This part uses the select method of BeautifulSoup to find all (table row) elements that are descendants of a (table body) element in the parsed HTML document. It returns a list of these elements.\n[row.td.get_text(strip=True) for row in ...]: This is a list comprehension that iterates over each element (row) found by select and extracts the text from the first cell within that row. row.td accesses the first element in the row, and get_text(strip=True) gets the text content of that element, stripping any leading or trailing whitespace.\n[:50]: Finally, [:50] at the end limits the resulting list of breed names to the first 50 entries, ensuring that only the top 50 breeds are considered.\n\nbreeds = [row.td.get_text(strip=True) for row in soup.select(\"tbody tr\")][:50]\nNow, let’s download images using our list of breeds as keywords for the search engine:\n# Set up the working directory\nroot = Path.cwd() / \"dog-breed-DS\"\n\n# Query and download images for each breed\nif not root.exists():\n  root.mkdir()\n  for breed in breeds:\n      # duckduckgo_search(root, breed, breed, max_results=100)\n      dest = root/breed\n      dest.mkdir(exist_ok=True)\n      download_images(dest, urls=search_images_ddg(f'{breed} dog', max_images=150))\n      resize_images(dest, dest=dest)\n\n\nData Cleaning\nIf you proceed with training your model on the dataset you collected, you might end up with an error similar to this:\nOSError: Caught OSError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/PIL/ImageFile.py\", line 249, in load\n    s = read(self.decodermaxblock)\n ...\n \nstruct.error: unpack_from requires a buffer of at least 4 bytes for unpacking 4 bytes at offset 0 (actual buffer size is 0)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n...    \n\n  File \"/usr/local/lib/python3.10/dist-packages/PIL/ImageFile.py\", line 256, in load\n    raise OSError(msg) from e\nOSError: image file is truncated\nThis error merely implies that one or more of your image files is corrupted or truncated. This can happen for various reasons, such as incomplete downloads, network issues, or issues with the image source. In order to fix this issue, you can clean the image dataset by deleting all the corrupted images in a given folder so that only the good ones remain in the directory:\ncnt = 0\nfor breedname in os.listdir(root):\n    for filename in os.listdir(os.path.join(root, breedname)):\n        try:\n           img=Image.open(os.path.join(root, breedname, filename))\n        except OSError:\n            print(\"FILE:\", os.path.join(breedname, filename), \"is corrupt!\")\n            cnt += 1\n            os.remove(os.path.join(root, breedname, filename))\nprint(\"Successfully Completed Operation! Files Courrupted are \", cnt)\n\n\nData Augmentation and Loading\nAt this step, we begin to set up a pipeline to process a dataset of dog images, applying necessary transformations and preparing it for training a deep learning model using the fastai library.\nFirst, we need to create the DataBlock. Let’s take it to smaller pieces and see what each part does:\n\nDataBlock: DataBlock is a fastai class that allows you to define how to create your dataset. It provides a flexible and simple way to handle data pipelines.\nblocks: This parameter defines the types of input and output blocks. ImageBlock is for handling images, and CategoryBlock is for categorical labels (the dog breeds in this case).\nget_items: get_image_files is a function that retrieves all the image file paths.\nsplitter: RandomSplitter(valid_pct=0.2, seed=42) is used to split the dataset into training and validation sets. Here, 20% of the data is used for validation, and the split is reproducible due to the set seed.\nget_y: parent_label is a function that gets the label for each image, typically by extracting it from the parent directory name.\nitem_tfms: Resize(129) is an item transformation that resizes each image to 129x129 pixels.\n\ndogs = DataBlock(\n      blocks=(ImageBlock, CategoryBlock),\n      get_items=get_image_files,\n      splitter=RandomSplitter(valid_pct=0.2, seed=42),\n      get_y=parent_label,\n      item_tfms=Resize(129)\n)\nIn the following cell we are trying to do the data augmentation to enhance the performance of our model:\n\ndogs.new: This creates a new DataBlock object with updated transformations.\nitem_tfms: RandomResizedCrop(129, min_scale=0.5) replaces the previous Resize(129). This transformation randomly crops the image to 129x129 pixels with a minimum scale of 0.5, which helps with data augmentation.\nbatch_tfms: aug_transforms(mult=2) applies a set of data augmentation transformations to the entire batch. The mult=2 argument increases the intensity of these transformations.\n\nYou can try to change the numbers to find the right adjustment for your model design.\ndogs = dogs.new(\n      item_tfms = RandomResizedCrop(129, min_scale=0.5),\n      batch_tfms = aug_transforms(mult=2)\n)\nThe next method creates the DataLoaders object, which is used to load the data in batches. root is the root directory where the images are stored.\ndls = dogs.dataloaders(root)\nFinally, we show a batch of images from the training set for inspection.\ndls.train.show_batch(max_n=4, nrows=1, unique=True)\n\n\n\nTraining the Model\nLet’s go ahead and train our model\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(4)\nAs you can see we get a training time of 3:53 min on the last epoch.\n\n\n\n\n\nThanks to the great work that’s been done by Ross Wightman, we can import the powerful timm library which provides state-of-the-art pre-trained computer vision models. This allows us to improve our model performance. You can learn more about the library through this analysis. \nHere are some of the models covered by timm:\ntimm.list_models('convnext*')\n\n\n\n\n\nNow, training on we get a better result!\nlearn = vision_learner(dls, 'convnext_tiny_in22k', metrics=error_rate).to_fp16()\nlearn.fine_tune(4)\n\n\n\n\n\nNotice the time is much faster and the accuracy increased from 81% to almost 86%!\nLastly, we download our model and we’re done with this part:\nlearn.export('/content/model.pkl')"
  },
  {
    "objectID": "posts/article2/index.html#building-a-restful-api-with-flask",
    "href": "posts/article2/index.html#building-a-restful-api-with-flask",
    "title": "Implementing Binary Image Classification Using FastAI, and Flask",
    "section": "Building a RESTful API with Flask",
    "text": "Building a RESTful API with Flask\nREST APIs have become essential to modern software applications, enabling seamless communication between systems and ensuring data consistency. They facilitate the easy exchange of information between various websites, databases, and SaaS (Software as a Service) applications. Whether you’re developing a mobile app, a web application, or integrating third-party services, understanding how to create a robust REST API is crucial. \nThere are numerous ways to build a REST API, and leveraging existing frameworks and tools can streamline the process. One such example is Flask, a web framework in Python. With the Flask-RESTful extension, developers can quickly and efficiently develop REST APIs. \nNow, let’s dive into the implementation and see how we can create a REST API using Flask and the Flask-RESTful extension.\n\nSetup Your Flask Project\n\nOpen VSCode\nCreate a new directory for your project and navigate into it.\nCreate a virtual environment: python -m venv .venv\nActivate the virtual environment:\n\nOn Windows: .venv\\Scripts\\activate\nOn macOS/Linux: source .venv/bin/activate\n\nInstall Flask and fastbook if you haven’t already:\n\npip install fastbook\npip install Flask\n\nCreate the following project structure:\n├── app.py\n├── templates\n│   └── index.html\n|   └── result.html\n└── static\n|   └── style.css\n├── model.pkl\n\n\n\n\n\n\n\n\nCreate Your Flask App (app.py)\n\nImport necessary libraries and set up your Flask app.\nDefine routes for the home page (index.html) and result page (result.html).\nUse a form to upload an image on the home page and process the uploaded image on the result page.\nfrom flask import Flask, render_template, request, jsonify, url_for\nfrom fastbook import *\nfrom fastai import *\nfrom fastai.vision import *\nimport os\n\n\n# Define a flask app\napp = Flask(__name__)\nroot = os.getcwd()\nfilename = 'model.pkl'\nlearn = load_learner(os.path.join(root, filename))\n\n\n@app.route('/')\ndef index():\n    # Main page\n    return render_template('index.html')\n\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    prediction = single_image_predict(request.files['uploaded-file'])\n    prediction_dict = json.loads(prediction.data.decode('utf-8'))  # Convert bytes to string and then to dict\n\n    return render_template('result.html', percentage=prediction_dict['probability'], prediction=prediction_dict['dog_type'])\n\n\n#function to predict image\ndef single_image_predict(image):\n    img_PIL = Image.open(image)\n    img = tensor(img_PIL) #converting PIL image to tensor\n    learn_inf = learn.predict(img)\n    return jsonify({'dog_type': learn_inf[0][0:],\n                    'probability': str(round(max(learn_inf[2].numpy()*100), 2))})\n\n\nif __name__ == '__main__':\n    app.run(host=\"0.0.0.0\", port=8000)\n\nCreate Your HTML Templates (index.html and result.html)\n\nindex.html for the home page with a form to upload the image.\nresult.html to display the result (breed of the dog).\n&lt;!-- index.html --&gt;\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;Home&lt;/title&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"/static/style.css\"&gt;\n    &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"{{ url_for('static', filename='/styles.css') }}\"&gt;\n&lt;/head&gt;\n\n&lt;body&gt;\n\n    &lt;div class=\"container main\"&gt;&lt;/div&gt;\n        &lt;div class=\"row\"&gt;\n          &lt;div class=\"col\"&gt;\n            &lt;h1 class=\"header\"&gt;Dog Breed Image Classifier&lt;/h1&gt;\n            &lt;form action=\"{{ url_for('predict')}}\" method=\"post\" enctype=\"multipart/form-data\"&gt;\n                &lt;label for=\"uploaded-file\" class=\"upload-frame\"&gt;\n                    Upload your photo here :)\n                    &lt;input type=\"file\" id=\"uploaded-file\" class=\"form-control\" name=\"uploaded-file\" style=\"display: none;\" onchange=\"displayImage(event)\"&gt;\n                &lt;/label&gt;\n                &lt;input type=\"hidden\" name=\"image_path\" id=\"image_path\"&gt;\n                &lt;div class=\"button-group\"&gt;\n                    &lt;!-- &lt;button class=\"upload-photo-btn\" type=\"button\" onclick=\"document.getElementById('file-input').click()\"&gt;Upload Photo&lt;/button&gt; --&gt;\n                    &lt;button class=\"submit-btn\" type=\"submit\"&gt;Submit&lt;/button&gt;\n                &lt;/div&gt;\n            &lt;/form&gt;\n\n            &lt;script&gt;\n                function displayImage(event) {\n                    var file = event.target.files[0];\n                    var reader = new FileReader();\n\n                    reader.onload = function(e) {\n                        var imageUrl = e.target.result;\n                        var uploadFrame = document.querySelector('.upload-frame');\n                        uploadFrame.style.backgroundImage = 'url(' + imageUrl + ')';\n                        document.getElementById('image_path').value = imageUrl; // Set the value of the hidden input field\n                    };\n\n                    reader.readAsDataURL(file);\n                }\n\n                function validateForm(event) {\n                    var fileInput = document.getElementById('uploaded-file');\n                    if (fileInput.files.length === 0) {\n                        alert('Please upload a photo first');\n                        event.preventDefault(); // Prevent the form from submitting\n                    }\n                }\n\n                document.querySelector('form').addEventListener('submit', validateForm);\n            &lt;/script&gt;\n\n        &lt;/div&gt;\n    &lt;/div&gt;\n\n    &lt;/body&gt;\n&lt;/html&gt; \n&lt;!-- result.html --&gt;\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;&lt;/title&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"/static/style.css\"&gt;\n    &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"{{ url_for('static', filename='/styles.css') }}\"&gt;\n&lt;/head&gt;\n&lt;body&gt;\n\n    &lt;header&gt;\n        &lt;div class=\"container\"&gt;\n            &lt;h1 class=\"header\"&gt;Classification Results&lt;/h1&gt;\n        &lt;/div&gt;\n    &lt;/header&gt;\n\n    &lt;div class=\"results\"&gt;\n        &lt;h3&gt;This is {{ percentage }}% {{ prediction }}&lt;/h3&gt;\n        &lt;p&gt;Let's play again! Hit the Home link below :)&lt;/p&gt;\n        &lt;div style=\"text-align: center;\"&gt;\n            &lt;a href=\"{{ url_for('index') }}\" class=\"back-btn\"&gt;Home&lt;/a&gt;\n        &lt;/div&gt;\n    &lt;/div&gt;\n\n&lt;/body&gt;\n&lt;/html&gt;\n\nAdd Pretrained Model for Dog Breed Classification\n\nUse a the downloaded pretrained model for dog breed classification.\nLoad the model in your Flask app and use it to classify the uploaded image.\n\nRun and Test Your Flask App\n\nRun your Flask app: python app.py\nAccess your web app in a browser at http://192.168.4.218:8000 (or any link that shows up on your terminal) to upload a dog photo and see the result!"
  },
  {
    "objectID": "posts/article2/index.html#next-step-deployment",
    "href": "posts/article2/index.html#next-step-deployment",
    "title": "Implementing Binary Image Classification Using FastAI, and Flask",
    "section": "Next Step: Deployment",
    "text": "Next Step: Deployment\nSo far, we have learned how to create our deep learning-based web app. We designed both the backend and the frontend. The final step is to deploy our model so everyone can access our application on their own devices.\nThere are many options for choosing the right platform, but one of the most widely used deployment environments is a cloud service provider. Cloud services are ubiquitous these days, making it essential for machine learning practitioners and tech experts to stay ahead with a solid understanding of these services. One of the most important cloud service providers in the job market is AWS.\nDeploying a project of this scale on AWS can be tricky (and costly). If you have never worked with the platform before and want to do some warm-up practices with DL models, the best alternative options would be using Heroku, Hugging Face, or Render.\nAlright, we will stop the project at this point, but you can keep experimenting with the code and perhaps create your own image classification app with your chosen data!\nFinally, I want to leave you with some sources that inspired me for this project. You can also find my code in this repository. Let me know what you think, and as always, all feedbacks are welcome! :)\n\nBinary Image Classifier Deployed on Heroku\nDeploy Machine Learning Model API on AWS EC2\nHow to Make a REST API"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CognitiveDiscoveries",
    "section": "",
    "text": "Detecting Fraudulent Transactions Using Random Forest: A Step-by-Step Guide\n\n\n\n\n\n\nTabular Data\n\n\nRandom Forest\n\n\nKaggle\n\n\n\n\n\n\n\n\n\nJul 24, 2024\n\n\nTooba Rahimnia\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Create a Web App with Django and YOLOv10 Models on Your Custom Dataset\n\n\n\n\n\n\nWeb App\n\n\nComputer Vision\n\n\nDjango\n\n\n\n\n\n\n\n\n\nJul 13, 2024\n\n\nTooba Rahimnia\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Setup a Machine for Deep Learning Environment (Mac Version)\n\n\n\n\n\n\nMamba\n\n\n\n\n\n\n\n\n\nJul 3, 2024\n\n\nTooba Rahimnia\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Build a Music Genre Classifier Using Hugging Face Transformers and Gradio\n\n\n\n\n\n\nWeb App\n\n\nAudio Signal Processing\n\n\n\n\n\n\n\n\n\nJun 20, 2024\n\n\nTooba Rahimnia\n\n\n\n\n\n\n\n\n\n\n\n\nNavigating the Kaggle U.S. Patent Phrase to Phrase Matching Competition: A Step-by-Step Guide\n\n\n\n\n\n\nNLP\n\n\nTabular Data\n\n\nKaggle\n\n\n\n\n\n\n\n\n\nJun 5, 2024\n\n\nTooba Rahimnia\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Binary Image Classification Using FastAI, and Flask\n\n\n\n\n\n\nWeb App\n\n\nComputer Vision\n\n\nFlask\n\n\n\n\n\n\n\n\n\nMay 28, 2024\n\n\nTooba Rahimnia\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Build a Web App Using HuggingFace and Gradio Within an Hour\n\n\n\n\n\n\nWeb App\n\n\nComputer Vision\n\n\n\n\n\n\n\n\n\nMay 20, 2024\n\n\nTooba Rahimnia\n\n\n\n\n\n\nNo matching items"
  }
]