[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello and welcome to my blog! I’m a tech enthusiast who’s constantly learning and eager to share my journey with you. Here, I share everything I’ve learned about applied machine learning and the latest cloud technologies. Whether you’re just starting out or already an expert, I hope you’ll find something valuable in my posts. Thank you for joining me on this learning adventure!"
  },
  {
    "objectID": "posts/blog1/index.html",
    "href": "posts/blog1/index.html",
    "title": "How to Build a Web App Using HuggingFace and Gradio Within an Hour",
    "section": "",
    "text": "Walkthrough of an end to end dog vs cat image classification model deployed on HuggingFace Spaces, supported by FastAI and Gradio.\n\n\n\nImage by pexels.com\n\n\nIt’s been a few weeks that I have started taking the fantastic deep learning course, fastai, by Jeremy Howard and it has been an amazing learning journey so far. I come from a STEM background but Howard’s style of teaching deep learning has brought a fresh perspective into learning this field. As I go through each lecture, I made the decision of documenting and sharing my experiences and learning outcomes publicly, so that it could inspire, help or encourage someone along the same path. This course has been a game-changer in my learning path so far, and I’m thrilled to talk about my first project in image classification.\nIn this very first blog, I will guide you through deploying an image classification model using HuggingFace and Gradio. This method is beginner-friendly, straightforward and completely free. Whether you are a newcomer or looking to refine your deployment skills, I’ll walk you through each step, ensuring that by the end, you’ll be able to deploy your own models effortlessly. So, let’s get started on the exciting journey!\n\nPrerequisites\nBefore we begin, make sure you have the followings:\n\nA basic understanding of Python\nA HuggingFace account (sign up here)\n\n\n\nGetting Started\nFirst thing, open the Google Colab and let’s make sure we have the necessary tools set up. Since we’re using the popular fastai library, you might need to install or upgrade it first with the following command.\n!pip install -Uqq fastai\nNow, install the one necessary package needed at this step.\nfrom fastai.vision.all import *\n\n\nGathering Data\nFastai makes it incredibly easy for us to work with datasets thanks to its built-in function: untar_data(). This function streamlines the process of downloading and extracting datasets.\nIn our case, we use untar_data(URLs.PETS) to download a compressed dataset of pet images from a specified URL and extract it to a specific location on our local machine (path).\npath = untar_data(URLs.PETS)/'images'\nThe PETS dataset includes images of 37 breeds of pets along with annotations, which are perfect for training an image classification model.\n\n\nLoading Data\nNext we need to load our data. ImageDataLoaders.from_name_func is a method we use that is provided by the fastai library. It is designed to help you create a DataLoader for training and validating image classification models. This method is particularly useful when you have a dataset of images and the labels can be inferred from the filenames of those images, which is basically what we have here.\nThe primary purpose of ImageDataLoaders.from_name_func is to create a data loader that can feed images and their corresponding labels to a machine learning model during training and validation phases. It streamlines the process of preparing data, handling transformations, and ensuring that the data is fed in batches to the model.\nBut how does it work? Let’s break it down into smaller pieces:\n\npath: The path to your dataset directory.\nget_image_files(path): A function from fastai that returns a list of all image file paths in the directory.\nlabel_func: A function that takes a file path and returns the label. In this example, it selects the label according to the first letter of the file’s name. If the name starts with a capital letter, the image belongs to a cat, and a dog otherwise.\nitem_tfms: Transformations applied to each image individually, such as resizing.\n\ndls = ImageDataLoaders.from_name_func('.', \n            get_image_files(path), valid_pct=0.2, seed=42, \n            label_func=is_cat, \n            item_tfms=Resize(192))\n\n\ndef is_cat(x): return x[0].isupper()          \nLet’s take a look at some randomly picked photos from our dataset.\ndls.show_batch()\n\n\n\n\n\n\n\nTraining the Model\nAfter setting up the DataLoader, the next step is to create and train a model using the fastai library. Here’s how you can do it:\nFirst, we initialize a vision_learner with our data loaders (dls), specifying the architecture we want to use—in this case, ResNet-152, a powerful and accurate convolutional neural network. We also specify the metrics we want to monitor during training, such as the error rate.\nlearn = vision_learner(dls, resnet152, metrics=error_rate)\nNext, we fine-tune the model using the fine_tune method:\nlearn.fine_tune(3)\nThis method fine-tunes the model for a specified number of epochs—in this case, 3 epochs. Fine-tuning involves training the model’s top layers, which are typically initialized with random weights, while gradually unfreezing and training the deeper layers of the pre-trained network. This process allows the model to adapt to our specific dataset while leveraging the powerful features learned by ResNet-152 on a much larger dataset.\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.051338\n0.039052\n0.010825\n01:58\n\n\n1\n0.026246\n0.006559\n0.002030\n01:58\n\n\n2\n0.010546\n0.004937\n0.001353\n01:57\n\n\n\n\n\nDownloading the Model\nLet’s go ahead and download our model.\nlearn.export('model.pkl')\n\n\nDeployment\nFor the deployment, it’s very straightforward. We will be using HuggingFace Spaces. It is a platform that allows you to host and share machine learning models and demos easily.\nBefore going onto HuggingFace, we need to create our app.py and requirements.txt files so that we can upload them to the HF repo.\nIn the app.py file, we load the model that we saved previously, using load_learner('model.pkl'), and then classify our new unseen images using the Gradio interface. Gradio is a Python library that allows you to quickly create customizable web apps for your machine learning models and data processing pipelines.\nWhat we are basically doing here is setting up a web interface where users can upload images, and our model will classify these images and return the results. We call our classify_image function, specify our inputs and outputs, and optionally include some example images for users to test.\nfrom fastai.vision.all import *\nimport gradio as gr\n\ndef is_cat(x): return x[0].isupper() \n\nlearn = load_learner('model.pkl')\n\ncategories = ('Dog', 'Cat')\n\ndef classify_image(img):\n    pred,idx,probs = learn.predict(img)\n    return dict(zip(categories, map(float,probs)))\n\nimage = gr.Image()\nlabel = gr.Label()\nexamples = ['dog.jpg', 'cat.jpg', 'cat_dog.jpg']\n\nintf = gr.Interface(fn=classify_image, inputs=image, outputs=label, examples=examples)\nintf.launch(inline=False)\nWe need to create a requirements.txt file as mentioned before, and all we need to mention is the fastai library because HF doesn’t automatically include it. This file ensures that the necessary dependencies are installed when the app is deployed on HuggingFace Spaces.\n\nSetting up HuggingFace Spaces\nSo how do you set up the HF Space? First, go to the HuggingFace Spaces website. Once you’re there, click on “Create new Space”.\nChoose a Space name of your own. For the License, choose “apache-2.0”. Select Gradio as the Space SDK. Leave the rest as is and click on “Create Space”.\n\nOpen your local terminal and navigate to the directory where your required files, that you created previously, are saved. Once there, do the following:\ngit clone &lt;https://huggingface.co/spaces/&gt;&lt;your_login&gt;/&lt;your_app_name&gt;\ncd &lt;your_app_name&gt;\nHave your three files ready:\n\napp.py\nrequirements.txt\nmodel.pkl\n\nYou can commit and push the first two files as follow:\ngit add &lt;your_file_name&gt; \ngit commit -m 'Add application file'\ngit push\nHowever, to upload your model, you need a different approach since it’s a large file (over 25 MB). You need to first. Follow these steps:\n\nInstall Git: If you do not have Git installed locally, please download from here.\nDownload and install Git Large File Storage (LFS): Git LFS is an extension for Git that allows you to handle large files. Download and install it from here.\nSet up Git LFS: Type the following commands in your terminal:\ngit lfs install \ngit lfs track \"*.pkl\"\ngit add .gitattributes \nAdd and commit your model file:\ngit add model.pkl \ngit commit -m \"Add model\" \nPush your changes to the repository:\ngit push -u origin master\n\n\n\n\nFinal Result\nAfter following all these steps, your app will show up on the screen in a few moments! You’ll see your HuggingFace Space with your deployed image classification model, ready to use and share with others!\n\nThanks for sticking with me up to this point! I hope you found this guide useful. Feel free to check out my app and all the source code by clicking here. Happy coding, and keep pushing boundaries!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CognitiveDiscoveries",
    "section": "",
    "text": "How to Build a Web App Using HuggingFace and Gradio Within an Hour\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nMay 2, 2024\n\n\nTooba Rahimnia\n\n\n\n\n\n\nNo matching items"
  }
]